<!DOCTYPE html>
```html
<!DOCTYPE html>
<html lang="zh-Hans">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>从零开始重现 GPT-2 (124M)：深度技术指南</title>
    <style>
        body {
            font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, Helvetica, Arial, sans-serif;
            line-height: 1.6;
            color: #333;
            max-width: 800px;
            margin: 0 auto;
            padding: 20px;
            font-size: 16px;
        }
        h1 { font-size: 2.5em; color: #1a1a1a; border-bottom: 2px solid #eaeaea; padding-bottom: 10px; }
        h2 { font-size: 1.8em; color: #2c3e50; margin-top: 40px; border-left: 5px solid #3498db; padding-left: 15px; }
        h3 { font-size: 1.4em; color: #34495e; margin-top: 30px; }
        p { margin-bottom: 1.5em; text-align: justify; }
        img { max-width: 100%; height: auto; display: block; margin: 20px auto; border-radius: 8px; box-shadow: 0 4px 10px rgba(0,0,0,0.1); }
        .caption { text-align: center; font-style: italic; color: #666; margin-top: -10px; margin-bottom: 20px; font-size: 0.9em; }
        code { background-color: #f4f4f4; padding: 2px 5px; border-radius: 4px; font-family: "SFMono-Regular", Consolas, "Liberation Mono", Menlo, monospace; }
        pre { background-color: #2d2d2d; color: #ccc; padding: 15px; border-radius: 8px; overflow-x: auto; font-family: "SFMono-Regular", Consolas, monospace; }
        .timestamp { color: #e67e22; font-weight: bold; }
    </style>
</head>
<body>

    <h1>从零开始重现 GPT-2 (124M)：深度技术指南</h1>

    <section>
        <h2>引言：让我们重现 GPT-2 (124M)</h2>
        <img src="frames/00_00_00.jpg" alt="intro: Let’s reproduce GPT-2 (124M)">
        <p class="caption">图 1：重现 GPT-2 (124M) 的旅程正式开启</p>
        <p><span class="timestamp">[00:00:00]</span> 欢迎回到“从零到英雄”系列。今天我们的目标非常明确且具有挑战性：我们要从头开始重现 OpenAI 在 2019 年发布的 GPT-2 模型，特别是其 124M 参数的版本。GPT-2 是自然语言处理史上的一个里程碑，它展示了通过大规模无监督预训练，语言模型可以获得惊人的多任务处理能力。虽然现在我们已经有了 GPT-4 这样更强大的模型，但理解 GPT-2 的架构仍然是掌握现代大语言模型（LLM）的基石。</p>
        <p>在本次教程中，我们将不仅构建网络架构，还将深入探讨如何优化训练过程，使其在现代 GPU 上运行得飞快。我们将遵循 GPT-2 和 GPT-3 论文中提到的超参数设置，确保我们的复现具有高度的权威性和准确性。通过这个过程，你会发现构建一个高性能的语言模型不仅仅是编写几行神经网络代码，更多的是关于硬件利用、数据流管理和精细的数学调优。这不仅仅是一个编程练习，更是一场关于计算效率的深度探索。</p>
        <p>我们将使用的核心工具是 PyTorch，因为它对开发者非常友好且功能强大。虽然 OpenAI 最初的代码是基于 TensorFlow 编写的，但由于生态系统的演变，PyTorch 已成为目前学术界和工业界的首选。我们将通过构建一个名为 <code>build-nanogpt</code> 的仓库，逐步提交代码，记录每一个细微的改进。这种循序渐进的方法将帮助你理解每一个优化步骤背后的逻辑和带来的性能提升。</p>
        <p>最后，我们会在云端 GPU 上运行完整的训练流程。通过 Lambda GPU Cloud 等平台，我们可以租用高性能的 A100 显卡，只需花费约 10 美元和不到一个小时的时间，就能训练出一个性能可观的模型。当你看到模型从最初输出随机字符，到逐渐能生成连贯的、具有莎士比亚风格或百科知识的文本时，你会感受到深度学习的真正魅力。现在，让我们正式开始这段充满技术细节的旅程。</p>
    </section>

    <section>
        <h2>探索 GPT-2 (124M) OpenAI 检查点</h2>
        <img src="frames/00_03_10.jpg" alt="exploring the GPT-2 (124M) OpenAI checkpoint">
        <p class="caption">图 2：分析 OpenAI 发布的 GPT-2 权重与参数结构</p>
        <p><span class="timestamp">[00:03:39]</span> 在动手编写代码之前，最明智的做法是先观察“终点”，即 OpenAI 官方发布的 GPT-2 检查点。GPT-2 系列包含四个不同规模的模型，从 1.24 亿参数一直到 15 亿参数。有趣的是，官方论文中的表格在参数统计上其实存在微小的偏差，如果你去查看 GitHub 仓库，会发现实际的参数计算方式略有不同。我们要复现的 124M 模型实际上是由 12 层 Transformer 组成的，每层具有 768 个通道。</p>
        <p>为了方便在 PyTorch 中加载这些权重，我们将利用 HuggingFace 的 <code>transformers</code> 库。HuggingFace 已经完成了繁重的权重转换工作，将原始的 TensorFlow 张量转换成了 PyTorch 兼容的格式。通过加载 <code>gpt2</code> 模型，我们可以直接访问其 <code>state_dict</code>，这是一个包含所有权重张量的字典。通过打印这些键值对，我们可以清晰地看到模型的内部构造，包括词嵌入（WTE）、位置嵌入（WPE）以及每一层 Transformer 块中的权重。</p>
        <p>我们可以对这些权重进行可视化，以获得直观的理解。例如，位置嵌入（WPE）矩阵的大小是 1024x768，代表了模型支持的最大序列长度。如果我们将其压平并绘制出来，会发现它具有明显的结构感，这说明模型在训练过程中学会了识别词元在序列中的相对位置。虽然 GPT-2 使用的是可学习的位置嵌入而非原始 Transformer 论文中的正弦函数编码，但最终学习到的模式在本质上是相似的，都体现了序列的周期性特征。</p>
        <p>观察这些权重还能揭示模型是否训练充分。在某些通道中，权重的分布可能看起来比较嘈杂，呈现出锯齿状，这通常暗示模型可能还存在进一步优化的空间。通过这种“试驾”，我们不仅确认了模型的架构细节，还建立了一个基准。接下来的任务就是编写我们自己的 <code>GPT</code> 类，确保它能完美加载这些官方权重，并生成同样连贯的文本。这是验证我们架构实现正确性的第一步。</p>
    </section>

    <section>
        <h2>第一部分：实现 GPT-2 nn.Module</h2>
        <img src="frames/00_13_25.jpg" alt="SECTION 1: implementing the GPT-2 nn.Module">
        <p class="caption">图 3：在 PyTorch 中构建 GPT-2 的神经网络骨架</p>
        <p><span class="timestamp">[00:13:47]</span> 现在开始构建我们自己的 GPT-2 神经网络模块。我们需要定义一个继承自 <code>nn.Module</code> 的类。为了保持与 HuggingFace 命名约定的一致性，我们将内部容器命名为 <code>transformer</code>，并使用 <code>nn.ModuleDict</code> 来存储词嵌入和位置嵌入。这种结构化的命名不仅让代码更易读，更重要的是它能让我们在加载官方权重时实现自动化的映射，避免手动处理成百上千个张量的尴尬。</p>
        <p>GPT-2 的架构本质上是一个“仅解码器”（Decoder-only）的 Transformer。这意味着它去掉了原始 Transformer 中的编码器部分，以及与之相关的交叉注意力机制。在我们的实现中，每一层 <code>Block</code> 将包含层归一化（LayerNorm）、自注意力（Self-Attention）和前馈网络（MLP）。需要特别注意的是，GPT-2 引入了“预归一化”（Pre-Norm）的设计，即 LayerNorm 位于残差连接的内部路径上。这种改动被证明能显著提高训练的稳定性。</p>
        <p>此外，GPT-2 在最后一层 Transformer 块之后增加了一个额外的 LayerNorm。这是许多初学者容易忽略的细节。如果没有这个归一化层，直接连接到输出的线性头（LM Head）可能会面临数值不稳定的问题。我们将这些组件有机地组合在一起，构建出从输入词元 ID 到输出 Logits 的完整路径。Logits 的维度是词汇表的大小，代表了模型对下一个词元的预测概率分布。</p>
        <p>在构建过程中，我们会定义一个 <code>GPTConfig</code> 类来管理超参数。对于 124M 模型，<code>n_layer</code> 为 12，<code>n_head</code> 为 12，<code>n_embd</code> 为 768。通过将配置与模型逻辑分离，我们可以轻松地扩展代码以支持更大规模的 GPT-2 变体。这种模块化的设计思路是编写高质量深度学习代码的核心。现在，骨架已经搭建完成，我们需要填充内部的 MLP 和注意力逻辑。</p>
    </section>

    <section>
        <h2>实现 MLP 与 GELU 激活函数</h2>
        <img src="frames/00_21_15.jpg" alt="Implementing the MLP and GELU Activation">
        <p class="caption">图 4：前馈网络（MLP）的内部构造与 GELU 激活函数</p>
        <p><span class="timestamp">[00:20:53]</span> 前馈网络（MLP）是 Transformer 块中负责“独立思考”的部分。在 GPT-2 中，每个 MLP 包含两个线性层，中间夹着一个非线性激活函数。第一个线性层将维度从 768 投影到 3072（即 4 倍扩展），第二个线性层再将其投影回 768。这种“升维再降维”的操作允许模型在更高维的空间中处理特征，从而捕捉复杂的语义关系。</p>
        <p>GPT-2 选择使用 GELU（高斯误差线性单元）作为激活函数，而不是传统的 ReLU。GELU 在零点附近更加平滑，且在负值区域保留了微小的梯度，这有助于缓解“神经元死亡”的问题。有趣的是，GPT-2 使用的是 GELU 的一个近似版本，其公式涉及到一个特定的常数 0.044715。在复现时，我们必须使用 <code>nn.GELU(approximate='tanh')</code> 来确保与原始模型的数值完全匹配。</p>
        <p>为什么当时要使用近似版本呢？这主要是一个历史遗留问题。在 2019 年，计算精确的 GELU 函数（涉及误差函数 <code>erf</code>）在某些硬件和框架上运行较慢。通过使用双曲正切函数 <code>tanh</code> 进行近似，可以显著加快计算速度，而对模型性能的影响几乎可以忽略不计。虽然现代硬件已经可以高效处理精确的 GELU，但为了完美重现 GPT-2，我们仍然坚持使用这个“带有历史气息”的近似公式。</p>
        <p>在代码实现上，MLP 模块非常简洁。我们使用 <code>nn.Sequential</code> 将线性层、GELU 和输出线性层连接起来。需要注意的是，GPT-2 的线性层通常带有偏置项（bias），这在我们的代码中需要明确指定。通过这个简单的结构，模型能够对每个词元的位置进行非线性变换，为后续的层提取更深层次的信息。接下来，我们将进入 Transformer 最核心的部分：多头自注意力机制。</p>
    </section>

    <section>
        <h2>实现多头自注意力机制</h2>
        <img src="frames/00_23_40.jpg" alt="Implementing Multi-Head Self-Attention">
        <p class="caption">图 5：自注意力机制中的 Q、K、V 交互与因果掩码</p>
        <p><span class="timestamp">[00:23:54]</span> 多头自注意力（Multi-Head Self-Attention）是 Transformer 的灵魂，它负责词元之间的“信息交流”。在 GPT-2 中，每个词元会产生三个向量：查询（Query）、键（Key）和值（Value）。查询向量代表“我在寻找什么”，键向量代表“我包含什么信息”，而值向量则是“我要传递的信息”。通过计算查询与键的点积，模型可以确定词元之间的关注权重，进而对值向量进行加权求和。</p>
        <p>为了提高效率，我们不会为每个“头”创建独立的模块，而是将所有头的 Q、K、V 拼接在一个巨大的线性层中。在 124M 模型中，我们有 12 个头，每个头的维度是 64（768/12）。这种做法允许我们利用 GPU 的并行计算能力，一次性完成所有头的投影运算。之后，我们通过张量变形（Reshape）和转置（Transpose）操作，将批次维度和头维度排列在一起，以便进行后续的矩阵乘法。</p>
        <p>由于 GPT-2 是自回归模型，它在预测下一个词元时不能“偷看”未来的信息。因此，我们必须应用因果掩码（Causal Mask）。这是一个下三角矩阵，将未来位置的注意力权重设为负无穷，经过 Softmax 后变为零。在 PyTorch 中，我们使用 <code>register_buffer</code> 来存储这个掩码，确保它随模型移动到 GPU 但不被视为训练参数。这种掩码机制确保了模型生成的逻辑严密性。</p>
        <p>最后，注意力机制的输出会经过一个输出投影层。这个层的作用是将多个头收集到的信息重新融合，并投影回残差流的维度。在我们的实现中，我们会非常小心地处理张量的维度变换，确保每一步都符合 PyTorch 的高效运算规则。自注意力机制的实现虽然复杂，但它是模型理解上下文联系的关键所在。现在，我们已经完成了 GPT-2 的核心组件，准备加载权重进行测试。</p>
    </section>

    <section>
        <h2>加载 HuggingFace GPT-2 参数</h2>
        <img src="frames/00_27_45.jpg" alt="loading the huggingface/GPT-2 parameters">
        <p class="caption">图 6：将预训练权重映射到自定义模型类中</p>
        <p><span class="timestamp">[00:28:08]</span> 搭建好模型骨架后，接下来的挑战是将 OpenAI 的预训练权重填入我们的类中。我们编写了一个 <code>from_pretrained</code> 类方法，它首先调用 HuggingFace 的库下载官方权重，然后遍历我们自己模型的 <code>state_dict</code>。由于我们刻意模仿了官方的命名规则，大部分权重的匹配应该是直截了当的。然而，这里隐藏着一个非常棘手的陷阱：卷积层与线性层的权重转置问题。</p>
        <p>OpenAI 原始代码中使用了 TensorFlow 的 <code>Conv1D</code> 层来实现线性变换。在 TensorFlow 中，权重的存储格式通常与 PyTorch 的 <code>nn.Linear</code> 相反。具体来说，对于注意力投影层和 MLP 中的线性层，我们需要对加载的权重进行转置操作（<code>.t()</code>）。如果你忽略了这一步，模型输出的 Logits 将完全是乱码，因为权重矩阵的维度虽然匹配，但内部数值的含义已经完全错位了。</p>
        <p>在加载过程中，我们还会进行严格的校验。我们会对比官方权重字典和我们模型字典的键名，并确保每个对应张量的形状完全一致。这种防御性编程习惯能让你在复杂的模型迁移工作中少走很多弯路。我们会忽略一些不需要加载的“缓冲区”，比如自注意力中的掩码矩阵，因为这些是在运行时自动生成的。通过这种精确的映射，我们的 <code>GPT</code> 类现在已经“注入”了 OpenAI 训练了数月的智慧。</p>
        <p>加载完成后，我们可以简单地打印一条成功消息。这标志着我们已经成功构建了一个与官方 GPT-2 语义等价的 PyTorch 实现。现在，这个模型不再是一个空壳，它已经具备了预测下一个词元的能力。为了验证这一点，我们需要实现前向传递逻辑，并尝试让模型生成一些文本。这是每一个复现项目中最令人兴奋的时刻之一。</p>
    </section>

    <section>
        <h2>实现前向传递以获取 Logits</h2>
        <img src="frames/00_30_40.jpg" alt="implementing the forward pass to get logits">
        <p class="caption">图 7：从词嵌入到预测概率的完整前向路径</p>
        <p><span class="timestamp">[00:31:00]</span> 前向传递（Forward Pass）是将输入数据转化为预测结果的过程。在 <code>forward</code> 方法中，我们首先将输入的词元 ID 转换为词嵌入向量，并加上位置嵌入向量。这两个嵌入的结合为模型提供了词元的语义信息及其在句子中的位置信息。接着，数据进入 12 层 Transformer 块的循环处理。每一层都会通过注意力和 MLP 进一步提炼特征，并不断更新残差流中的数值。</p>
        <p>在通过所有 Transformer 块后，数据会经过最后的一层 LayerNorm。这一步非常关键，因为它将残差流中可能已经发散的数值重新归一化到合理的范围内。最后，我们使用 <code>lm_head</code>（语言模型头）将特征向量映射回 50257 维的词汇表空间。输出的张量形状为 <code>(B, T, vocab_size)</code>，其中每个元素代表了在特定位置出现特定词元的原始得分，即 Logits。</p>
        <p>在推理模式下，我们通常只需要 Logits。但在训练模式下，我们还需要计算损失函数。为了使 <code>forward</code> 方法更加通用，我们添加了一个可选的 <code>targets</code> 参数。如果提供了目标值，我们会在方法内部直接计算交叉熵损失（Cross Entropy Loss）。这种封装方式非常符合 PyTorch 的习惯，也方便了后续的训练循环编写。通过这种方式，我们的模型既可以用于生成文本，也可以用于自我提升。</p>
        <p>现在，我们可以通过输入一个简单的张量来测试这个方法。如果一切正常，模型应该能返回一个形状正确的 Logits 张量。即使模型目前还没有经过训练，其输出的 Logits 也应该具有一定的统计特性（例如，均值接近于零）。有了这个基础，我们就可以编写采样逻辑，看看这个加载了官方权重的模型到底能“说”出什么样的话来。</p>
    </section>

    <section>
        <h2>采样初始化、前缀词元与分词</h2>
        <img src="frames/00_33_05.jpg" alt="sampling init, prefix tokens, tokenization">
        <p class="caption">图 8：使用 Tiktoken 进行文本编码与采样准备</p>
        <p><span class="timestamp">[00:33:31]</span> 为了让模型生成文本，我们首先需要一个分词器（Tokenizer）。GPT-2 使用的是字节对编码（BPE）算法，OpenAI 发布了一个名为 <code>tiktoken</code> 的高效实现。我们将使用 <code>gpt2</code> 编码方案。分词器的作用是将人类可读的字符串转换为模型能理解的整数序列。例如，字符串 "Hello, I'm a language model," 会被切分成若干个词元 ID，这些 ID 构成了我们生成的“种子”或“前缀”。</p>
        <p>我们准备了五个完全相同的初始序列，并将它们堆叠成一个批次。这样做是为了展示语言模型的随机性：即使输入完全相同，通过不同的采样过程，模型也能生成截然不同的续写。我们将这些 ID 转换为 PyTorch 张量，并移动到正确的计算设备（CPU 或 GPU）。在开始生成之前，我们将模型设置为 <code>eval()</code> 模式，这会关闭某些仅在训练时使用的特性（虽然在 GPT-2 中这主要影响 Dropout）。</p>
        <p>采样过程通常从一个固定的随机种子开始，以确保结果的可重复性。我们将最大生成长度设为 30 个词元。在每一步生成中，模型都会接收当前已有的序列，预测下一个词元的 Logits。我们只关注序列中最后一个位置的输出，因为那才是我们想要预测的“未来”。这种逐步推进的方式虽然在计算上有些冗余，但它是自回归生成的标准流程。</p>
        <p>为了获得更好的生成质量，我们通常会对 Logits 进行一些处理，比如 Softmax 归一化。但在最简单的实现中，我们可以直接选取概率最大的词元（贪婪搜索），或者根据概率分布进行随机采样。在本次演示中，我们将使用 <code>top-k</code> 采样，这是一种平衡生成多样性和逻辑连贯性的常用技巧。现在，前缀已经准备好，分词器也已就绪，让我们进入核心的采样循环。</p>
    </section>

    <section>
        <h2>采样循环</h2>
        <p><span class="timestamp">[00:37:02]</span> 采样循环是生成文本的引擎。在循环的每一次迭代中，我们都会将当前的 ID 序列 <code>x</code> 输入模型。由于模型在 <code>forward</code> 方法中会处理整个序列，我们只需要提取最后一个时间步的 Logits。这个 Logits 向量包含了词汇表中所有 50257 个词元的得分。接着，我们应用 <code>F.softmax</code> 将得分转换为概率分布，确保所有概率之和为 1。</p>
        <p>为了避免模型生成过于离谱的词元，我们采用了 <code>top-k</code> 过滤。具体来说，我们只保留概率最高的前 50 个词元，将其他词元的概率设为零，并重新归一化。这一步非常重要，因为它能有效防止模型在长尾分布中随机游走，从而产生“胡言乱语”。然后，我们使用 <code>torch.multinomial</code> 根据这个精简后的分布抽取一个词元 ID。这个抽取的 ID 就是模型预测的下一个词。</p>
        <p>新生成的词元会被拼接到原有的序列 <code>x</code> 中，作为下一次迭代的输入。随着循环的进行，序列会不断增长，直到达到我们预设的 30 个词元的上限。在这个过程中，你会发现模型的计算开销在逐渐增加，因为序列越长，Transformer 需要处理的注意力矩阵就越大。这就是为什么长文本生成在计算上非常昂贵的原因之一。</p>
        <p>当循环结束时，我们得到了五个长度为 30 的整数序列。最后一步是使用 <code>tiktoken</code> 的 <code>decode</code> 方法将这些 ID 转换回人类可读的文本。当你看到屏幕上跳出“Hello, I'm a language model, and I'm here to help you...”这样流畅的句子时，你会意识到，我们的 PyTorch 实现已经完美继承了官方 GPT-2 的灵魂。这证明了我们对模型架构和权重加载的理解是完全正确的。</p>
    </section>

    <section>
        <h2>采样与自动检测设备</h2>
        <img src="frames/00_41_50.jpg" alt="sample, auto-detect the device">
        <p class="caption">图 9：实现跨平台硬件支持（CPU/CUDA/MPS）</p>
        <p><span class="timestamp">[00:41:47]</span> 为了让我们的代码更具通用性，我们实现了一个简单的设备自动检测逻辑。深度学习代码应该能在任何环境下运行，无论是配备 NVIDIA GPU 的服务器，还是搭载 Apple Silicon 的 MacBook，甚至是只有 CPU 的普通笔记本。我们通过检查 <code>torch.cuda.is_available()</code> 和 <code>torch.backends.mps.is_available()</code> 来决定使用 <code>cuda</code>、<code>mps</code> 还是默认的 <code>cpu</code>。</p>
        <p>这种自动检测机制不仅方便了开发者，也优化了运行效率。例如，在 A100 GPU 上，模型的推理速度会比 CPU 快上百倍。在代码中，我们只需一行 <code>device = "cuda" if ...</code> 就能锁定目标。之后，在创建张量或初始化模型时，我们统一使用 <code>.to(device)</code>。这种一致性是避免“张量不在同一设备”错误的最佳实践。</p>
        <p>在采样过程中，设备的选择尤为关键。虽然单次推理在 CPU 上尚可接受，但如果我们要进行大规模生成或后续的训练，GPU 是必不可少的。我们还注意到，在不同设备上运行可能会导致微小的数值差异，这是由于底层算子实现的精度处理不同造成的。但对于生成文本这种任务，这些差异通常不会影响最终的感官质量。</p>
        <p>通过这个环节，我们确保了代码的健壮性。现在，无论你手头有什么样的硬件，你都可以运行这个 GPT-2 复现程序。我们已经成功验证了推理路径，这意味着我们已经掌握了如何使用一个训练好的模型。接下来，我们将进入最具挑战性的阶段：从零开始训练我们自己的模型，让它从随机噪声中学习人类语言的规律。</p>
    </section>

    <section>
        <h2>开始训练：数据批次 (B, T) → Logits (B, T, C)</h2>
        <img src="frames/00_45_35.jpg" alt="let’s train: data batches (B,T) → logits (B,T,C)">
        <p class="caption">图 10：训练数据流的维度变换过程</p>
        <p><span class="timestamp">[00:45:50]</span> 训练一个语言模型的第一步是理解数据的流动。我们不再使用预训练权重，而是将模型初始化为随机状态。这意味着 <code>lm_head</code> 输出的 Logits 最初将是完全随机的。训练的目标是通过不断的迭代，调整模型参数，使得模型对真实文本序列的预测概率最大化。我们使用“微型莎士比亚”数据集作为起步，因为它体积小、易于调试。</p>
        <p>数据以批次（Batch）的形式输入模型。一个批次的形状是 <code>(B, T)</code>，其中 <code>B</code> 是批次大小（同时处理的序列数），<code>T</code> 是序列长度。例如，如果我们设置 <code>B=4, T=32</code>，模型一次会看到 4 条长度为 32 的文本片段。模型的工作是针对每个位置的词元，预测它后面的下一个词元。这是一个典型的监督学习任务，虽然数据本身是无标签的文本，但我们通过“自监督”的方式创造了标签。</p>
        <p>在前向传递中，模型将 <code>(B, T)</code> 的整数张量转换为 <code>(B, T, C)</code> 的浮点张量，其中 <code>C</code> 是词汇表的大小。这个过程涉及了复杂的矩阵运算和注意力交互。每一个位置 <code>t</code> 的输出向量都试图预测位置 <code>t+1</code> 的词元。这种预测是并行的，即模型在一次前向传递中同时计算了 32 个位置的预测结果。这种并行性是 Transformer 优于循环神经网络（RNN）的核心优势之一。</p>
        <p>为了训练，我们还需要准备“目标”张量 <code>Y</code>。<code>Y</code> 实际上就是输入 <code>X</code> 向左平移一个位置的结果。例如，如果输入是 "To be or not to"，目标就是 "o be or not to be"。通过对比模型输出的 Logits 和真实的目标 <code>Y</code>，我们可以计算出损失值。理解这种 <code>(B, T)</code> 到 <code>(B, T, C)</code> 的维度变换是掌握语言模型训练逻辑的关键。接下来，我们将详细讨论如何计算交叉熵损失。</p>
    </section>

    <section>
        <h2>交叉熵损失 (Cross Entropy Loss)</h2>
        <img src="frames/00_52_55.jpg" alt="cross entropy loss">
        <p class="caption">图 11：计算预测 Logits 与真实标签之间的交叉熵</p>
        <p><span class="timestamp">[00:52:53]</span> 交叉熵损失是衡量模型预测好坏的标准尺。它本质上是衡量两个概率分布之间的差异：一个是模型输出的概率分布，另一个是真实词元的“独热编码”（One-hot encoding）分布。损失值越低，代表模型对正确词元的预测概率越高。在 PyTorch 中，<code>F.cross_entropy</code> 函数能高效地完成这个计算，但它对输入的形状有严格要求。</p>
        <p>我们的 Logits 形状是 <code>(B, T, C)</code>，而目标 <code>Y</code> 的形状是 <code>(B, T)</code>。PyTorch 的交叉熵函数期望输入是 <code>(N, C)</code> 形式，其中 <code>N</code> 是样本总数。因此，在计算损失之前，我们必须将 Logits 和 <code>Y</code> 压平。我们将 Logits 变形为 <code>(B*T, C)</code>，将 <code>Y</code> 变形为 <code>(B*T,)</code>。这种“压平”操作将整个批次的所有位置视为独立的分类任务，从而简化了计算逻辑。</p>
        <p>在模型刚初始化时，我们可以通过数学推导预估初始损失值。由于词汇表大小为 50257，且初始预测是均匀分布的，每个词元的概率约为 <code>1/50257</code>。交叉熵损失等于概率的负对数，即 <code>-ln(1/50257) ≈ 10.82</code>。如果我们的代码运行后输出的初始损失接近 11，那就说明我们的模型架构和损失计算逻辑是正确的。这是一种非常有效的“合理性检查”（Sanity Check）。</p>
        <p>计算出损失后，PyTorch 会自动构建计算图。通过调用 <code>loss.backward()</code>，梯度会沿着网络反向传播，计算出每个参数对损失的影响。这是模型学习的基石。在接下来的优化循环中，我们将使用这些梯度来更新权重，逐步降低损失值。现在，我们已经准备好进入真正的训练阶段，先从过拟合一个微小批次开始测试。</p>
    </section>

    <section>
        <h2>优化循环：过拟合单个批次</h2>
        <img src="frames/00_56_25.jpg" alt="optimization loop: overfit a single batch">
        <p class="caption">图 12：通过过拟合小规模数据验证训练流水线的正确性</p>
        <p><span class="timestamp">[00:56:42]</span> 在进行大规模预训练之前，一个经典的调试技巧是尝试“过拟合单个批次”。我们选取一个极小的训练集（比如只有 4 条序列），并让模型反复在这些数据上进行训练。如果模型架构正确且优化器配置得当，损失值应该会迅速下降，最终趋近于零。这意味着模型已经完美“背诵”了这几条序列。如果损失值不下降或波动剧烈，那通常意味着代码中存在严重的 Bug。</p>
        <p>我们选择使用 AdamW 优化器，这是目前训练 Transformer 的标准选择。AdamW 在 Adam 的基础上改进了权重衰减（Weight Decay）的实现，能提供更好的正则化效果。我们将学习率设为 <code>3e-4</code>，这是一个非常稳健的初始值。在优化循环中，我们遵循“梯度清零 -> 前向传递 -> 计算损失 -> 反向传播 -> 优化器步进”的五步法。每一步都至关重要，漏掉任何一步都会导致训练失败。</p>
        <p>在运行过程中，我们会实时打印迭代步数和损失值。观察损失值从 11 左右开始，平稳地下降到 8、6、4 甚至更低，会给你极大的信心。这证明了我们的梯度流是通畅的，模型确实在学习。过拟合测试完成后，我们会发现模型生成的文本开始变得有意义——它能完美复现我们给它的那几句莎士比亚台词。这说明我们的“学习机器”已经转动起来了。</p>
        <p>然而，过拟合单个批次只是第一步。真正的目标是让模型在海量数据上学习通用的语言规律，而不是死记硬背。为了实现这一点，我们需要一个更强大的数据加载器，能够源源不断地为模型提供新鲜的文本片段。接下来，我们将构建一个 <code>DataLoaderLite</code> 类，为真正的预训练做好准备。这种从简单到复杂的开发流程是处理大型 AI 项目的最佳实践。</p>
    </section>

    <section>
        <h2>轻量级数据加载器 (Data Loader Lite)</h2>
        <img src="frames/01_02_05.jpg" alt="data loader lite">
        <p class="caption">图 13：设计高效的文本分批读取机制</p>
        <p><span class="timestamp">[01:02:00]</span> 真正的训练需要处理数以亿计的词元，因此我们需要一个高效的数据加载机制。我们的 <code>DataLoaderLite</code> 类旨在以最小的内存开销读取大型文本文件。它首先将整个文本文件加载并分词，存储为一个巨大的整数数组。接着，它维护一个当前指针，每次调用 <code>next_batch()</code> 时，它会从当前位置切下 <code>B * T + 1</code> 个词元，并将其整理成输入 <code>X</code> 和目标 <code>Y</code>。</p>
        <p>为什么要取 <code>B * T + 1</code> 个词元呢？这是因为目标 <code>Y</code> 是输入 <code>X</code> 的右移版本，所以我们需要额外的一个词元来作为最后一个位置的预测目标。这种设计非常巧妙，它允许我们通过简单的切片操作就完成批次的构建。当指针到达文件末尾时，加载器会自动重置到开头，开启下一个训练周期（Epoch）。这种循环机制确保了训练可以无限制地进行下去。</p>
        <p>为了进一步优化，我们确保加载器返回的张量直接位于正确的设备上。在多 GPU 训练的环境下，我们还需要对加载器进行修改，让每个进程读取数据的不同部分。这被称为“分片”（Sharding）。通过简单的步进逻辑（<code>current_position += B * T * num_processes</code>），我们可以确保不同的 GPU 不会看到重复的数据，从而最大化训练的多样性和效率。</p>
        <p>有了这个轻量级加载器，我们就可以告别“过拟合单个批次”的阶段，开始真正的探索。模型现在会接触到莎士比亚全集中的每一行文字。你会观察到损失值的下降速度变慢了，这是因为任务变得更难了——模型需要学习更通用的模式。此时，任何微小的架构优化都会对最终结果产生显著影响。接下来，我们将讨论一个能减少 30% 参数量的优化：参数共享。</p>
    </section>

    <section>
        <h2>参数共享：WTE 与 LM_Head</h2>
        <img src="frames/01_06_15.jpg" alt="parameter sharing wte and lm_head">
        <p class="caption">图 14：权重绑定技术及其在减少参数量中的作用</p>
        <p><span class="timestamp">[01:06:14]</span> 在 GPT-2 的架构中，有一个非常聪明的设计：词嵌入矩阵（WTE）和输出层线性头（LM Head）共享完全相同的权重。这意味着模型在“理解词义”和“预测词语”时使用的是同一套表征。在 124M 模型中，这个矩阵的大小是 50257x768，占据了约 4000 万个参数。通过共享，我们直接节省了近 30% 的参数量，这不仅减少了显存占用，还加快了收敛速度。</p>
        <p>为什么这样做是合理的呢？从语义角度看，如果两个词在含义上相似，它们在嵌入空间中应该靠得很近；同样地，在预测时，如果模型倾向于输出其中一个词，那么它也应该具有相似的概率输出另一个词。这种对称性使得 WTE 和 LM Head 的功能高度重合。2017 年的一篇论文正式论证了这种“权重绑定”（Weight Tying）的有效性，并被 GPT 系列模型广泛采用。</p>
        <p>在 PyTorch 中实现这一点非常简单。我们只需一行代码：<code>self.transformer.wte.weight = self.lm_head.weight</code>。这实际上是让两个变量指向内存中的同一个张量。在反向传播时，这个共享张量会同时接收来自嵌入层和输出层的梯度，并进行累加更新。这种“梯度合力”使得词向量的训练更加充分，尤其是在处理低频词元时效果显著。</p>
        <p>这种优化体现了深度学习中“归纳偏置”（Inductive Bias）的力量。通过在架构层面引入合理的约束，我们可以通过更少的参数达到更好的效果。在我们的 124M 复现中，这一改动将总参数量从约 1.6 亿降低到了 1.24 亿，这正是“124M”这个名字的由来。现在，我们的模型更加精炼，准备好迎接更复杂的初始化挑战。</p>
    </section>

    <section>
        <h2>模型初始化：标准差 0.02 与残差缩放</h2>
        <img src="frames/01_13_50.jpg" alt="model initialization: std 0.02, residual init">
        <p class="caption">图 15：精细化权重初始化对深度网络稳定性的影响</p>
        <p><span class="timestamp">[01:13:47]</span> 深度神经网络的初始化是一门艺术。如果权重初始值太大，激活值会爆炸；如果太小，信号会逐渐消失。GPT-2 论文中提到，所有权重都应从均值为 0、标准差为 0.02 的正态分布中采样。这个 0.02 是经过精心挑选的，它大致对应于 <code>1/sqrt(d_model)</code>，能确保信号在多层网络中传递时保持方差稳定。</p>
        <p>然而，残差连接的存在引入了一个特殊问题。由于每个块都会将自己的贡献加到残差流中，随着层数的增加，残差流的方差会线性增长。为了抵消这种增长，GPT-2 提出了一种特殊的缩放方法：对于位于残差路径末端的线性层（即每个块中的输出投影层），其初始化标准差应除以层数的平方根，即 <code>0.02 / sqrt(2 * n_layer)</code>。这里的 <code>2</code> 是因为每个块有两个主要的残差分支（注意力和 MLP）。</p>
        <p>这种缩放机制确保了在初始化时，残差流的增长受到抑制，使得深层网络在训练初期表现得像浅层网络一样稳定。我们在代码中通过 <code>_init_weights</code> 方法实现了这一逻辑。我们会遍历所有子模块，根据它们的类型（线性层或嵌入层）应用不同的初始化规则。对于那些需要特殊缩放的层，我们通过一个特殊的标记位来识别它们。这种对细节的追求是复现顶级模型的标志。</p>
        <p>通过正确的初始化，我们的模型现在不仅在数学上是正确的，在动力学上也是健康的。你会发现，即使不使用复杂的归一化技术，模型在训练初期的损失下降也会更加平稳。现在，我们的 GPT-2 已经完全准备就绪。但在按下“运行”键之前，我们必须解决另一个核心问题：速度。目前的训练速度太慢了，我们需要利用现代 GPU 的黑科技来加速它。</p>
    </section>

    <section>
        <h2>第二部分：让它飞起来。GPU 与混合精度</h2>
        <img src="frames/01_22_20.jpg" alt="SECTION 2: Let’s make it fast. GPUs, mixed precision, 1000ms">
        <p class="caption">图 16：开启硬件加速与混合精度训练的优化之路</p>
        <p><span class="timestamp">[01:22:18]</span> 欢迎来到优化的世界。目前我们的训练循环每步耗时约 1000 毫秒，这对于大规模预训练来说是不可接受的。要让训练变快，第一步是理解硬件的局限。现代 GPU（如 A100）拥有惊人的算力，但如果我们只使用标准的 <code>float32</code> 精度，我们实际上是在浪费一半以上的潜力。<code>float32</code> 虽然精确，但它占用的内存多，且计算速度慢。</p>
        <p>混合精度训练（Mixed Precision）是解决这一问题的银弹。其核心思想是：在计算密集的矩阵乘法中使用低精度（如 <code>float16</code> 或 <code>bfloat16</code>），而在需要高精度的累加操作中保留 <code>float32</code>。这就像是在做粗略计算时用四舍五入，只在最后结算时用精确数字。通过这种方式，我们不仅能节省显存，还能触发 GPU 内部的特殊硬件单元——张量核心（Tensor Cores）。</p>
        <p>我们将重点介绍 <code>bfloat16</code> 格式。与传统的 <code>float16</code> 不同，<code>bfloat16</code> 拥有与 <code>float32</code> 相同的指数范围，只是牺牲了尾数的精度。这意味着它不容易发生数值溢出，因此在训练时不需要复杂的梯度缩放（Gradient Scaling）。这对于 Transformer 这种深层架构来说非常友好。只需在代码中加入 <code>torch.autocast</code> 上下文管理器，PyTorch 就会自动帮我们处理这些繁琐的转换。</p>
        <p>开启混合精度后，你会立即观察到速度的提升。但这只是开始。我们还将探讨如何减少内存带宽的压力，以及如何利用编译器优化来融合零散的操作。优化的过程就像是在挤海绵，每一滴水的挤出都代表着更短的训练时间和更低的成本。现在，让我们深入探讨 NVIDIA GPU 的核心科技：TF32 精度。</p>
    </section>

    <section>
        <h2>张量核心、代码计时与 TF32 精度</h2>
        <img src="frames/01_28_15.jpg" alt="Tensor Cores, timing the code, TF32 precision, 333ms">
        <p class="caption">图 17：利用 Tensor Cores 和 TF32 实现 3 倍速提升</p>
        <p><span class="timestamp">[01:28:14]</span> 在 NVIDIA Ampere 架构（如 A100）中，引入了一种名为 TF32（Tensor Float 32）的新格式。TF32 是一种神奇的格式：它在内部计算时使用 19 位精度，但在输入和输出时仍然保持 32 位。最棒的是，它不需要你修改任何一行神经网络代码，只需设置一个全局开关：<code>torch.set_float32_matmul_precision('high')</code>。这就像是给 GPU 换上了高性能燃油。</p>
        <p>通过启用 TF32，我们的训练速度从 1000 毫秒骤降到了约 333 毫秒，实现了近 3 倍的加速！这种提升几乎是“免费”的。张量核心能以极高的并行度处理 4x4 的小矩阵乘法，而 TF32 正是触发这一机制的关键。虽然精度略有下降，但对于深度学习这种对噪声具有高度容忍性的任务来说，这种权衡是极其划算的。事实上，几乎所有现代大模型的预训练都依赖于这种技术。</p>
        <p>为了准确衡量这些优化，我们必须建立严谨的计时机制。在异步执行的 GPU 上，简单的 <code>time.time()</code> 是不准确的。我们必须使用 <code>torch.cuda.synchronize()</code> 来确保在计时开始和结束时，GPU 上的所有任务都已经真正完成。通过这种方式，我们可以客观地看到每一个优化动作带来的毫秒级改进。这种对性能的量化分析是高级 AI 工程师必备的技能。</p>
        <p>我们还引入了“每秒处理词元数”（Tokens per second）作为核心指标。这比单纯的“每步耗时”更能反映训练的真实吞吐量。目前，我们的吞吐量已经有了质的飞跃。但我们不会止步于此。接下来，我们将探讨如何通过 <code>bfloat16</code> 进一步降低内存带宽压力，并尝试挑战 300 毫秒的大关。优化的旅程才刚刚进入高潮。</p>
    </section>

    <!-- 这里的圆桌话题 20, 1, 9, 10, 19, 32 属于视频中关于 TFLOPS 计算和硬件性能的深度讨论，合并为一个深度技术分析段落 -->
    <section>
        <h2>深度分析：TFLOPS 计算与硬件利用率</h2>
        <img src="frames/01_38_15.jpg" alt="Hardware performance analysis">
        <p class="caption">图 18：计算理论峰值与实际吞吐量的差距</p>
        <p><span class="timestamp">[01:34:40]</span> 作为一个追求卓越的工程师，我们不能仅仅满足于“变快了”，我们必须知道“有多快”以及“为什么不能更快”。A100 GPU 在 TF32 模式下的理论峰值算力是 156 TFLOPS。这意味着它每秒可以进行 156 万亿次浮点运算。通过简单的数学计算，我们可以估算出训练 GPT-2 每步所需的计算量（FLOPs），并将其与实际耗时对比，从而得出硬件利用率。</p>
        <p>目前的利用率大约只有 30% 到 40%。剩下的算力去哪了？答案通常隐藏在内存带宽中。虽然张量核心计算速度极快，但将数据从显存（HBM）搬运到计算单元（SRAM）的速度却相对较慢。如果我们的操作太零散，GPU 就会花大量时间在搬运数据上，而不是在计算上。这就是所谓的“访存受限”（Memory Bound）问题。为了解决它，我们需要更激进的优化手段。</p>
        <p>我们讨论了 7 TFLOPS 到 5 TFLOPS 的性能波动，这通常与内核启动开销和数据对齐有关。在处理 1.24 亿参数这种规模的模型时，由于矩阵维度相对较小，硬件往往无法达到满载状态。这就好比用一辆重型卡车去运送一小盒快递，效率自然不高。通过增加批次大小或使用更高级的编译器技术，我们可以更好地填满 GPU 的计算槽位。</p>
        <p>这种底层的性能剖析虽然枯燥，但它是区分平庸与顶尖开发者的分水岭。理解了 TFLOPS 和内存带宽的关系，你就能明白为什么某些架构设计在理论上很美，但在实际硬件上却跑不动。这种“软硬结合”的视角将指导我们接下来的优化方向：使用 <code>torch.compile</code> 进行内核融合。现在，让我们先看看 <code>bfloat16</code> 能为我们带来什么。</p>
    </section>

    <section>
        <h2>速度优化：Bfloat16 (BF16)</h2>
        <img src="frames/01_43_50.jpg" alt="Speed Optimization: Bfloat16 (BF16)">
        <p class="caption">图 19：Bfloat16 格式对内存带宽的节省</p>
        <p><span class="timestamp">[01:39:38]</span> 混合精度训练的下一步是引入 <code>bfloat16</code> (BF16)。相比于 TF32，BF16 是真正的 16 位存储格式，这意味着它能将内存带宽消耗直接减半。在我们的训练循环中，通过将 <code>autocast</code> 的 <code>dtype</code> 设置为 <code>torch.bfloat16</code>，模型在计算注意力矩阵和 MLP 激活值时会自动使用这种更轻量的格式。这不仅加快了计算速度，还让我们能训练更大的批次。</p>
        <p>BF16 的精妙之处在于它对浮点数结构的重新分配。它保留了 8 位指数位（与 <code>float32</code> 相同），这保证了它能表示非常大或非常小的数值，从而避免了 <code>float16</code> 常见的溢出问题。虽然它只有 7 位尾数位，精度较低，但深度学习模型通常对这种“舍入误差”非常鲁棒。事实上，Google 的 TPU 和 NVIDIA 的新一代 GPU 都将 BF16 作为首选的训练格式。</p>
        <p>在我们的实验中，切换到 BF16 后，每步耗时进一步缩短到了约 300 毫秒。虽然提升幅度看似不如 TF32 那么惊人，但它为后续的优化打下了坚实的基础。因为现在数据搬运的压力减小了，GPU 有更多的余力去处理复杂的逻辑。需要注意的是，权重参数本身仍然建议以 <code>float32</code> 存储，以保证更新时的微小增量不会被舍入掉。这就是“混合”精度的真正含义。</p>
        <p>目前，我们已经通过改变数据格式榨取了大量的性能。但我们的代码仍然是由 Python 解释器一行行调度的，这产生了不少开销。每一个小操作（如加法、激活函数）都会触发一次 GPU 内核的启动。有没有办法将这些零散的操作合并成一个大操作呢？答案就是 PyTorch 2.0 的杀手锏：<code>torch.compile</code>。让我们见证奇迹的发生。</p>
    </section>

    <section>
        <h2>torch.compile：Python 开销与内核融合</h2>
        <img src="frames/01_47_50.jpg" alt="torch.compile, Python overhead, kernel fusion, 130ms">
        <p class="caption">图 20：使用 torch.compile 消除 Python 解释器瓶颈</p>
        <p class="timestamp">[01:48:15] <span class="timestamp">这是本次优化中最令人震撼的一步。只需一行代码 <code>model = torch.compile(model)</code>，PyTorch 就会启动一个复杂的编译器后端，分析你的整个模型图。它会识别出那些可以合并的操作，并自动生成高度优化的 Triton 内核。通过消除 Python 解释器的逐行调度开销，并实现深度的内核融合（Kernel Fusion），训练速度实现了质的飞跃。</span></p>
        <p>什么是内核融合？简单来说，如果你有连续的三个操作：乘法、加法和 GELU，传统的做法是读写三次显存。而融合后的内核只需读取一次数据，在芯片内部完成所有计算，再写回一次。由于显存访问速度远慢于计算速度，这种减少读写次数的操作能带来巨大的性能红利。这就像是把三趟短途快递合并成了一趟，极大地节省了路上的时间。</p>
        <p>运行 <code>torch.compile</code> 后，我们的每步耗时从 300 毫秒惊人地缩短到了 130 毫秒！性能提升了 2.3 倍。此时，我们的累计加速比已经超过了 7 倍。需要注意的是，第一次迭代会非常慢，因为编译器正在后台进行繁重的分析和编译工作。但一旦编译完成，后续的迭代将以全速运行。这种“先苦后甜”的机制是现代高性能 AI 框架的共同特征。</p>
        <p>通过 <code>torch.compile</code>，我们不仅获得了速度，还简化了代码。我们不再需要手动编写复杂的 CUDA 内核，编译器帮我们处理了一切。现在，我们的 GPT-2 已经跑得非常快了，但还有一个顽固的性能瓶颈隐藏在自注意力机制中。那个巨大的 <code>(T, T)</code> 注意力矩阵仍然在消耗大量的内存和带宽。是时候祭出最终武器了：Flash Attention。</p>
    </section>

    <section>
        <h2>Flash Attention：极致加速</h2>
        <img src="frames/02_00_20.jpg" alt="flash attention, 96ms">
        <p class="caption">图 21：Flash Attention 算法及其对显存读写的优化</p>
        <p><span class="timestamp">[02:00:18]</span> Flash Attention 是近年来 Transformer 领域最重要的算法创新之一。它通过一种聪明的“分块计算”策略，彻底改变了注意力机制的计算方式。传统的注意力机制需要显式地存储一个 <code>N x N</code> 的注意力矩阵，这在序列较长时会迅速耗尽显存。而 Flash Attention 能够直接计算最终结果，而无需在显存中实例化这个中间大矩阵。</p>
        <p>这种算法的核心在于它对 GPU 内存层次结构的极致利用。它将数据划分为小块，放入高速的 SRAM 中处理，通过在线 Softmax 技巧实现结果的累加。由于避免了对慢速 HBM 的大规模读写，它的速度比传统方法快得多。在 PyTorch 中，我们只需使用 <code>F.scaled_dot_product_attention</code> 这一内置函数，它会自动在后台调用最快的 Flash Attention 实现。</p>
        <p>应用 Flash Attention 后，我们的每步耗时从 130 毫秒进一步缩短到了 96 毫秒。这意味着我们每秒可以处理超过 10 万个词元！这种极致的性能让我们在单台机器上复现 GPT-2 变得触手可及。更重要的是，Flash Attention 极大地扩展了模型处理长文本的能力，因为它将内存复杂度从二次方降低到了线性。这是现代 LLM 迈向长文本时代的通行证。</p>
        <p>至此，我们的模型在算法和硬件利用上都已经接近了极限。但作为一个追求完美的工程师，我们还有一个小细节需要处理：数字的“审美”。你会发现 50257 这个词汇表大小看起来非常刺眼，它不是 2 的幂，也不是任何硬件友好数字的倍数。这种“丑陋”的数字实际上正在暗中拖慢我们的速度。让我们通过最后一次微调，完成性能优化的最后一块拼图。</p>
    </section>

    <section>
        <h2>优美与丑陋的数字：词汇表对齐</h2>
        <p><span class="timestamp">[02:06:54]</span> 在 CUDA 编程的世界里，数字是有“审美”的。GPU 的计算单元和内存总线通常是以 32、64 或 128 为步长进行优化的。如果你的矩阵维度是这些数字的倍数，计算会非常顺滑；如果不是，硬件就不得不启动额外的“边缘处理”内核来处理零头，这会产生显著的开销。GPT-2 的词汇表大小 50257 就是一个典型的“丑陋”数字，因为它是一个奇数，且没有任何良好的对齐特性。</p>
        <p>为了优化这一点，我们将词汇表大小从 50257 增加到了 50304。为什么是 50304？因为它能被 128 整除（50304 = 128 * 393）。虽然我们实际上增加了计算量（模型现在需要预测更多不存在的词元），但由于触发了更高效的硬件对齐路径，实际运行速度反而变快了。在我们的测试中，这一改动让每步耗时从 96 毫秒降低到了 93 毫秒，提升了约 4%。</p>
        <p>这种“多即是快”的悖论在高性能计算中屡见不鲜。那些多出来的词元永远不会在训练数据中出现，因此模型很快就会学会给它们分配极低的概率。这不会影响模型的最终效果，却能实实在在地缩短训练时间。这种对底层硬件特性的深刻洞察，正是区分普通开发者与资深架构师的关键。现在，我们的模型在硬件上已经跑到了极致，是时候关注训练的科学性了。</p>
        <p>通过这一系列的优化，我们将单步训练时间从 1000 毫秒缩短到了 93 毫秒，实现了超过 10 倍的加速。这意味着原本需要训练一周的模型，现在只需一天就能完成。这种效率的提升不仅是技术上的胜利，更是研究成本的巨大节省。接下来，我们将进入第三部分，探讨如何配置科学的超参数，确保模型不仅跑得快，而且学得好。</p>
    </section>

    <section>
        <h2>第三部分：超参数、AdamW 与梯度裁剪</h2>
        <img src="frames/02_15_00.jpg" alt="SECTION 3: hyperpamaters, AdamW, gradient clipping">
        <p class="caption">图 22：配置科学的优化器参数与训练策略</p>
        <p><span class="timestamp">[02:14:55]</span> 拥有了飞快的引擎后，我们需要一套精准的控制系统。我们将参考 GPT-3 论文中的设置，因为它们比 GPT-2 的设置更成熟且记录更详尽。首先是 AdamW 优化器的参数：我们将 <code>beta1</code> 设为 0.9，<code>beta2</code> 设为 0.95。这些参数控制了动量和二阶矩的衰减速度，对 Transformer 的收敛至关重要。此外，我们将 <code>epsilon</code> 设为 <code>1e-8</code>，以防止除零错误。</p>
        <p>梯度裁剪（Gradient Clipping）是另一个不可或缺的工具。在训练过程中，偶尔会出现巨大的梯度冲击，这可能会瞬间破坏模型已经学到的权重。通过设置 <code>max_norm=1.0</code>，我们将全局梯度的范数限制在一个合理的范围内。如果梯度太长，我们就将其等比例缩放。这就像是给电路装上了保险丝，虽然平时不起作用，但在关键时刻能防止模型“炸掉”。</p>
        <p>我们还实现了一个复杂的权重衰减逻辑。并不是所有的参数都应该被衰减：权重矩阵（如线性层和词嵌入）需要衰减以防止过拟合，但偏置项（bias）和 LayerNorm 的缩放因子则不需要。我们在 <code>configure_optimizers</code> 方法中将参数分为两组，分别应用不同的衰减策略。这种精细化的管理能让模型在保持灵活性的同时，获得更好的泛化能力。</p>
        <p>最后，我们引入了 <code>FusedAdamW</code>。这是 PyTorch 提供的一个优化版本，它将多个小的数学运算合并为一个 CUDA 内核。这不仅能带来微小的速度提升（约 4%），更重要的是它减少了 GPU 的内存占用。现在，我们的优化器已经全副武装，准备好应对长达数万次的迭代。接下来，我们将设计最关键的动态控制变量：学习率调度器。</p>
    </section>

    <section>
        <h2>学习率调度器：预热 + 余弦衰减</h2>
        <img src="frames/02_21_10.jpg" alt="learning rate scheduler: warmup + cosine decay">
        <p class="caption">图 23：学习率随训练进度的动态变化曲线</p>
        <p><span class="timestamp">[02:21:06]</span> 学习率是训练中最敏感的超参数。在训练初期，模型处于完全随机状态，巨大的梯度可能会导致训练崩溃。因此，我们采用“线性预热”（Linear Warmup）策略：在最初的 715 步中，将学习率从零线性增加到最大值 <code>6e-4</code>。这给了模型一个缓冲期，让它先学会一些简单的统计规律，再进入高强度的学习阶段。</p>
        <p>预热结束后，我们进入“余弦衰减”（Cosine Decay）阶段。学习率会按照余弦曲线逐渐降低，直到达到最大值的 10%（即 <code>6e-5</code>）。这种设计符合直觉：随着训练的进行，模型越来越接近最优解，我们需要更小的步长来进行精细调节。余弦衰减在实践中被证明优于传统的阶梯式衰减，因为它能提供更平滑的过渡，减少训练后期的震荡。</p>
        <p>我们在 <code>get_lr</code> 函数中实现了这一逻辑。它根据当前的迭代步数自动计算学习率。你会发现，即使在训练的最后阶段，我们也不会将学习率降为零，而是保持一个微小的常数值。这是为了让模型在面对海量数据时，始终保持一定的探索能力。这种动态调整机制是现代深度学习算法的精髓之一。</p>
        <p>通过在日志中打印学习率，我们可以清晰地观察到这条优美的曲线。预热、高峰、缓慢滑落，每一步都对应着模型认知的进化。现在，我们的训练方案已经非常完善了。但还有一个实际问题：如果我们想使用更大的批次大小（比如 50 万个词元），但显存放不下怎么办？我们需要引入“梯度累积”技术。</p>
    </section>

    <section>
        <h2>梯度累积 (Gradient Accumulation)</h2>
        <img src="frames/02_34_10.jpg" alt="gradient accumulation">
        <p class="caption">图 24：通过多步累积模拟超大规模批次训练</p>
        <p><span class="timestamp">[02:34:09]</span> GPT-3 论文指出，对于 124M 规模的模型，理想的批次大小是约 50 万个词元（524,288 tokens）。然而，即使是 A100 显卡，也无法一次性装下这么大的数据。为了解决这个矛盾，我们使用梯度累积。其原理很简单：我们将 50 万个词元分成许多个小块（微批次），逐个进行前向和反向传播，但不立即更新权重，而是将梯度累加起来。</p>
        <p>在我们的设置中，每个微批次包含 <code>16 * 1024 = 16,384</code> 个词元。为了达到 50 万的总量，我们需要连续进行 32 次累积步骤（32 * 16,384 = 524,288）。只有在第 32 步结束时，我们才调用 <code>optimizer.step()</code>。这种做法在数学上完全等价于一次性处理 50 万个词元，但内存占用仅为原来的 1/32。这是一种以时间换空间的精妙策略。</p>
        <p>在实现时，我们需要特别注意损失函数的归一化。由于 PyTorch 的 <code>cross_entropy</code> 默认会对微批次内的样本取平均，我们需要手动将损失值除以累积步数（32），以确保最终累加得到的梯度量级是正确的。如果忘记这一步，梯度会放大 32 倍，导致模型直接训练炸掉。这种细节处理体现了底层开发的严谨性。</p>
        <p>通过梯度累积，我们终于可以在单台机器上模拟出顶级实验室的训练规模。这不仅让我们能忠实复现论文中的实验条件，还显著提高了训练的稳定性，因为大批次意味着更准确的梯度估计。现在，我们已经万事俱备，只差最后一步：利用多块 GPU 进行并行训练。让我们进入分布式数据并行的领域。</p>
    </section>

    <section>
        <h2>分布式数据并行 (DDP)</h2>
        <img src="frames/02_46_55.jpg" alt="distributed data parallel (DDP)">
        <p class="caption">图 25：多 GPU 协同训练与梯度全归约（All-Reduce）</p>
        <p><span class="timestamp">[02:46:52]</span> 如果你幸运地拥有多块 GPU，分布式数据并行（DDP）将是你的终极武器。DDP 的核心思想是“多进程复制”：我们在每块 GPU 上启动一个独立的 Python 进程，每个进程都持有一份完全相同的模型副本。在训练时，每个进程负责读取数据的一个分片，并独立计算梯度。关键点在于：在权重更新之前，所有进程会通过网络进行一次“全归约”（All-Reduce）操作，交换并平均彼此的梯度。</p>
        <p>这种机制确保了所有 GPU 上的模型在每一步更新后仍然保持完全一致。在代码实现上，我们需要处理 <code>RANK</code>（进程编号）和 <code>WORLD_SIZE</code>（总进程数）等环境变量。我们使用 <code>torchrun</code> 命令来启动这些进程。DDP 的效率极高，因为它在反向传播的过程中就能并行地进行梯度交换，几乎不会产生额外的等待时间。</p>
        <p>在 DDP 环境下，梯度累积的逻辑需要微调。由于现在有 8 块 GPU 同时工作，每一步实际上处理了 8 倍的数据。因此，我们的累积步数可以从 32 减少到 4（4 * 8 = 32）。这种横向扩展能力是现代大模型训练的基石。此外，我们还必须确保只有主进程（Rank 0）负责打印日志和保存检查点，否则控制台会被八份重复的信息淹没。</p>
        <p>我们还处理了 DDP 中的一个微妙细节：梯度同步的开关。在累积步骤中，我们不希望进程之间频繁通信，只有在最后一步需要更新权重时才进行同步。通过使用 <code>model.no_sync()</code> 上下文管理器，我们可以精确控制通信时机，从而进一步压榨性能。现在，我们的训练系统已经达到了工业级水平，准备好迎接海量数据的洗礼。</p>
    </section>

    <section>
        <h2>数据集：GPT-2、GPT-3 与 FineWeb-Edu</h2>
        <img src="frames/03_09_55.jpg" alt="datasets used in GPT-2, GPT-3, FineWeb (EDU)">
        <p class="caption">图 26：从 WebText 到高质量教育数据的演变</p>
        <p><span class="timestamp">[03:10:21]</span> 模型的好坏很大程度上取决于它“吃”了什么。GPT-2 使用的是 WebText 数据集，这是通过抓取 Reddit 上高赞链接得到的。虽然这种方法能保证数据具有一定的人类认可度，但它仍然包含大量的噪声。到了 GPT-3 时代，Common Crawl 成为了主力，但其质量参差不齐，包含大量的垃圾邮件、广告和乱码，需要极其复杂的清洗流程。</p>
        <p>今天，我们选择使用 HuggingFace 最近发布的 FineWeb-Edu 数据集。这是一个从数万亿词元中精挑细选出来的、具有极高教育价值的子集。HuggingFace 使用 Llama-3 模型作为评分员，对网页内容进行打分，只保留那些解释清晰、逻辑严密的教育性文本。实验证明，在相同计算量下，使用 FineWeb-Edu 训练出的模型在理解能力和知识储备上远超传统数据集。</p>
        <p>为了处理这 100 亿个词元的数据集，我们编写了一个预处理脚本。它将原始文本转换为分片存储的 <code>numpy</code> 文件。每个分片包含 1 亿个词元，使用 <code>uint16</code> 格式存储以节省空间。在训练时，我们的数据加载器会随机打乱这些分片，并为每个 GPU 进程分配不同的起始位置。这种“流式读取”机制确保了我们不需要将 TB 级的数据全部载入内存。</p>
        <p>观察 FineWeb-Edu 的内容，你会发现它包含大量的科学论文、百科条目和深度博客。这种高质量的数据输入意味着模型能学到更深刻的概念，而不仅仅是模仿语言的表面统计特性。有了顶级的数据和顶级的引擎，我们已经万事俱备。现在，让我们进行最后的配置，准备开始这场跨越夜晚的训练长跑。</p>
    </section>

    <section>
        <h2>评估：HellaSwag 与启动运行</h2>
        <img src="frames/03_28_25.jpg" alt="evaluation: HellaSwag, starting the run">
        <p class="caption">图 27：使用 HellaSwag 基准测试评估模型的常识推理能力</p>
        <p><span class="timestamp">[03:28:23]</span> 仅仅观察训练损失是不够的，我们需要更客观的评估指标。我们引入了 HellaSwag 基准测试。HellaSwag 是一个挑战模型常识推理能力的测试：给定一个场景描述，模型需要从四个选项中选出最合理的后续。这对于语言模型来说非常难，因为它要求模型不仅要理解语法，还要理解物理世界和人类行为的逻辑。</p>
        <p>在训练过程中，我们每隔一段步数就会在验证集上运行一次评估。我们会计算模型在 HellaSwag 上的准确率，并观察它是否随着训练的进行而稳步提升。一个随机初始化的模型在四选一任务上的准确率应该是 25%。我们的目标是看看在训练结束后，我们的模型能否接近甚至超过原始 GPT-2 在该任务上的表现。这才是检验复现成功与否的真金白银。</p>
        <p>现在，所有的代码都已经合并，所有的优化都已经开启。我们检查了最后一遍超参数：学习率 <code>6e-4</code>，批次大小 <code>524,288</code>，训练步数 <code>19,073</code>（对应约 100 亿个词元）。我们在终端输入了 <code>torchrun</code> 命令，看着八块 GPU 同时发出轰鸣，屏幕上开始跳动起一行行训练日志。损失值在下降，吞吐量稳定在每秒 15 万个词元以上。</p>
        <p>这是一场漫长的长跑。我们将离开电脑，让模型在深夜里独自学习。这就像是种下一颗种子，然后静待它破土而出。在接下来的几个小时里，数以亿计的参数将经历数万次的微调，从混沌的随机状态逐渐演化出理解人类文明的能力。明天早上，我们将回来验收结果，看看我们亲手打造的 GPT-2 表现如何。</p>
    </section>

    <section>
        <h2>第四部分：清晨的结果！GPT-2 复现成功</h2>
        <img src="frames/03_42_55.jpg" alt="SECTION 4: results in the morning! GPT-2, GPT-3 repro">
        <p class="caption">图 28：训练曲线分析与模型生成效果展示</p>
        <p><span class="timestamp">[03:43:05]</span> 清晨回到实验室，最紧张的时刻莫过于查看训练曲线。结果令人振奋：训练损失曲线异常平滑，从最初的 11 左右一路下降到了 3.2 附近。这意味着模型已经深刻吸收了 FineWeb-Edu 数据集中的知识。更令人惊喜的是 HellaSwag 的表现：我们的模型准确率达到了约 30%，这不仅远超随机水平，甚至在同等计算量下优于当年 OpenAI 发布的官方 124M 模型。这证明了高质量数据（FineWeb-Edu）的巨大威力。</p>
        <p>我们尝试让模型生成一些文本。输入前缀 "The key to life is," 模型给出了极具哲学深度的回答，探讨了知识、教育和自我提升。输入一段 Python 代码的开头，它也能准确地补全后续的逻辑。虽然它偶尔还会出现一些幻觉或重复，但考虑到它只有 1.24 亿参数且只训练了不到一个小时，这种表现已经堪称惊艳。它已经不再是一个随机数生成器，而是一个初具雏形的智能体。</p>
        <p>通过对比实验，我们发现 <code>torch.compile</code> 和 Flash Attention 是复现成功的关键。如果没有这些优化，我们可能需要等待数天才能看到结果。此外，正确的权重初始化和学习率调度确保了模型没有在训练中途崩溃。这次复现不仅验证了 GPT-2 架构的经典性，更展示了在 2024 年，一个普通开发者利用开源工具和云端算力，就能达到几年前顶级实验室的水平。</p>
        <p>这一结果标志着我们复现任务的圆满完成。我们从最底层的矩阵乘法开始，一路构建到了分布式训练系统。我们不仅得到了一个模型，更掌握了一套完整的、工业级的 LLM 开发流水线。这种从底层到顶层的全栈理解，是任何调包侠都无法比拟的。最后，让我们向那些更极致的性能挑战者致敬，看看 <code>llm.c</code> 是如何将性能推向另一个巅峰的。</p>
    </section>

    <section>
        <h2>致敬 llm.c：纯 C/CUDA 的极致性能</h2>
        <img src="frames/03_56_00.jpg" alt="shoutout to llm.c, equivalent but faster code in raw C/CUDA">
        <p class="caption">图 29：llm.c 项目——追求极致简洁与效率的典范</p>
        <p><span class="timestamp">[03:56:21]</span> 虽然 PyTorch 已经非常快，但它仍然带有 Python 的沉重负担。如果你想追求极致的性能，我强烈推荐关注我的另一个项目：<code>llm.c</code>。在这个项目中，我们抛弃了 Python 和大型框架，完全使用纯 C 和 CUDA 重新编写了 GPT-2 的训练代码。这不仅是一个技术挑战，更是一次回归计算机科学本质的修行。</p>
        <p>在 <code>llm.c</code> 中，没有任何多余的抽象。我们手动管理内存，手动编写每一个 CUDA 内核。结果是惊人的：它的代码量极小（仅几千行），但运行速度却比优化后的 PyTorch 还要快。更重要的是，它对编译器的依赖极低，这使得它在各种嵌入式设备或超算集群上都能轻松部署。这种对效率的极致追求，正是推动 AI 技术不断边界突破的原动力。</p>
        <p>通过对比 <code>nanogpt</code> 和 <code>llm.c</code>，你可以清晰地看到不同抽象层次的权衡。PyTorch 提供了极高的开发效率和灵活性，适合快速迭代和研究；而纯 C/CUDA 则提供了无可比拟的运行效率，适合大规模生产环境。掌握了这两者，你就拥有了从实验室到生产线的全谱系战斗力。这也是我一直倡导的“深入底层”学习法的最终目标。</p>
        <p>目前 <code>llm.c</code> 社区非常活跃，许多开发者正在贡献更快的内核实现。如果你想真正理解计算机是如何处理数万亿次运算的，去读一读 <code>llm.c</code> 的源码吧。在那里，你会看到每一个比特是如何流动的，每一个时钟周期是如何被利用的。这种通透感将让你对 AI 的理解提升到一个全新的维度。现在，让我们为这段漫长的旅程画上句号。</p>
    </section>

    <section>
        <h2>总结：build-nanogpt 仓库与未来展望</h2>
        <img src="frames/03_59_20.jpg" alt="summary, phew, build-nanogpt github repo">
        <p class="caption">图 30：项目总结与开源社区贡献</p>
        <p><span class="timestamp">[03:59:39]</span> 呼，这真是一场漫长的技术马拉松。我们从一个空的 Python 文件开始，逐步构建出了一个高性能的 GPT-2 训练系统。所有的代码改动都已经以清晰的提交记录保存在了 <code>build-nanogpt</code> GitHub 仓库中。我建议你克隆这个仓库，跟随每一个 commit 去亲手实现一遍。没有什么比亲手写代码更能让你掌握这些复杂的概念了。</p>
        <p>我们学习了如何构建 Transformer 架构，如何加载官方权重，如何通过 TF32、BF16、<code>torch.compile</code> 和 Flash Attention 将训练速度提升 10 倍以上。我们还探讨了 AdamW 优化器的精妙配置，以及如何利用梯度累积和 DDP 进行大规模并行训练。最后，我们见证了高质量教育数据对模型能力的巨大提升。这些知识加在一起，构成了现代大语言模型开发的完整版图。</p>
        <p>未来是属于那些既懂算法又懂硬件的“全栈 AI 工程师”的。随着模型规模的不断扩大，如何更高效地利用计算资源将成为核心竞争力。我希望这个视频和这篇博客能成为你通往这一目标的阶梯。不要害怕那些复杂的数学公式和底层的硬件细节，当你真正弄懂它们时，你会发现它们其实非常优美且逻辑自洽。</p>
        <p>感谢你陪伴我走完这段四个小时的旅程。如果你觉得有收获，欢迎在 GitHub 上给项目点个星，或者在社区中分享你的心得。AI 的世界变化极快，但底层的原理是永恒的。保持好奇，不断探索，我们下一个视频再见！</p>
    </section>

</body>
</html>
```