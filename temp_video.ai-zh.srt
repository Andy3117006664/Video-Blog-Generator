1
00:00:00,080 --> 00:00:00,600
呃大家好

2
00:00:00,600 --> 00:00:01,320
我是永琪

3
00:00:01,320 --> 00:00:03,960
我现在是香港理工大学的研究助理教授

4
00:00:03,960 --> 00:00:08,640
然后非常感谢AI time给我们提供的这一个平台

5
00:00:08,640 --> 00:00:09,620
呃

6
00:00:09,620 --> 00:00:13,980
同时呢也感谢中国科学技术大学

7
00:00:13,980 --> 00:00:16,520
冯福利教授和王强教授的支持

8
00:00:16,520 --> 00:00:17,020
呃

9
00:00:17,020 --> 00:00:21,150
感谢张瑶博士和我一起组织这一次的呃talk

10
00:00:21,150 --> 00:00:24,329
最后也感谢呃四位speaker

11
00:00:24,329 --> 00:00:27,209
OK然后我做一个简短的opening

12
00:00:27,209 --> 00:00:31,690
就是呃因为我们是一个生存日推荐的一个专题

13
00:00:31,690 --> 00:00:33,340
可能呃

14
00:00:33,340 --> 00:00:39,830
我觉得大家可以从下面这两点去梳理和归纳呃

15
00:00:39,830 --> 00:00:42,810
我这些已有的生师推荐的方法

16
00:00:42,810 --> 00:00:44,650
第一个大家可以去关注

17
00:00:44,650 --> 00:00:49,750
就是这些生成的推荐是怎样去预测item的

18
00:00:49,750 --> 00:00:52,990
当然或者更具体而言是要预测一个item list

19
00:00:52,990 --> 00:00:56,800
OK呃通常来讲呢其实是有两种方式

20
00:00:56,800 --> 00:01:00,080
第一第一种方式就是和传统的推荐一样

21
00:01:00,080 --> 00:01:01,140
是做matching

22
00:01:01,140 --> 00:01:07,720
通常情况下大家都会要通过把用户的历史呃

23
00:01:07,720 --> 00:01:10,100
额产生embedding

24
00:01:10,100 --> 00:01:14,189
然后呢会和所有的item的信息呃

25
00:01:14,189 --> 00:01:15,629
也会把它变成embedding

26
00:01:15,629 --> 00:01:18,430
然后做一个计算它们之间的matching score

27
00:01:18,430 --> 00:01:19,370
通过这种方式呢

28
00:01:19,370 --> 00:01:22,450
就可以得到一个item的一个rank list

29
00:01:22,450 --> 00:01:23,970
OK当然了

30
00:01:23,970 --> 00:01:25,910
现在也有一种呃

31
00:01:25,910 --> 00:01:27,950
另外一种就是纯生成式的

32
00:01:27,950 --> 00:01:32,070
也就是说用一个生成式模型去生成

33
00:01:32,070 --> 00:01:35,130
某一个现有的item

34
00:01:35,130 --> 00:01:38,170
那这种通常要怎么做呢

35
00:01:38,170 --> 00:01:39,270
通常情况下来说

36
00:01:39,270 --> 00:01:42,510
他需要给每一个item一个identify

37
00:01:42,510 --> 00:01:44,250
也就是比如说最简单的

38
00:01:44,250 --> 00:01:47,130
可能每一个item有一个独一无二的

39
00:01:47,130 --> 00:01:48,390
有这么一个title

40
00:01:48,390 --> 00:01:50,840
然后你去预测出他的title出来

41
00:01:50,840 --> 00:01:53,770
就代表着你去把这个item生成出来了

42
00:01:53,770 --> 00:01:55,810
然后在这种呃方式下面

43
00:01:55,810 --> 00:01:56,810
大家也可以去关注

44
00:01:56,810 --> 00:02:02,470
这个生成是一次自回归step还是N次自回归step

45
00:02:02,470 --> 00:02:03,370
Ok

46
00:02:03,370 --> 00:02:06,390
当然这种生成式也有一个非常需要关注的一点

47
00:02:06,390 --> 00:02:09,389
就是它是怎么去产生rank list的

48
00:02:09,389 --> 00:02:14,770
OK呃其实这两种不论是matching还是GENER呃

49
00:02:14,770 --> 00:02:17,750
generative都属于生成式推荐

50
00:02:17,750 --> 00:02:23,070
现在就是呃去predict item list的方式

51
00:02:23,070 --> 00:02:23,710
呃

52
00:02:23,710 --> 00:02:25,270
一也就是说嗯

53
00:02:25,270 --> 00:02:27,390
生生推进并不一定意味着

54
00:02:27,390 --> 00:02:31,690
必须要用深层次的这种方法去预测一个呃item

55
00:02:31,690 --> 00:02:35,070
OK然后第二点大家可以去注意的就是

56
00:02:35,070 --> 00:02:38,410
这些生成式推荐的模型是怎样去训练的

57
00:02:38,410 --> 00:02:44,510
它有没有区别于之前的呃推荐方法的训呃

58
00:02:44,510 --> 00:02:46,830
区别于之前推荐系统的训练方法

59
00:02:46,830 --> 00:02:50,050
比如之前传统的都是一些判别是loss

60
00:02:50,050 --> 00:02:51,690
那现现有的生推荐

61
00:02:51,690 --> 00:02:53,650
他有没有用一些新的一些loss呀

62
00:02:53,650 --> 00:02:55,730
包括next token prediction

63
00:02:55,730 --> 00:02:58,910
包括有没有用一些新的一些训练框架

64
00:02:58,910 --> 00:03:01,410
比如说强化学习之类的

65
00:03:01,410 --> 00:03:02,130
Ok

66
00:03:02,130 --> 00:03:07,950
然后以上两点是我觉得大家在下面听他们的

67
00:03:07,950 --> 00:03:10,899
presentation的时候可以去关注的呃

68
00:03:10,899 --> 00:03:11,679
两点通过

69
00:03:11,679 --> 00:03:13,719
这样可能更好的能帮大家理解

70
00:03:13,719 --> 00:03:16,530
生存的推荐是怎样去呃运作的

71
00:03:17,610 --> 00:03:17,930
好的

72
00:03:17,930 --> 00:03:18,250
主持人

73
00:03:18,250 --> 00:03:20,030
我的欧佩林就到这了

74
00:03:20,030 --> 00:03:21,450
谢谢

75
00:03:28,900 --> 00:03:29,580
啊hello

76
00:03:29,580 --> 00:03:30,320
主持人

77
00:03:30,320 --> 00:03:32,360
我们是不是可以啊

78
00:03:32,360 --> 00:03:34,700
直接进入到第一个speak了

79
00:03:44,350 --> 00:03:46,630
瑞阳你你那个分享屏幕

80
00:03:46,630 --> 00:03:50,710
然后进行下那个你的开始你的那个PRINATION吧

81
00:03:52,320 --> 00:03:56,040
好的好的嗯

82
00:04:08,220 --> 00:04:10,960
OK感谢李老师的opening

83
00:04:10,960 --> 00:04:13,649
然后我先开个头开始介绍

84
00:04:13,649 --> 00:04:16,689
我们这边工作标题是reasoning

85
00:04:16,689 --> 00:04:17,488
React towards

86
00:04:17,488 --> 00:04:18,369
Large recommender

87
00:04:18,369 --> 00:04:20,089
Models with reasoning

88
00:04:20,089 --> 00:04:21,930
然后我先说一下

89
00:04:21,930 --> 00:04:24,570
我们当初为什么想到要去做这样一个idea

90
00:04:24,570 --> 00:04:26,970
主要是就在今年年初嘛

91
00:04:26,970 --> 00:04:29,090
在deep sk r one出来以后

92
00:04:29,090 --> 00:04:31,890
就是大家对于就是test can scaling

93
00:04:31,890 --> 00:04:34,470
也就是说让模型生成更多的reasoning

94
00:04:34,470 --> 00:04:38,480
更多的推理能够得到一个更好的performance

95
00:04:38,480 --> 00:04:41,680
这个事情呃感到非常的有兴趣

96
00:04:41,680 --> 00:04:44,710
然后我们想看到的也是这样的事情

97
00:04:44,710 --> 00:04:48,570
这样的benefit是不是能够也能在RECOMMENDER

98
00:04:48,570 --> 00:04:50,450
models上又能显出

99
00:04:50,450 --> 00:04:53,310
也就是说我们也希望推荐模型

100
00:04:53,310 --> 00:04:54,950
也能think to recommend

101
00:04:56,350 --> 00:05:00,620
然后呢想实现这个事情其实呃并不简单

102
00:05:00,620 --> 00:05:02,600
也需要一些更创新的方案

103
00:05:02,600 --> 00:05:07,080
因为其实reasoning本身在推荐系统这个领域

104
00:05:07,080 --> 00:05:09,560
也也已经被广泛使用了

105
00:05:09,560 --> 00:05:11,800
但是呢以以往的这些步骤

106
00:05:11,800 --> 00:05:13,920
它主要还是将LM

107
00:05:13,920 --> 00:05:15,800
也就是做reasoning的这个模型

108
00:05:15,800 --> 00:05:19,040
作为一个external accelery module

109
00:05:19,040 --> 00:05:21,340
就比如说他可以负责呃

110
00:05:21,340 --> 00:05:25,290
对用户的行为序列再进行一些补充和分析

111
00:05:25,290 --> 00:05:28,130
产生一些新的文字性的特征

112
00:05:28,130 --> 00:05:31,370
然后这个特征要被送进另外一个推荐模型

113
00:05:31,370 --> 00:05:34,880
然后那个推荐模型才是真正的去做推荐的

114
00:05:34,880 --> 00:05:38,120
然后这样的一个普遍的pipeline的方式呢

115
00:05:38,120 --> 00:05:39,720
会有这样一些问题

116
00:05:39,720 --> 00:05:44,350
比如说你没有办法真正的训练那个large language

117
00:05:44,350 --> 00:05:46,130
model去做呃

118
00:05:46,130 --> 00:05:48,330
为recommendation做reasoning

119
00:05:48,330 --> 00:05:52,650
你只能说通过SFT去训练他的呃

120
00:05:52,650 --> 00:05:56,810
reasoning更像一个更厉害的模型呃

121
00:05:56,810 --> 00:05:58,890
就算是两个模型要一起训

122
00:05:58,890 --> 00:06:01,250
reasoning和推reasoning和推荐模型

123
00:06:01,250 --> 00:06:02,690
那你也不能完全做到

124
00:06:02,690 --> 00:06:06,260
同时你可能只能先训一个再训另外一个

125
00:06:06,260 --> 00:06:07,460
另外一个问题呢

126
00:06:07,460 --> 00:06:10,740
就是你如果想要host这两个模型的话

127
00:06:10,740 --> 00:06:13,000
那你所占的memory也是很重的

128
00:06:13,000 --> 00:06:17,279
然后inference latency本身也是也是不太符合

129
00:06:17,279 --> 00:06:19,080
推荐系统的实施需求的

130
00:06:19,080 --> 00:06:22,320
第三个问题就是推荐数据本身是海量的

131
00:06:22,320 --> 00:06:24,260
但是对于推荐数据的分析

132
00:06:24,260 --> 00:06:26,780
这个reasoning的数据基本是没有的

133
00:06:26,780 --> 00:06:29,000
或者说是非常不优质的

134
00:06:29,000 --> 00:06:33,359
所以我们必须要在没有一个合理的reasoning

135
00:06:33,359 --> 00:06:36,499
的情况下去训练这个模型呃

136
00:06:36,499 --> 00:06:38,039
综合以上几点

137
00:06:38,039 --> 00:06:39,639
我们想要提出的就是

138
00:06:39,639 --> 00:06:42,439
首先一个unified large language model

139
00:06:42,439 --> 00:06:43,959
它既能做reasoning

140
00:06:43,959 --> 00:06:45,159
又能去做推荐

141
00:06:45,159 --> 00:06:50,670
这样就能呃让我们统一的jointly去训练这个

142
00:06:50,670 --> 00:06:53,150
reasoning to recommend目标

143
00:06:54,110 --> 00:06:57,710
然后呢我们还需要一个合理的训练方法

144
00:06:57,710 --> 00:07:00,190
能够让我们在没有reasoning annotation

145
00:07:00,190 --> 00:07:04,110
没有reasoning trace的情况下去优化这个模型

146
00:07:05,030 --> 00:07:07,150
然后先说架构方面

147
00:07:07,150 --> 00:07:09,730
这个架构其实是比较直观的

148
00:07:09,730 --> 00:07:13,390
就是我们只是在普通的large recommender model

149
00:07:13,390 --> 00:07:16,079
也就是LM的八控backbone

150
00:07:16,079 --> 00:07:18,909
上多加了一个recommendation的头

151
00:07:18,909 --> 00:07:23,189
然后呢这个头也就是说可以说是一个线性层吧

152
00:07:23,189 --> 00:07:24,609
他负责呃

153
00:07:24,609 --> 00:07:28,969
将large recommend model的hidden states的隐藏表征

154
00:07:28,969 --> 00:07:33,810
转换成呃其中一个item呃

155
00:07:33,810 --> 00:07:35,870
从另外一个角度上也可以理解为

156
00:07:35,870 --> 00:07:37,030
这个recommendation had

157
00:07:37,030 --> 00:07:39,110
它就是一堆item embedding

158
00:07:39,110 --> 00:07:44,859
然后或者说嗯我们以额identify长度为一

159
00:07:44,859 --> 00:07:47,260
去做generative recommendation

160
00:07:47,540 --> 00:07:51,180
然后呢在influence时候主要是这样一些步骤

161
00:07:51,180 --> 00:07:53,929
第一个就是我们以纯文字的形式

162
00:07:53,929 --> 00:07:58,509
Uh prompt udl m with user interaction history

163
00:07:58,509 --> 00:08:01,210
比如像图中鼠说的呃

164
00:08:01,210 --> 00:08:04,490
呃我已经买了这个这个这个呃和这个那个东西

165
00:08:04,490 --> 00:08:07,110
请你帮我推荐我的下一个purchase

166
00:08:07,110 --> 00:08:09,630
然后呢模型就会根据这些输入

167
00:08:09,630 --> 00:08:13,850
像普通的reasoning任务一样去生成一些分析呃

168
00:08:13,850 --> 00:08:18,540
去呃比如说做一些特征的提取

169
00:08:18,540 --> 00:08:19,460
文字性的

170
00:08:19,460 --> 00:08:21,880
就是比如说这个用户特别喜欢什么

171
00:08:21,880 --> 00:08:26,420
然后呢最终生成一个answer的这个这个pardon

172
00:08:26,420 --> 00:08:27,140
然后呢

173
00:08:27,140 --> 00:08:30,940
在这之后我们就使用另外一个recommendation头

174
00:08:30,940 --> 00:08:35,000
然后去找到得分最高的那个物品作为推荐

175
00:08:36,780 --> 00:08:41,068
然后说一下这个是怎么训练呃

176
00:08:41,068 --> 00:08:42,429
正如我之前所说的

177
00:08:42,429 --> 00:08:43,909
就是我们面临的一个问题

178
00:08:43,909 --> 00:08:45,569
是没有label的reason data

179
00:08:45,569 --> 00:08:47,949
所以说呢非常INTUITI里的

180
00:08:47,949 --> 00:08:50,540
就是要使用强化学习呃

181
00:08:50,540 --> 00:08:54,380
让模型自己去生成一些returing trace

182
00:08:54,380 --> 00:08:58,530
然后我们挑其中比较好的鼓励它呃

183
00:08:58,530 --> 00:09:00,150
去学习那些比较好的trace

184
00:09:00,150 --> 00:09:03,200
这样的方式去做呃优化

185
00:09:03,200 --> 00:09:06,440
然后呢这个步骤简单来说应该是分为三步

186
00:09:06,440 --> 00:09:08,630
第一个就是trajectory samply

187
00:09:08,630 --> 00:09:10,950
就是让模型根据同一个输入

188
00:09:10,950 --> 00:09:13,230
它生成不同的reasoning sequence

189
00:09:13,230 --> 00:09:15,470
然后我们可以用tok sampling

190
00:09:15,470 --> 00:09:18,650
还有temperature去控制这个随机的强度

191
00:09:18,650 --> 00:09:20,950
然后呢我们需要做reward

192
00:09:20,950 --> 00:09:23,430
现在模型生成了三四条trajectory

193
00:09:23,430 --> 00:09:24,190
有的比较好

194
00:09:24,190 --> 00:09:25,050
有的不太好

195
00:09:25,050 --> 00:09:28,560
我们要定一个规则去决定哪些是好的

196
00:09:28,560 --> 00:09:31,180
哪些应该是鼓励模型去学习的

197
00:09:31,180 --> 00:09:33,290
哪些应该不是这个就是reward

198
00:09:33,290 --> 00:09:38,539
然后第三个呢就是我们呃derive的一个joint呃

199
00:09:38,539 --> 00:09:39,939
强化学习的目标

200
00:09:39,939 --> 00:09:44,670
然后呢嗯简单来说就是把reasoning recommend

201
00:09:44,670 --> 00:09:46,610
这场序列的行为

202
00:09:46,610 --> 00:09:49,560
作为一个trace去做强化学习的训练

203
00:09:49,560 --> 00:09:50,320
然后呢

204
00:09:50,320 --> 00:09:52,880
这也意味着我们的这个损失方程

205
00:09:52,880 --> 00:09:56,020
可以和目前大部分的

206
00:09:56,020 --> 00:10:00,030
基本上是所有的强化学习的框架相融合

207
00:10:00,030 --> 00:10:00,870
呃

208
00:10:00,870 --> 00:10:03,590
我现在之后会详细讲解

209
00:10:03,590 --> 00:10:06,990
rewarding和那个our for reversement

210
00:10:06,990 --> 00:10:08,960
learning of objective这块

211
00:10:09,200 --> 00:10:12,240
我们的reward设计呢主要涵盖两个部分

212
00:10:12,240 --> 00:10:15,710
一个是discread reward和continuous reward呃

213
00:10:15,710 --> 00:10:17,990
我觉得就是在推荐这个任务上

214
00:10:17,990 --> 00:10:19,870
大家非常INTU里会想到

215
00:10:19,870 --> 00:10:22,609
就是用ranking metrics去作为reward

216
00:10:22,609 --> 00:10:24,249
你推荐的物品呃

217
00:10:24,249 --> 00:10:24,909
ranking越高

218
00:10:24,909 --> 00:10:26,569
那么这个如果是越越好的

219
00:10:26,569 --> 00:10:27,389
这个没有问题

220
00:10:27,389 --> 00:10:29,769
但是discrete reward有个问题

221
00:10:29,769 --> 00:10:33,109
就是它有可能你生成的所有trace

222
00:10:33,109 --> 00:10:34,929
它的reward是一模一样的

223
00:10:34,929 --> 00:10:36,210
因为它是discrete

224
00:10:36,210 --> 00:10:37,890
所以在这样的基础上

225
00:10:37,890 --> 00:10:41,430
为了保证嗯1even的side

226
00:10:41,430 --> 00:10:44,830
The difference between recently trees can be identified

227
00:10:44,830 --> 00:10:48,070
我们在这个基础上加了一个小小的soft

228
00:10:48,070 --> 00:10:50,660
max similarity reward呃

229
00:10:50,660 --> 00:10:54,400
这个就是完全比较呃

230
00:10:55,040 --> 00:10:59,640
哪一个就是这些items和目前这个user的相似度

231
00:10:59,640 --> 00:11:00,520
相似度越高

232
00:11:00,520 --> 00:11:02,550
那当然就是REWAR就越高

233
00:11:02,550 --> 00:11:05,450
我们把这一块rework设置的比较小

234
00:11:05,450 --> 00:11:09,829
是希望模型还是能focus在这个任务ranking

235
00:11:09,829 --> 00:11:11,709
也就是NDCG本身上

236
00:11:11,709 --> 00:11:15,460
然后这个呢主要就是为为了在模型呃

237
00:11:15,460 --> 00:11:17,740
它的trace都是相同的

238
00:11:17,740 --> 00:11:21,060
ranking下的上还是能有一些细微的区别

239
00:11:21,620 --> 00:11:24,340
然后再说一下我们的训练目标

240
00:11:24,340 --> 00:11:27,220
我先简单介绍一下强化学习本身

241
00:11:27,220 --> 00:11:28,900
它大概是什么样的概念

242
00:11:28,900 --> 00:11:31,260
就是呃如之前所说

243
00:11:31,260 --> 00:11:33,839
我们可能需要生成不同的reading trace

244
00:11:33,839 --> 00:11:38,059
每个trace呢按强化学习定义就是一个输入

245
00:11:38,059 --> 00:11:41,140
然后中间的一些action呃

246
00:11:41,140 --> 00:11:43,620
在LM的强化学习这个领域呢

247
00:11:43,620 --> 00:11:46,540
action其实也就是只生成每一个token了

248
00:11:46,540 --> 00:11:48,460
然后呢这样一个trace的话

249
00:11:48,460 --> 00:11:51,379
我们通过reward在计算advantage

250
00:11:51,379 --> 00:11:54,919
那么就可以做到鼓励这个模型向这个trace

251
00:11:54,919 --> 00:11:57,160
学习或者是不鼓励嗯

252
00:11:57,160 --> 00:11:59,020
所以我们所做的一件事情

253
00:11:59,020 --> 00:12:01,940
其实也就是在这一个sequence of action上

254
00:12:01,940 --> 00:12:04,359
最后补充了一个recommend action

255
00:12:04,359 --> 00:12:07,630
就比如就如这个这里的这个公司所说

256
00:12:07,630 --> 00:12:10,070
这个O1到O0呢就是reason的token

257
00:12:10,070 --> 00:12:14,069
最后这个呢就是推荐正确那个物品这个行为

258
00:12:14,389 --> 00:12:15,389
然后呢

259
00:12:15,389 --> 00:12:20,390
呃具体这个每个action在强化学习的损失中

260
00:12:20,390 --> 00:12:21,510
是如何表达的呢

261
00:12:21,510 --> 00:12:24,510
就是通过呃这个这个的形式

262
00:12:24,510 --> 00:12:27,070
具体而言的公式就是这样的呃

263
00:12:27,070 --> 00:12:29,170
下面这个pi theta old

264
00:12:29,170 --> 00:12:33,190
这个是supermaximmilarity of outputting the token

265
00:12:33,190 --> 00:12:35,520
This token in the sequence

266
00:12:35,520 --> 00:12:39,620
也就是这个代表的是我们用来采样的

267
00:12:39,620 --> 00:12:42,640
没有经过这一步训练的那个模型上面呢

268
00:12:42,640 --> 00:12:43,920
就是带有梯度的

269
00:12:43,920 --> 00:12:47,140
现在需要接受训练的这个模型

270
00:12:47,140 --> 00:12:50,620
然后我们所说的把这个行为放在最后呢

271
00:12:50,620 --> 00:12:54,889
其实也就是在最后这个时刻使用呃

272
00:12:54,889 --> 00:12:57,649
推荐正确物品的这个概率呃

273
00:12:57,649 --> 00:12:59,109
作为这个行为

274
00:12:59,109 --> 00:13:03,589
然后并且把这个定义放到最终的强化学习的

275
00:13:03,589 --> 00:13:05,780
额损失定义当中

276
00:13:06,100 --> 00:13:07,540
然后具体来说

277
00:13:07,540 --> 00:13:11,810
recommendation的这个probability呢是用in batch negative

278
00:13:11,810 --> 00:13:15,810
这也就意味着我们其实在训练强化学习的时候

279
00:13:15,810 --> 00:13:19,139
是有对item做embedding的

280
00:13:19,139 --> 00:13:23,869
也就是说我们会把item的一些输入信息告诉LM

281
00:13:23,869 --> 00:13:25,449
然后让它生成一个embedding

282
00:13:25,449 --> 00:13:27,309
然后对用户也是一样

283
00:13:27,309 --> 00:13:28,949
只不过在生成embedding前

284
00:13:28,949 --> 00:13:30,120
它会有一个reason

285
00:13:30,120 --> 00:13:32,520
这样我们就能得到一个in batch的

286
00:13:32,520 --> 00:13:34,180
推荐物品的概率

287
00:13:34,180 --> 00:13:36,770
并且把这个东西放到强化学习损失中

288
00:13:36,770 --> 00:13:39,070
呃同时呢经过一些后续的实验

289
00:13:39,070 --> 00:13:43,710
我们提出这样一些in periodic的较优的设定

290
00:13:43,710 --> 00:13:46,330
第一个就是用PPO的这个clip呃

291
00:13:46,330 --> 00:13:47,010
Racial loss

292
00:13:48,170 --> 00:13:53,320
第二个呢就是呃我们发现最好是用呃

293
00:13:53,320 --> 00:13:56,370
就是如果是最高的那个trajectory

294
00:13:56,370 --> 00:13:59,890
去8prose reason in的这个policy比较好

295
00:13:59,890 --> 00:14:03,110
也就是说这个这个数值只有在reward

296
00:14:03,110 --> 00:14:05,400
最高的时候才会等于一

297
00:14:05,640 --> 00:14:06,460
然后呢

298
00:14:06,460 --> 00:14:11,560
其余的次优的trajectory呢都会update to token policy

299
00:14:11,560 --> 00:14:14,870
让他学习更好的reason

300
00:14:16,740 --> 00:14:20,300
然后呢接下来是一些实验结果的报道了

301
00:14:20,300 --> 00:14:23,420
这个就是我们文章的一些主表

302
00:14:23,420 --> 00:14:25,180
然后也是在呃

303
00:14:25,180 --> 00:14:28,710
包括瑞巴特期间补上了非常多的baseline呃

304
00:14:28,710 --> 00:14:32,830
主要还是涵盖一些以lm base的large

305
00:14:32,830 --> 00:14:34,900
recommend model为主呃

306
00:14:34,900 --> 00:14:37,480
他们大部分的都是并没有带reasoning的

307
00:14:37,480 --> 00:14:40,770
包括比较著名的像LARA和big rag嗯

308
00:14:40,770 --> 00:14:42,790
其中有一个lam tune的话

309
00:14:42,790 --> 00:14:46,440
它就是我刚刚说的就是两个模型

310
00:14:46,440 --> 00:14:48,940
一个负责生成reasoning

311
00:14:48,940 --> 00:14:53,350
一个负责做推荐一个BT模型这样的价格

312
00:14:53,350 --> 00:14:54,970
然后可以看出它的效果

313
00:14:54,970 --> 00:14:57,250
其实是比如说在这个数据集上

314
00:14:57,250 --> 00:14:59,170
效果是是是很好的

315
00:14:59,170 --> 00:15:01,550
所以reasoning本身的话呃

316
00:15:01,550 --> 00:15:04,350
它也是在之前就be serified

317
00:15:04,350 --> 00:15:05,670
比较有用的一个东西

318
00:15:05,670 --> 00:15:09,450
除此以外呢嗯比较强的baseline呃

319
00:15:09,450 --> 00:15:11,650
除了传统的SAASRC以外

320
00:15:11,650 --> 00:15:13,790
其实LARA的呃

321
00:15:13,790 --> 00:15:16,970
constraint generative版也是非常强的

322
00:15:16,970 --> 00:15:17,890
然后呢

323
00:15:17,890 --> 00:15:20,370
我觉得文章中比较主要的一个实验

324
00:15:20,370 --> 00:15:21,970
其实是这个abolation study

325
00:15:21,970 --> 00:15:23,980
它主要是为了确认

326
00:15:23,980 --> 00:15:26,540
就是我们的rewarding和reasoning有没有用

327
00:15:26,540 --> 00:15:27,560
这个事情

328
00:15:27,560 --> 00:15:29,460
比如第一行呃

329
00:15:29,460 --> 00:15:31,340
这个with classification head呢

330
00:15:31,340 --> 00:15:34,290
就是我们不通过呃

331
00:15:34,290 --> 00:15:37,090
embed item这样方式去训练这个模型

332
00:15:37,090 --> 00:15:43,540
而是简单地使用一个呃线性层作为呃训练

333
00:15:43,540 --> 00:15:46,300
然后可以发现它的效果是非常糟糕的

334
00:15:46,300 --> 00:15:50,580
所以说我们通过embed ion的信息得到的语义

335
00:15:50,580 --> 00:15:53,879
以及contrastive learning这样的训练理念

336
00:15:53,879 --> 00:15:56,610
放在强化学习中也是非常有必要的

337
00:15:56,610 --> 00:15:57,010
呃

338
00:15:57,010 --> 00:15:58,210
第二个without reasoning

339
00:15:58,210 --> 00:16:01,750
也就是我觉得这篇文章最主要的实验了呃

340
00:16:01,750 --> 00:16:05,240
这可以看到就是这一行对比我们的模型

341
00:16:05,240 --> 00:16:07,540
它大概相等于15%到20左右

342
00:16:07,540 --> 00:16:10,480
这就证明了其实我们让模型做reasoning

343
00:16:10,480 --> 00:16:13,600
它还是呃有效果提升的

344
00:16:13,600 --> 00:16:14,340
第三行

345
00:16:14,340 --> 00:16:15,020
第四行呢

346
00:16:15,020 --> 00:16:22,160
就是分别去update这两个DISCATE和continuous word

347
00:16:22,160 --> 00:16:23,280
然后可以看到

348
00:16:23,280 --> 00:16:26,960
其实不用continue to reword的效果也很不错

349
00:16:26,960 --> 00:16:31,020
但是如果我们能够在呃

350
00:16:31,460 --> 00:16:34,180
加上一点点a continuous reward

351
00:16:34,180 --> 00:16:38,839
为了防止当所有的item排名都一样的时候

352
00:16:38,839 --> 00:16:39,279
呃

353
00:16:39,279 --> 00:16:42,319
当所有的trajectory带来的item排名都一样的时候

354
00:16:42,319 --> 00:16:43,839
能带来细微的signal

355
00:16:43,839 --> 00:16:47,039
这个还是对训练有增益的

356
00:16:48,230 --> 00:16:51,630
呃这里是一些高参的分析

357
00:16:51,630 --> 00:16:52,270
呃

358
00:16:52,270 --> 00:16:54,590
首先第一幅图A和B

359
00:16:54,590 --> 00:16:59,860
它主要是在呃寻找就是比较优的训练时的

360
00:16:59,860 --> 00:17:00,560
Trajectory

361
00:17:00,560 --> 00:17:02,440
采样的参数

362
00:17:02,760 --> 00:17:04,040
第一个就是temperature

363
00:17:04,040 --> 00:17:05,220
第二个就是top a

364
00:17:05,220 --> 00:17:07,500
可以看出对于temperature这个参数来说

365
00:17:07,500 --> 00:17:12,579
其实我们最好是把呃随机性放大一点会比较好

366
00:17:12,579 --> 00:17:16,500
然后top k的话它有一个sweet pot effect

367
00:17:16,500 --> 00:17:18,200
最好是在mediate

368
00:17:18,200 --> 00:17:19,319
也就是大概200

369
00:17:19,319 --> 00:17:21,359
这个时候是比较优的

370
00:17:21,359 --> 00:17:25,839
然后同时可以看出就是我们引入随机性比较强

371
00:17:25,839 --> 00:17:27,119
也就在他们身上的话

372
00:17:27,119 --> 00:17:31,089
可以看到一个呃在deep seek上也有的现象

373
00:17:31,089 --> 00:17:35,470
也就是呃能看到reasoning length的增加

374
00:17:35,470 --> 00:17:36,590
除此以外呢

375
00:17:36,590 --> 00:17:39,870
因为我们在不同的backbone上进行了实验

376
00:17:39,870 --> 00:17:42,190
一个是Q问系列的3B

377
00:17:42,190 --> 00:17:44,280
一个是JA的2B

378
00:17:44,280 --> 00:17:46,920
然后我们通过增加group size

379
00:17:46,920 --> 00:17:47,560
可以看到

380
00:17:47,560 --> 00:17:49,540
其实解码这个模型本身

381
00:17:49,540 --> 00:17:52,800
它会有一个比较好的初始化的

382
00:17:52,800 --> 00:17:55,010
reasoning for rap的能力

383
00:17:55,010 --> 00:17:58,090
因为哪怕是只有在我们只采样一条

384
00:17:58,090 --> 00:17:59,370
trajectory的情况下

385
00:17:59,370 --> 00:18:01,270
他们能达到一个比较好的效果

386
00:18:01,270 --> 00:18:05,560
并且我们如果增加这个采样的直占据数量的话

387
00:18:05,560 --> 00:18:09,140
嗯并不会有非常显著的指标的提升

388
00:18:09,140 --> 00:18:10,909
但是呢Q问是可以的

389
00:18:10,909 --> 00:18:12,389
所以这也另一方面说明

390
00:18:12,389 --> 00:18:16,710
就是Q问本身他是需要这样一个多trajectory的

391
00:18:16,710 --> 00:18:17,570
相较对比

392
00:18:17,570 --> 00:18:19,070
才能学到一个比较好的

393
00:18:19,070 --> 00:18:23,530
reasoning to recommend的能力呃

394
00:18:23,530 --> 00:18:24,750
除此以外

395
00:18:24,750 --> 00:18:26,070
我们还做了一些

396
00:18:26,070 --> 00:18:31,390
就是关于这个reasoning本身的行为的分析

397
00:18:31,390 --> 00:18:33,750
就是我们想看到呃

398
00:18:33,750 --> 00:18:36,570
这个模型具体它是生成了什么样的reasoning

399
00:18:36,570 --> 00:18:37,750
为什么这样的reasoning

400
00:18:37,750 --> 00:18:39,590
能够帮助他做更好的推荐

401
00:18:39,590 --> 00:18:43,889
然后我们是通过呃让更大的模型

402
00:18:43,889 --> 00:18:45,149
当时是GPT4O

403
00:18:45,149 --> 00:18:47,429
然后以及我们人工去归类

404
00:18:47,429 --> 00:18:50,190
总结了一些他会展现的行为

405
00:18:50,190 --> 00:18:54,330
例如他会呃做attribute abstraction

406
00:18:54,330 --> 00:18:58,350
也就是说去identify fy and generalize item features

407
00:18:58,350 --> 00:19:00,550
他会做一些negative exclusion

408
00:19:00,550 --> 00:19:02,870
会说用户有可能是不喜欢这些东西

409
00:19:02,870 --> 00:19:04,409
有可能是不喜欢那些东西的

410
00:19:04,409 --> 00:19:08,109
然后他会在说我觉得你可能喜欢这些东西之后

411
00:19:08,109 --> 00:19:12,360
去试图解释为什么他那样说self explanation

412
00:19:12,360 --> 00:19:14,480
然后他也会做pattern recognition

413
00:19:18,720 --> 00:19:20,290
items和preference

414
00:19:20,290 --> 00:19:22,950
他还会做scenario based reasoning

415
00:19:22,950 --> 00:19:26,130
也就是说试图猜测为什么最近你在买这些东西

416
00:19:26,130 --> 00:19:27,210
你是不是什么坏了

417
00:19:27,210 --> 00:19:28,620
你是不是在寻找什么

418
00:19:28,620 --> 00:19:32,490
然后他也还会做temple的reason去看呃

419
00:19:32,490 --> 00:19:35,010
就是目前距离是距离试验

420
00:19:35,010 --> 00:19:37,290
在目前时间点比较近的呃

421
00:19:37,290 --> 00:19:41,650
物品可能会更具有被注意的必要

422
00:19:41,650 --> 00:19:42,810
除此以外呢

423
00:19:42,810 --> 00:19:45,270
我们还研究了这些行为

424
00:19:45,270 --> 00:19:49,480
在不同数据集上的一个呃分布

425
00:19:49,480 --> 00:19:52,540
也就是说我们想看在这个数据集里

426
00:19:52,540 --> 00:19:56,790
有多少比例的test数据展现出了这样的行为

427
00:19:56,790 --> 00:19:58,310
如果掩饰的比较深的话

428
00:19:58,310 --> 00:20:00,739
那么就是呃可能占比比较高

429
00:20:00,739 --> 00:20:04,499
也就是说大部分的test数据集上都有这样的行为

430
00:20:04,499 --> 00:20:07,150
如果是偏蓝色的话就比较低

431
00:20:07,430 --> 00:20:09,070
然后可以得出一个结论

432
00:20:09,070 --> 00:20:12,470
就是呃首先在不同的数据集上

433
00:20:12,470 --> 00:20:15,239
模型的行为是不一样的

434
00:20:15,239 --> 00:20:19,779
有些数据集上呢他会呃比较偏好于呃去展示

435
00:20:19,779 --> 00:20:21,559
比如说这个mod objective

436
00:20:21,559 --> 00:20:23,519
但有些数据上就不太会

437
00:20:23,519 --> 00:20:27,430
所以说呢我们能够得到的就是uh

438
00:20:27,430 --> 00:20:30,070
This adaptive reasoning strategy

439
00:20:30,070 --> 00:20:33,350
uh它能够去self organize this decision

440
00:20:33,350 --> 00:20:36,710
Making process based on the main characteristics

441
00:20:36,870 --> 00:20:43,170
嗯这也是我觉得嗯算是可解释性研究的一部分

442
00:20:43,490 --> 00:20:45,610
然后除此以外嗯

443
00:20:45,610 --> 00:20:48,610
我们的框架还有一个比较优势的点

444
00:20:48,610 --> 00:20:52,190
是相较于以往的大部分嗯

445
00:20:52,190 --> 00:20:53,750
Larger rank mode model

446
00:20:53,750 --> 00:20:55,870
Which needs to output item

447
00:20:55,870 --> 00:20:58,850
identifiers to呃

448
00:20:58,850 --> 00:21:00,110
Give recommendation

449
00:21:00,110 --> 00:21:05,630
它使用的方式只是生成仅仅一个呃item的

450
00:21:07,110 --> 00:21:08,110
identify吧

451
00:21:08,110 --> 00:21:08,670
可以这么说

452
00:21:08,670 --> 00:21:10,750
就是它的identify length等于一

453
00:21:10,750 --> 00:21:14,550
所以说在这样的嗯优势下呢

454
00:21:14,550 --> 00:21:18,550
它并没有因为多了reasoning而sacrifice了

455
00:21:19,330 --> 00:21:21,210
然后可以看到左边这个表

456
00:21:21,210 --> 00:21:24,250
就是我们的latency大概是1.67

457
00:21:24,250 --> 00:21:31,500
这个呢其实已经比以往的一些呃比较常见的呃

458
00:21:31,500 --> 00:21:33,940
large recommender model的框架要快很多了

459
00:21:33,940 --> 00:21:35,420
比如说D3

460
00:21:35,420 --> 00:21:37,090
它是4.62

461
00:21:37,090 --> 00:21:40,810
当然了呃他和SUSROG还是有非常大的差距的

462
00:21:40,810 --> 00:21:44,370
我觉得这也是目前我们会需要notice的一个问题

463
00:21:44,370 --> 00:21:46,590
就是大部分large room他们都with reading

464
00:21:46,590 --> 00:21:47,130
Large room

465
00:21:47,130 --> 00:21:49,600
他们的model本身就是比较慢的

466
00:21:49,600 --> 00:21:50,240
当然了

467
00:21:50,240 --> 00:21:52,600
VLM它能很好的缓解这个问题

468
00:21:52,600 --> 00:21:58,190
然后呃我觉得能把later7降到差不多

469
00:21:58,190 --> 00:22:01,850
有一半以下也是非常好的

470
00:22:02,250 --> 00:22:05,780
所以说我觉得这也是我们这个框架的

471
00:22:05,780 --> 00:22:06,500
另外一个优势

472
00:22:10,420 --> 00:22:12,540
去换来performance的提升

473
00:22:13,040 --> 00:22:15,520
然后这里做一个简单的summary

474
00:22:15,520 --> 00:22:18,210
我们首先提出了这样的模型架构

475
00:22:18,210 --> 00:22:20,210
使用一个unified larger and comment

476
00:22:20,210 --> 00:22:22,970
Al model with intrinsic reasoning capabilities

477
00:22:22,970 --> 00:22:25,679
然后我们提出了一套合理的训练方案

478
00:22:25,679 --> 00:22:26,759
Uh recommendation

479
00:22:26,759 --> 00:22:27,199
Policy

480
00:22:27,199 --> 00:22:30,039
Optimization to learn effective reasoning

481
00:22:30,039 --> 00:22:30,889
Strategies

482
00:22:30,889 --> 00:22:33,229
Without human annotations

483
00:22:33,229 --> 00:22:36,060
我们有很好的performance呃

484
00:22:36,060 --> 00:22:37,220
基本上嗯

485
00:22:38,540 --> 00:22:42,220
在以往上能提升至少30~20到30个点

486
00:22:42,220 --> 00:22:44,360
同时它还有一定的adaptability

487
00:22:44,360 --> 00:22:46,280
他的reason里有可解释性

488
00:22:46,280 --> 00:22:49,620
并且能self organize他的strategies

489
00:22:49,620 --> 00:22:50,870
Across domains

490
00:22:50,870 --> 00:22:53,310
然后我们最终得出的结论

491
00:22:53,310 --> 00:22:55,080
也就是最开始那个问题

492
00:22:55,080 --> 00:22:56,660
A reasoning and recommendation

493
00:22:56,660 --> 00:22:59,160
Can be effectively unified a single model

494
00:22:59,160 --> 00:23:02,000
Achieving both performance and if it

495
00:23:02,000 --> 00:23:05,740
然后呃谢谢大家的聆听

496
00:23:05,740 --> 00:23:07,180
然后如果有需要的话

497
00:23:07,180 --> 00:23:09,280
可以扫一下底下这个二维码

498
00:23:09,280 --> 00:23:10,880
看一下我们的paper本身

499
00:23:10,880 --> 00:23:12,780
还有code和数据集

500
00:23:12,780 --> 00:23:14,860
还有model chequ

501
00:23:15,340 --> 00:23:17,860
OK大概是这样

502
00:23:19,080 --> 00:23:20,640
好的谢谢润阳

503
00:23:20,640 --> 00:23:24,440
如果大家有任何的问题可以发在我们的弹幕里

504
00:23:30,580 --> 00:23:31,480
好的

505
00:23:31,480 --> 00:23:36,340
那下面我们有请NOS的林子杰同学嗯

506
00:23:36,340 --> 00:23:37,020
子杰你好

507
00:23:37,020 --> 00:23:39,020
可以共享屏幕了

508
00:23:43,100 --> 00:23:46,740
嗯嗯嗯晓得

509
00:23:49,880 --> 00:23:51,220
呃大家好

510
00:23:51,220 --> 00:23:53,880
我这边是要拍视频

511
00:24:00,440 --> 00:24:01,440
喂你好

512
00:24:01,440 --> 00:24:03,920
可以听到可以听得到可以听得见

513
00:24:03,920 --> 00:24:05,160
哎是的

514
00:24:05,160 --> 00:24:08,230
那是不是有点回声嗯

515
00:24:08,230 --> 00:24:09,990
嗯还好还好

516
00:24:09,990 --> 00:24:13,630
如果有任何问题会随时跟您说

517
00:24:13,630 --> 00:24:14,770
嗯好的

518
00:24:14,770 --> 00:24:17,850
那我开始我的培训天数呃

519
00:24:17,850 --> 00:24:19,250
我做的一篇工作呃

520
00:24:19,250 --> 00:24:22,330
我是来自NOS里的硕士生林子杰

521
00:24:22,330 --> 00:24:26,610
然后我做一篇工作叫做token m decisiveness modeling

522
00:24:26,610 --> 00:24:31,290
We are information game in larger models for personneuh

523
00:24:31,290 --> 00:24:33,110
Personalized recommendation

524
00:24:33,110 --> 00:24:36,110
这份工作呢具体来说就是呃

525
00:24:36,110 --> 00:24:40,590
通过提出用IG这个特征来建模呃

526
00:24:40,590 --> 00:24:42,050
token的一个重要性

527
00:24:42,050 --> 00:24:47,430
然后来辅助大圆模型额更好的做一个推荐任务

528
00:24:49,000 --> 00:24:52,620
呃先来介简单介绍一下这边的一个背光

529
00:24:52,620 --> 00:24:57,070
就是所谓的圣城市推荐中的l m for rk

530
00:24:57,070 --> 00:24:58,790
推理系统啊

531
00:24:58,790 --> 00:25:02,410
随着大圆模型的一个呃能力的进展

532
00:25:02,410 --> 00:25:07,210
大家开始呃把大语言模型应用在推荐任务上面

533
00:25:07,210 --> 00:25:10,930
呃就是因为道理模型它本身拥有强大的语言啊

534
00:25:10,930 --> 00:25:15,050
理解能力和一些瑞森的一些能力

535
00:25:15,690 --> 00:25:17,370
那在这样的一个范式中呢

536
00:25:17,370 --> 00:25:20,550
推荐任务经常是被啊formulated呃

537
00:25:20,550 --> 00:25:23,170
作为一个类似自然语言的一个任务

538
00:25:23,170 --> 00:25:29,620
也就是user过去的点击的历史都作为呃一个输入

539
00:25:29,620 --> 00:25:32,600
然后ENCORDED在prompt中间

540
00:25:32,600 --> 00:25:34,800
然后呢大语言模型呃

541
00:25:34,800 --> 00:25:38,300
通过微调或者一些其他的学习方式去呃

542
00:25:38,300 --> 00:25:42,240
生成接下来的top canext recommended items

543
00:25:42,240 --> 00:25:46,240
那这个方法已经被证明有很强的一个呃

544
00:25:46,240 --> 00:25:47,340
语言的理解能力

545
00:25:47,340 --> 00:25:50,960
并且可以插入一些最新的大语言模型推呃

546
00:25:50,960 --> 00:25:54,720
REON的一些能力是比较有前景的一种方式

547
00:25:57,600 --> 00:26:01,300
那么在大圆模型推荐系统l m for瑞克中间呢

548
00:26:01,300 --> 00:26:03,480
一种比较典型的呃

549
00:26:03,480 --> 00:26:05,880
额就是应用或turning和decoding

550
00:26:05,880 --> 00:26:08,090
方法呢呃是这样子的

551
00:26:08,090 --> 00:26:12,570
那首先prompt是用户点击的一些item的history

552
00:26:12,570 --> 00:26:15,650
加上这个text description呃

553
00:26:15,650 --> 00:26:19,740
就是告诉大圆模型说他要呃predict出呃

554
00:26:19,740 --> 00:26:22,300
呃根据用户之前的一些item history去

555
00:26:22,300 --> 00:26:25,929
可定出接下来的一些top k item

556
00:26:26,209 --> 00:26:29,029
那output呢呃一种方式是呃

557
00:26:29,029 --> 00:26:33,980
别让大圆模型就呃predict出这个next item呃

558
00:26:33,980 --> 00:26:36,160
但这边比较重要一点是

559
00:26:36,160 --> 00:26:40,340
next item经常是被imported as a token sequence

560
00:26:40,340 --> 00:26:45,300
也就是说呃我们最终想要呃解码出的那个item

561
00:26:45,300 --> 00:26:46,280
Identifire

562
00:26:46,280 --> 00:26:49,020
他很多时候是一个token sequence

563
00:26:49,020 --> 00:26:50,520
因为只有token sequence

564
00:26:50,520 --> 00:26:52,840
才能让大圆模型捕捉到它的一个

565
00:26:52,840 --> 00:26:55,080
那就更好的去理解它的运营能力

566
00:26:55,080 --> 00:26:58,040
那我理解就是生成式推荐跟判别式推荐

567
00:26:58,040 --> 00:27:00,210
一个很大的一个区别

568
00:27:00,810 --> 00:27:02,750
那我们典型的额

569
00:27:02,750 --> 00:27:06,130
把大圆模型应用在下游的recommendation任务中呢

570
00:27:06,130 --> 00:27:08,830
用的方式就是SFT嗯

571
00:27:08,830 --> 00:27:12,030
呃SFT去呃去T呃

572
00:27:12,030 --> 00:27:15,330
用户用户历史中的下一个next item

573
00:27:15,330 --> 00:27:18,949
比如说是他的title或者是symatic id

574
00:27:19,229 --> 00:27:21,389
但是注入支持的方式嗯

575
00:27:21,389 --> 00:27:24,170
很多时候还是要依赖SFT

576
00:27:24,730 --> 00:27:29,030
那虽然说呃就是真正的数据集中的光truth

577
00:27:29,030 --> 00:27:31,570
那只有下一个make single items

578
00:27:31,570 --> 00:27:34,890
那大语言模型呢为了应用在真实的场景中

579
00:27:34,890 --> 00:27:37,429
它还是需要PREDIC出tok items

580
00:27:37,429 --> 00:27:39,309
所以一种比较有效率的方式

581
00:27:39,309 --> 00:27:43,310
就是通过并search去找到呃

582
00:27:43,310 --> 00:27:48,000
就是概率最高的top k candidate items

583
00:27:48,640 --> 00:27:50,920
那在这个研究中呢

584
00:27:50,920 --> 00:27:54,680
我们主要是呃专注于l m for瑞克

585
00:27:54,680 --> 00:27:57,520
利用f f t bm search这个方式

586
00:27:57,520 --> 00:27:59,420
然后这是必须要注意的一点

587
00:27:59,420 --> 00:28:00,840
如果是用其他方式的话

588
00:28:00,840 --> 00:28:05,040
可能就是这个研究就没有办法散发在上面了

589
00:28:06,000 --> 00:28:10,000
那刚刚说的这个范式它有什么样的问题呢

590
00:28:10,000 --> 00:28:13,320
那我们所发现的就是呃所谓的token by token

591
00:28:13,320 --> 00:28:15,340
的一个likely food maximization

592
00:28:15,340 --> 00:28:18,720
他和最终推荐的那个他给最终推荐的目标

593
00:28:18,720 --> 00:28:21,320
它实际上是不完全一致的

594
00:28:21,320 --> 00:28:23,380
那怎么去理解这件事情呢

595
00:28:23,380 --> 00:28:26,040
我们可以看下面左图和右图的一个yes

596
00:28:26,040 --> 00:28:27,920
and but的例子

597
00:28:28,080 --> 00:28:29,460
比如左图中呢

598
00:28:29,460 --> 00:28:29,840
呃

599
00:28:29,840 --> 00:28:32,120
我们真正如果想要去predict出

600
00:28:32,120 --> 00:28:33,440
每个item的一个概率

601
00:28:33,440 --> 00:28:37,230
比如说理想情况下super mario它的概率是四

602
00:28:37,230 --> 00:28:40,630
然后塞尔达传说他的概率是0.4

603
00:28:40,630 --> 00:28:43,490
然后教父他的概率是0.2

604
00:28:43,490 --> 00:28:46,310
这是我们的一个理想的一个情况

605
00:28:46,710 --> 00:28:48,950
但是通过SFT呃

606
00:28:48,950 --> 00:28:50,810
然后在been search出来的时候

607
00:28:50,810 --> 00:28:53,770
我们发现它实际上每个item它是由token sequence

608
00:28:53,770 --> 00:28:54,490
组成的

609
00:28:54,490 --> 00:28:55,610
这个token sequence

610
00:28:55,610 --> 00:28:59,600
就会导致就算他们的最后总和的这个分数概率

611
00:28:59,600 --> 00:29:02,659
也就是P等于面试或者是SC等于0.4吧

612
00:29:02,659 --> 00:29:06,359
但实际上它呃每个token的分布是不一致的

613
00:29:06,359 --> 00:29:09,650
比如说他要有可能super它的概率是0.4

614
00:29:09,650 --> 00:29:12,510
然后这个token的的概率是0.6

615
00:29:12,510 --> 00:29:14,210
就导致说呃

616
00:29:14,210 --> 00:29:17,510
最后如果通过bm search去选top two item的话

617
00:29:17,510 --> 00:29:21,050
他就解码出的是那雷end of diatheta

618
00:29:21,050 --> 00:29:22,380
那grandfather

619
00:29:22,380 --> 00:29:24,940
而不是这个比较高的SUPERMARIO

620
00:29:24,940 --> 00:29:26,830
这就是一个典型的嗯

621
00:29:26,830 --> 00:29:30,610
就是最终那个target的那个分数和概率

622
00:29:30,610 --> 00:29:33,390
跟实际上token中间那个分布概率不一致

623
00:29:33,390 --> 00:29:34,710
哪个例子

624
00:29:37,140 --> 00:29:39,400
这个东西呢实际上并不是

625
00:29:39,400 --> 00:29:42,750
只在我们研究中所观察到呃

626
00:29:42,750 --> 00:29:45,250
这个被称为一个token level biases的一个问题

627
00:29:45,250 --> 00:29:47,920
在过去的一些年中也有观察到现象

628
00:29:47,920 --> 00:29:50,320
比如最典型的一个现象叫做呃

629
00:29:50,320 --> 00:29:53,000
很多解码出的一个item

630
00:29:53,000 --> 00:29:54,880
它们都有非常一样的前缀

631
00:29:54,880 --> 00:29:56,960
比如说the这个前缀

632
00:29:57,489 --> 00:30:00,520
然后呢有的研究还发现说呃

633
00:30:00,520 --> 00:30:03,660
有些token他有非常非常高的logic概率

634
00:30:03,660 --> 00:30:07,880
然后直接主导整个解码的过程

635
00:30:10,060 --> 00:30:11,380
过去的研究呢

636
00:30:11,380 --> 00:30:15,350
呃虽然尝试提出一些方法去解决相应的问题

637
00:30:15,350 --> 00:30:17,630
但是他们有个很大的问题在于

638
00:30:17,630 --> 00:30:20,530
他们没有呃真的去figure out

639
00:30:20,530 --> 00:30:23,050
或者去找出这个token level bias

640
00:30:23,050 --> 00:30:24,710
它的一个根源在什么地方

641
00:30:24,710 --> 00:30:27,890
也就是说他没有真正去呃深入实践

642
00:30:27,890 --> 00:30:32,320
去挖掘出哪些token容易去引起呃

643
00:30:32,320 --> 00:30:34,320
非常高的一个解码概率

644
00:30:34,320 --> 00:30:40,280
或者是呃就是它有非呃很容易被解码出来

645
00:30:44,760 --> 00:30:47,760
所以这就其实引发了我们一个问题呃

646
00:30:47,760 --> 00:30:50,960
就是我们如何去model token importance

647
00:30:50,960 --> 00:30:52,880
在element for week中了

648
00:30:52,880 --> 00:30:55,500
是不是呃一个token sequence

649
00:30:55,500 --> 00:30:56,860
就是我们icon

650
00:30:56,860 --> 00:31:00,420
当我们嗯encoded as a token sequence的时候

651
00:31:00,420 --> 00:31:00,980
呃

652
00:31:00,980 --> 00:31:04,980
中间每个token他对于最终我们呃那个item

653
00:31:04,980 --> 00:31:08,800
the identify identification的贡献是不是一样的呢

654
00:31:08,800 --> 00:31:11,380
这个就是我们需要研究或者去探索的

655
00:31:11,380 --> 00:31:12,820
一个问题了

656
00:31:13,460 --> 00:31:15,480
按直觉上去理解的话

657
00:31:15,480 --> 00:31:17,420
我们可以考虑下面几个问题

658
00:31:17,420 --> 00:31:21,660
比如说第一个就是呃是否所有的tokens呃

659
00:31:21,660 --> 00:31:25,180
对于最终决定那个item都一样重要的

660
00:31:25,180 --> 00:31:26,080
感觉上来说

661
00:31:26,080 --> 00:31:27,040
其实并不是一样的

662
00:31:27,040 --> 00:31:28,920
比如说the godfather

663
00:31:28,920 --> 00:31:31,140
The legend of zelda

664
00:31:31,140 --> 00:31:33,210
他们这中间那个定冠词the

665
00:31:33,210 --> 00:31:36,670
对于呃让我们找到最终的final item的贡献

666
00:31:36,670 --> 00:31:38,430
可能是不如super mario的

667
00:31:38,430 --> 00:31:43,470
因为super可能最终呃呃就是呃对应的那个item

668
00:31:43,470 --> 00:31:45,840
只有super mario

669
00:31:46,480 --> 00:31:49,340
然后另外呢我们在观察SFT的过程中

670
00:31:49,340 --> 00:31:52,360
我们是否可以观察到大圆模型更容易学习

671
00:31:52,360 --> 00:31:54,850
或者是呃你和某些token

672
00:31:54,850 --> 00:31:57,450
然后导致这些token上面已经over fitting了

673
00:31:57,450 --> 00:32:00,290
但是一些更重要或更有语义的一些token

674
00:32:00,290 --> 00:32:01,950
他们啊实际上还没有被嗯

675
00:32:01,950 --> 00:32:04,370
没没有被完全学习过

676
00:32:04,650 --> 00:32:07,090
然后另外就是解码的过程中呃

677
00:32:07,090 --> 00:32:09,490
是否呃部分token呃

678
00:32:09,490 --> 00:32:10,810
我们可以把它找出来

679
00:32:10,810 --> 00:32:13,980
然后发现说他们确确实实有非常高的一个

680
00:32:13,980 --> 00:32:14,760
解法概率

681
00:32:14,760 --> 00:32:17,280
而导致我们兵力摄取的过程中呃

682
00:32:17,280 --> 00:32:19,960
他只能从这个token嗯出发

683
00:32:19,960 --> 00:32:23,480
然后所导致说最终解码top k item嗯

684
00:32:23,480 --> 00:32:26,120
都是具有共有某种特定的prefix

685
00:32:27,960 --> 00:32:30,900
为了去呃调查这些问题的话

686
00:32:30,900 --> 00:32:34,400
我们就提出额去evaluating token

687
00:32:34,400 --> 00:32:35,100
Importance

688
00:32:35,100 --> 00:32:36,739
In element for rk

689
00:32:36,739 --> 00:32:41,819
通过每个token的decisiveness及how much a token

690
00:32:41,819 --> 00:32:43,259
Contribute to the models

691
00:32:43,259 --> 00:32:45,719
Final item prediction

692
00:32:48,220 --> 00:32:51,660
啊这个是我们主要如何去model token

693
00:32:51,660 --> 00:32:54,460
DESEES的一个过程

694
00:32:54,840 --> 00:33:00,180
那首先呢我们是把每个token generation的process呃

695
00:33:00,180 --> 00:33:03,740
去建模为一个decision making的一个过程

696
00:33:03,740 --> 00:33:05,300
那我们可以看到这张图

697
00:33:05,300 --> 00:33:09,630
就是每个token实际上他在慢慢逐渐的呃

698
00:33:09,630 --> 00:33:12,450
从一开始的start是完全不知道这个item的

699
00:33:12,450 --> 00:33:13,670
到最终解码处

700
00:33:13,670 --> 00:33:16,380
比如super mario或者是the legend of zelda

701
00:33:16,380 --> 00:33:19,220
那他们实际上是一个就是慢慢确定

702
00:33:19,220 --> 00:33:20,880
我们最终那个item的一个过程

703
00:33:20,880 --> 00:33:23,660
那每个token在identify

704
00:33:23,660 --> 00:33:26,770
final item过程中间的贡献是不一样的

705
00:33:26,770 --> 00:33:32,170
那我们如何去建模这样的一个呃token的贡献呢

706
00:33:32,320 --> 00:33:34,200
受到信息论的一个启发

707
00:33:34,200 --> 00:33:39,890
我们可以去呃建模在每一个step t的一个

708
00:33:39,890 --> 00:33:44,149
uncertainty就是呃呃呃怎么说

709
00:33:44,149 --> 00:33:47,249
就是它可解码的一个item的一个选项吧

710
00:33:47,249 --> 00:33:48,659
的一个不确定性

711
00:33:48,659 --> 00:33:53,880
我们通过相同商来建模这个呃这个不确定性

712
00:33:53,880 --> 00:33:57,280
然后接下来呢我们就可以通过information game

713
00:33:57,280 --> 00:33:59,960
也就是信息增益这个事情来建模

714
00:33:59,960 --> 00:34:04,630
每个token对于呃这个reduction of uncertainty

715
00:34:04,630 --> 00:34:07,980
也就是不确定性的降低程度是多少

716
00:34:07,980 --> 00:34:10,400
右边是呃一些公式

717
00:34:10,400 --> 00:34:13,849
比如说这个如何去估计不确定性呢

718
00:34:13,849 --> 00:34:19,449
就通过当前所可以候选的item时的那个概率DI

719
00:34:19,449 --> 00:34:22,759
然后去计算一个去计算一个熵

720
00:34:22,759 --> 00:34:28,159
然后呢每一个token的这个信息增益IG是额后

721
00:34:28,159 --> 00:34:32,579
后一个就是前一个的熵减去后一个的商

722
00:34:32,579 --> 00:34:34,418
然后我们可以看左边这张图

723
00:34:34,418 --> 00:34:38,790
比如说呃当我们从super解码到MARIO的时候

724
00:34:38,790 --> 00:34:42,350
它呃它的那个不确定性存在一个降低

725
00:34:42,350 --> 00:34:47,050
那这个降低呢就是token mario的一个IG值

726
00:34:47,050 --> 00:34:50,929
那这个特征是呃通过训练集来估计出来的

727
00:34:50,929 --> 00:34:53,429
也就是实际上它是跟模型无关的

728
00:34:53,429 --> 00:34:56,590
每个token在大远模型推荐系统这个任务中

729
00:34:56,590 --> 00:34:58,550
固有的一个属性

730
00:35:02,380 --> 00:35:09,420
那既然我们可以额用information game这个特征来表示token

731
00:35:09,420 --> 00:35:10,520
Desessiveness

732
00:35:10,520 --> 00:35:13,780
那我们也可以去互相compare

733
00:35:13,780 --> 00:35:15,760
每个token的decisiveness

734
00:35:15,760 --> 00:35:17,990
比如说token the

735
00:35:17,990 --> 00:35:22,170
他的那个DECISIERS就是比token super来的小的

736
00:35:22,170 --> 00:35:25,210
因为它被很多token item所共有

737
00:35:25,210 --> 00:35:27,330
在那个公式计算中

738
00:35:27,330 --> 00:35:32,650
就会发现这个IG的质量是小于小于IG super的

739
00:35:33,310 --> 00:35:37,270
那同时我们的真实数集中会发现说很多的token

740
00:35:37,270 --> 00:35:40,620
它实际上是呈现出一个自由IG的状态了

741
00:35:40,620 --> 00:35:43,580
呃IG这个值它是一定是大于等于零的

742
00:35:43,580 --> 00:35:45,680
因为呃这个信息呃

743
00:35:45,680 --> 00:35:48,050
这个ASCERITY是一定慢慢是降低的

744
00:35:48,050 --> 00:35:50,450
然后duo EDG token呢表示的一个含义

745
00:35:50,450 --> 00:35:52,030
就是在呃这个替补

746
00:35:52,030 --> 00:35:52,930
比如说第二部

747
00:35:52,930 --> 00:35:55,700
第三部它实际上只有这一个选项了

748
00:35:55,700 --> 00:35:59,160
那这些token就不会有任何不确定的

749
00:35:59,160 --> 00:36:01,080
像呃不确定度的降低

750
00:36:01,080 --> 00:36:04,520
所以他们的IG都是等于零

751
00:36:06,020 --> 00:36:08,510
给自己讲过呃

752
00:36:08,510 --> 00:36:11,930
呃提出了这个token desession model里的一个方法之后

753
00:36:11,930 --> 00:36:15,040
我们可以进行一些简单的thetics analysis

754
00:36:15,040 --> 00:36:17,740
一个比较显著的一个观察是

755
00:36:17,740 --> 00:36:20,120
zoatric tokens在真实数据中集中

756
00:36:20,120 --> 00:36:21,850
它的占比实际上是非常高的

757
00:36:21,850 --> 00:36:24,290
他大概占了55.96%

758
00:36:24,290 --> 00:36:26,710
到72.97%的范围

759
00:36:26,710 --> 00:36:28,400
也就是说实际上呃

760
00:36:28,400 --> 00:36:30,120
当我们应用生成式推荐的时候

761
00:36:30,120 --> 00:36:31,080
大部分人token

762
00:36:31,080 --> 00:36:35,780
它对于解码最终那个items实际上没有什么帮助

763
00:36:35,780 --> 00:36:39,480
他可能只是一些辅助的一个表示而已

764
00:36:41,400 --> 00:36:43,500
好接下来就回到我们刚才的问题

765
00:36:43,500 --> 00:36:45,950
我们通过想通过我们的建模来看一下

766
00:36:45,950 --> 00:36:47,830
是否真正存在这个所谓token

767
00:36:47,830 --> 00:36:49,150
deo biases的一个问题

768
00:36:49,150 --> 00:36:52,819
能不能更好的去表彰这样的一个biases

769
00:36:52,819 --> 00:36:55,920
那第一个我们发现的就是所谓的一个turn bs

770
00:36:55,920 --> 00:36:58,880
就是在模型SFT的一个过程中呢

771
00:36:58,880 --> 00:37:02,760
这个zo i g token总是学的比nzo IG token快

772
00:37:02,760 --> 00:37:03,920
非常非常多

773
00:37:03,920 --> 00:37:05,880
那就导致了大圆模型

774
00:37:05,880 --> 00:37:09,940
它实际上就很快就把那些呃自由IG token给学完

775
00:37:09,940 --> 00:37:10,780
甚至是过拟合的

776
00:37:10,780 --> 00:37:13,690
但是弄自由i id token他可能还没有完全学习好

777
00:37:13,690 --> 00:37:15,410
那可能这就其他

778
00:37:15,410 --> 00:37:17,810
我们需要做一个对waiting的一个操作

779
00:37:17,810 --> 00:37:20,250
这是一个turn bs的一个部分

780
00:37:20,250 --> 00:37:23,105
右边是note图的对比

781
00:37:25,420 --> 00:37:28,980
然后接下来呃我们还可以观察到decoding bias

782
00:37:28,980 --> 00:37:29,900
Decoding bias

783
00:37:29,900 --> 00:37:32,800
我们可以从两个perspective去观察它

784
00:37:32,800 --> 00:37:36,260
第一个观察就是这个IG值总是跟呃

785
00:37:36,260 --> 00:37:40,569
token的一个LOGI是呈一个正比的一个状态吧

786
00:37:40,569 --> 00:37:42,529
也就是我们可以发现说

787
00:37:42,529 --> 00:37:45,689
大语言模型非常倾向去给NO zo i id

788
00:37:45,689 --> 00:37:46,949
token就比较低

789
00:37:46,949 --> 00:37:49,340
IG的这些token呃

790
00:37:49,340 --> 00:37:51,600
给一个他们非常高的一个生存概率

791
00:37:51,600 --> 00:37:54,560
就尤其是那些zo IG token

792
00:37:54,560 --> 00:37:57,680
比如说这张图里面这个对数的一个logic图

793
00:37:57,680 --> 00:37:59,040
如果是duo h token的话

794
00:37:59,040 --> 00:38:00,600
它的平均值已经接近于零

795
00:38:00,600 --> 00:38:02,750
也就是他们的解码概率接近于一

796
00:38:02,750 --> 00:38:05,750
这实际上就反映了之前呃一些研究中

797
00:38:05,750 --> 00:38:07,310
为什么会发现很多token

798
00:38:07,310 --> 00:38:10,540
他们的概率非常非常高的一个呃

799
00:38:10,540 --> 00:38:11,620
最本质的一个原因

800
00:38:11,620 --> 00:38:14,380
也就是他们实际上就是ZUI这条狗

801
00:38:15,370 --> 00:38:17,430
那还有一个perspective的话

802
00:38:17,430 --> 00:38:20,430
是一个商下降速率的一个perspective

803
00:38:20,430 --> 00:38:23,690
就是我们呃去观察呃

804
00:38:23,690 --> 00:38:25,570
我们大圆模型训练好的一个

805
00:38:25,570 --> 00:38:27,130
大圆模型和解码的过程

806
00:38:27,130 --> 00:38:30,390
我们实际上可以算出它每一步的一个商呃

807
00:38:30,390 --> 00:38:32,350
平均的伤是多少

808
00:38:32,350 --> 00:38:35,070
那我们把它跟我们的光truth

809
00:38:35,070 --> 00:38:37,150
就是测试集的光truth进行一个对比

810
00:38:37,150 --> 00:38:39,950
我们会发现说大元模型呃

811
00:38:39,950 --> 00:38:41,590
去predict出的item

812
00:38:41,590 --> 00:38:45,110
这个商的这个下降的一个速度

813
00:38:45,110 --> 00:38:50,630
总是比呃真正光处死的一个现象速度来得慢的

814
00:38:50,630 --> 00:38:53,530
这导致了说就是呃

815
00:38:53,530 --> 00:38:55,910
大语言模型总是倾向于生成呃

816
00:38:55,910 --> 00:38:58,850
一开始更不确定的那些token

817
00:38:58,850 --> 00:39:03,350
然后引引发了一个跟真实这个数据额

818
00:39:03,350 --> 00:39:07,860
真实这个测试极光处置呃不一样的一个BIOS

819
00:39:07,860 --> 00:39:10,960
就是一个在任何数据集上都都不存在的

820
00:39:10,960 --> 00:39:13,040
一个一个情况

821
00:39:16,180 --> 00:39:21,599
那呃已经提出了这个token i dre呃

822
00:39:21,599 --> 00:39:24,279
i g based的token desession modeling之后

823
00:39:24,279 --> 00:39:26,900
我们成功的identify to biases

824
00:39:26,900 --> 00:39:29,460
第一个是token level tuning bias

825
00:39:29,460 --> 00:39:29,900
呃

826
00:39:29,900 --> 00:39:32,490
接下来是token level decoding bias

827
00:39:32,490 --> 00:39:34,290
And how do we get this spice

828
00:39:34,290 --> 00:39:34,490
哎

829
00:39:34,490 --> 00:39:38,560
就是我们如何解决这些bus基于我们的finding的呃

830
00:39:38,560 --> 00:39:42,800
我们这边有一些呃简单的method design principles

831
00:39:42,800 --> 00:39:45,560
就是第一个我们想要让它

832
00:39:45,560 --> 00:39:47,120
我们的这个解决方法

833
00:39:47,120 --> 00:39:51,000
真正跟我们所观察到的这个bias online

834
00:39:51,000 --> 00:39:52,080
这是第一点

835
00:39:52,080 --> 00:39:53,160
然后第二点是

836
00:39:53,160 --> 00:39:55,700
我们需要尽可能减少我们的hyper parameters

837
00:39:55,700 --> 00:39:59,270
这是因为实际上我们提出这个呃model里

838
00:39:59,270 --> 00:40:01,230
我们是想解决更好的解决

839
00:40:01,230 --> 00:40:02,670
token nel bias这个问题

840
00:40:02,670 --> 00:40:04,810
那如果我们方法设计的非常复杂的话

841
00:40:04,810 --> 00:40:07,870
呃这可能说明我们一开始那个modeling提出的

842
00:40:07,870 --> 00:40:09,420
其实并不是特别好

843
00:40:09,420 --> 00:40:11,220
那第三个是呃

844
00:40:11,220 --> 00:40:14,220
我们想要让我们方法尽可能比较容易的呃

845
00:40:14,220 --> 00:40:16,920
plug in出我们current的一个pipeline

846
00:40:16,920 --> 00:40:20,260
那基于这三个呃设计原则

847
00:40:20,260 --> 00:40:22,640
我们就propose了一个叫做information game

848
00:40:22,640 --> 00:40:25,680
best decisiware token handling啊

849
00:40:25,680 --> 00:40:27,200
简称为IJD的一个方法

850
00:40:27,200 --> 00:40:29,120
它包含两个步骤的呃

851
00:40:29,120 --> 00:40:34,250
呃呃重叫做呃REWAITING的方法呃

852
00:40:34,250 --> 00:40:35,930
一个叫IGDTNING

853
00:40:35,930 --> 00:40:38,490
是在ST阶段的一个REWAKING方法

854
00:40:38,490 --> 00:40:40,050
一个叫做i h d decoding

855
00:40:40,050 --> 00:40:43,180
是在呃being search的一个reawaken的方法

856
00:40:44,140 --> 00:40:47,960
他们都只有一个parameter

857
00:40:49,760 --> 00:40:52,520
哦我们这边来看一下i h d decoding

858
00:40:52,520 --> 00:40:56,100
那我们的s f t nos中间

859
00:40:56,100 --> 00:40:58,300
我们实际上是可以针对某某些

860
00:40:58,300 --> 00:41:01,160
就是部分token去做一个加权的操作的

861
00:41:01,160 --> 00:41:03,740
然后主要在前面加上一个normalization项

862
00:41:03,740 --> 00:41:05,660
这个normalization项是呃

863
00:41:05,660 --> 00:41:10,040
这边所有的额加权的一个平均吧

864
00:41:10,040 --> 00:41:13,080
这样就导致了我们学习率整体上是不会变化的

865
00:41:13,080 --> 00:41:15,460
那我们具体来看一下我们是怎么加权的

866
00:41:15,460 --> 00:41:17,330
实际上这个加权非常简单

867
00:41:17,330 --> 00:41:19,370
就是对那些zo IG token

868
00:41:19,370 --> 00:41:21,490
把他们的这个权重呃

869
00:41:21,490 --> 00:41:25,290
设置为比一来的小的一个数

870
00:41:25,770 --> 00:41:26,770
那这样子的话

871
00:41:26,770 --> 00:41:28,930
实际上就有效的提升了一个

872
00:41:28,930 --> 00:41:30,690
ni j token的一个学习速度

873
00:41:30,690 --> 00:41:34,029
然后同时降低了joi g token的一个学习速度

874
00:41:34,029 --> 00:41:35,709
这是IDDD呃

875
00:41:35,709 --> 00:41:38,709
tn的一个呃表达式

876
00:41:40,760 --> 00:41:43,580
呃这边呃同时可以讲一下一个

877
00:41:43,580 --> 00:41:46,100
为什么我们要采取一个二分类的一个

878
00:41:46,100 --> 00:41:47,160
REWAITING的一个方法

879
00:41:47,160 --> 00:41:48,660
在HD通景中

880
00:41:48,660 --> 00:41:52,620
这是因为首先我们刚才也知道就是IG呃

881
00:41:52,620 --> 00:41:54,090
duo IG token呃

882
00:41:54,090 --> 00:41:55,630
这个0IG的一些token

883
00:41:55,630 --> 00:41:57,210
他们占比实际上是非常高的

884
00:41:57,210 --> 00:41:59,370
而且他们某些特征是非常显著的

885
00:41:59,370 --> 00:42:00,970
那所以我觉得呃

886
00:42:00,970 --> 00:42:04,830
就是把它单独作为呃一个群体去REVIATING

887
00:42:04,830 --> 00:42:06,069
是比较合理的

888
00:42:06,069 --> 00:42:06,709
另外呢

889
00:42:06,709 --> 00:42:08,229
同时我们其实尝试过

890
00:42:08,229 --> 00:42:12,640
就是根据IG的值去设计一个类似线性的一个呃

891
00:42:12,640 --> 00:42:13,760
重家庭的方法

892
00:42:13,760 --> 00:42:16,900
这样他还是得保持有一个参数

893
00:42:16,900 --> 00:42:19,960
但是实验表明说就是它虽然是有效果的

894
00:42:19,960 --> 00:42:21,830
就是这个i h d t linear

895
00:42:21,830 --> 00:42:22,990
他虽然是有效果的

896
00:42:22,990 --> 00:42:26,910
实际上是不如这个二分的一个加权更有效果的

897
00:42:26,910 --> 00:42:30,030
所以到最后我们为了让我们的方法更简单实用

898
00:42:30,030 --> 00:42:34,200
就设计的是一个binary ring的一个scheme

899
00:42:36,580 --> 00:42:39,360
那接下来是agrid decoding的一个过程

900
00:42:39,360 --> 00:42:42,080
IRIDECODING呃也是非常简单的

901
00:42:42,080 --> 00:42:46,200
这边的一个S实际上就是logic的一个和额

902
00:42:46,200 --> 00:42:49,670
这个是beam search中的一个token sequence的一个分数

903
00:42:49,670 --> 00:42:50,990
那对于这个分数呢

904
00:42:50,990 --> 00:42:53,610
我们实际上是尝试去呃

905
00:42:53,610 --> 00:42:56,990
提高高i g token的一个logic呃

906
00:42:56,990 --> 00:42:58,710
降低呃呃

907
00:42:58,710 --> 00:43:02,970
第一ui g token以及feel IG token的一个logic

908
00:43:02,970 --> 00:43:04,570
它的表达式大概是这样子的

909
00:43:04,570 --> 00:43:07,010
阿尔法是属于0~11中间一个值

910
00:43:07,010 --> 00:43:10,570
然后这个IG呢是一个对于当前top k

911
00:43:10,570 --> 00:43:13,090
就比如说我decoding到step two的时候

912
00:43:13,090 --> 00:43:14,380
接下来有十个选项

913
00:43:14,380 --> 00:43:18,180
然后我对我对这十个选项的这个他们的IP值

914
00:43:18,180 --> 00:43:20,020
进行一个normalize来

915
00:43:20,020 --> 00:43:21,800
我normalize到0~1

916
00:43:21,800 --> 00:43:25,180
然后呢额就可以得到一个WD

917
00:43:25,180 --> 00:43:27,200
如果这个中间所有的值

918
00:43:27,200 --> 00:43:28,720
比如说他们IH值都是零的话

919
00:43:28,720 --> 00:43:30,220
那这边就设置成零

920
00:43:30,220 --> 00:43:32,240
这样子就保证了呃

921
00:43:32,240 --> 00:43:35,309
如果在同一个decoding step中呃

922
00:43:35,309 --> 00:43:37,629
有些token他的IG比较高

923
00:43:37,629 --> 00:43:38,669
有些token i h比较低

924
00:43:38,669 --> 00:43:41,220
比如说惹的惹的这个token IG比较低

925
00:43:41,220 --> 00:43:43,500
然后super这个token edge比较高的话

926
00:43:43,500 --> 00:43:46,520
嗯那super的这个logic会得到一个加强

927
00:43:46,520 --> 00:43:48,040
刁德的呃

928
00:43:48,040 --> 00:43:50,800
这个token的doge也会得到一个减减弱

929
00:43:51,960 --> 00:43:54,580
那呃表格中我们也可以发现说

930
00:43:54,580 --> 00:43:57,740
就是随着阿尔法这个超参数的一个增加

931
00:43:57,740 --> 00:44:02,940
这个之前的这个decoding BIOS呃被有效的一个减少

932
00:44:04,280 --> 00:44:09,050
哎呀妈OK接下来是一个主过程呃

933
00:44:09,050 --> 00:44:10,410
呃叫做main result

934
00:44:10,410 --> 00:44:12,010
就是一个主表吧

935
00:44:12,010 --> 00:44:16,370
额我们的一个方法证明了一个呃比较稳定

936
00:44:16,370 --> 00:44:20,060
并且高额比较显著的一个提升呃

937
00:44:20,060 --> 00:44:21,400
在四个数值

938
00:44:21,400 --> 00:44:26,680
以及两个不同的LMFORREK的一个BACKB上面

939
00:44:26,680 --> 00:44:29,460
额这边那个评价标准一个是NDCG

940
00:44:29,460 --> 00:44:30,680
还有一个是hr

941
00:44:30,680 --> 00:44:32,640
都是比较经典的一个评价标准

942
00:44:32,640 --> 00:44:35,260
这证明了就是IGED额

943
00:44:35,260 --> 00:44:36,560
无论tuning和呃

944
00:44:36,560 --> 00:44:39,600
这边的表格是decoding和tiling结合起来

945
00:44:39,600 --> 00:44:43,230
然后呃这个表示的就是我们IGD在呃

946
00:44:43,230 --> 00:44:45,370
不同数据集不同的BB

947
00:44:45,370 --> 00:44:48,530
然后这些上面都有一个比较稳定

948
00:44:48,530 --> 00:44:49,850
而且显著的一个提升

949
00:44:49,850 --> 00:44:52,810
是一个比较即插即用的一个方法

950
00:44:54,640 --> 00:44:56,320
那a blish study的话

951
00:44:56,320 --> 00:44:58,640
就是我们想要把这个h d tuning

952
00:44:58,640 --> 00:45:01,300
和i h d decoding两部分拆开来看一下

953
00:45:01,300 --> 00:45:04,850
呃是不是就是我们放的那么显著

954
00:45:04,850 --> 00:45:06,710
是因为两部分合在一起了

955
00:45:06,710 --> 00:45:10,030
那实际上呢我们会发现一个叫做ith decoding

956
00:45:10,030 --> 00:45:12,590
是比ith decoding要显著的

957
00:45:12,590 --> 00:45:15,240
更有用的嗯这么一个现象

958
00:45:15,240 --> 00:45:17,930
然后哪怕仅有IGD呃

959
00:45:17,930 --> 00:45:18,870
tn的话

960
00:45:18,870 --> 00:45:20,750
也是比之前的一些重加权

961
00:45:20,750 --> 00:45:23,490
通过重加权的方法更有效果

962
00:45:26,540 --> 00:45:29,040
呃这边最后再简单做个总结

963
00:45:29,040 --> 00:45:33,280
就是呃我们这个工作有两个contribution

964
00:45:33,280 --> 00:45:37,660
第一个是我们呃为了解决这个token level bias

965
00:45:37,660 --> 00:45:39,480
in l for rec的一个问题

966
00:45:39,480 --> 00:45:42,300
我们introduce一个information game

967
00:45:42,300 --> 00:45:45,109
基于信息增益的token decisive modering方法

968
00:45:45,109 --> 00:45:49,970
那这个框架呢identifies就是呃找出了这个look

969
00:45:49,970 --> 00:45:54,340
Course of token level tuning bias and decoding bas

970
00:45:54,820 --> 00:45:57,100
第二我们还有另外一个contribution

971
00:45:57,100 --> 00:45:59,940
是我们根据这个token nel modeling

972
00:45:59,940 --> 00:46:03,830
我们propose的一个IGD这个重加权方法

973
00:46:03,830 --> 00:46:06,190
提供了一个很小的超参数

974
00:46:06,190 --> 00:46:08,680
然后去mitigate token deo base

975
00:46:08,680 --> 00:46:11,680
但是实际上取得一个相当稳定

976
00:46:11,680 --> 00:46:14,480
而且非常高额有效的一个呃

977
00:46:14,480 --> 00:46:18,560
呃呃推荐准确率一个提升

978
00:46:21,580 --> 00:46:24,520
呃还是有一些limitation和future work的

979
00:46:24,520 --> 00:46:27,540
我们这边一个工作主要的两个limitation

980
00:46:27,540 --> 00:46:29,180
以及对应的就出work格式

981
00:46:29,180 --> 00:46:31,800
第一个是我们这个所谓token desessive

982
00:46:31,800 --> 00:46:33,160
mos modeling方法

983
00:46:33,160 --> 00:46:35,560
实际上呃并没有

984
00:46:35,560 --> 00:46:38,400
我觉得并没有提到一个非常好的一个程度吧

985
00:46:38,400 --> 00:46:40,670
因为我没发现说IG地铁呃

986
00:46:40,670 --> 00:46:43,650
tony他只能使用一个binary呃

987
00:46:43,650 --> 00:46:45,670
binary reading scheme的一个主要原因

988
00:46:45,670 --> 00:46:49,359
就是因为这个IG的分布实在是太exponential

989
00:46:49,359 --> 00:46:52,479
那实际上如果我们未来尝试不同的一个信息

990
00:46:52,479 --> 00:46:53,279
增益的一个表征

991
00:46:53,279 --> 00:46:57,590
比如说呃基尼系数或者是这个额game racial

992
00:46:57,590 --> 00:46:59,540
或者一些其他的一个THETICS

993
00:46:59,540 --> 00:47:03,940
他们可能会呃对这个token session model里会更加频繁

994
00:47:03,940 --> 00:47:05,900
然后对应的一个REVIING方法

995
00:47:05,900 --> 00:47:07,740
它可能会更好设计一些

996
00:47:07,740 --> 00:47:10,500
然后第二个是我们的一个局限

997
00:47:10,500 --> 00:47:15,100
是我们的framework仍然局限在一个就是这个呃

998
00:47:15,100 --> 00:47:17,300
item是encoded as a title

999
00:47:17,300 --> 00:47:19,860
就是我们只用他的title作为一个呃

1000
00:47:19,860 --> 00:47:21,610
item的identity fire呃

1001
00:47:21,610 --> 00:47:24,410
如果是对应的其他生成式推荐方法

1002
00:47:24,410 --> 00:47:27,310
比如semantic etokens或者是multimodel tokens

1003
00:47:27,310 --> 00:47:28,750
他们的一个token sequence

1004
00:47:28,750 --> 00:47:30,130
可能会有不同的一个表征

1005
00:47:30,130 --> 00:47:32,990
他们的一个token section可能又会有不同的表征

1006
00:47:32,990 --> 00:47:36,765
这是一个更直接更加值得去探究的一个问题

1007
00:47:38,400 --> 00:47:41,720
呃以上是我的一些呃

1008
00:47:41,720 --> 00:47:43,120
就是简单的presentation

1009
00:47:43,120 --> 00:47:44,720
谢谢各位

1010
00:47:46,660 --> 00:47:49,500
好的谢谢子杰

1011
00:47:49,500 --> 00:47:51,100
那我们之后整理好提问

1012
00:47:51,100 --> 00:47:54,900
也会统一把提问转达给我们各位的讲者

1013
00:48:00,960 --> 00:48:03,380
好嘞

1014
00:48:03,380 --> 00:48:08,940
那下面我们有请中国科大的范承肖同学

1015
00:48:08,940 --> 00:48:09,780
范丞嗯

1016
00:48:09,780 --> 00:48:11,920
陈潇你好嗯

1017
00:48:11,920 --> 00:48:13,160
主持人您好

1018
00:48:13,160 --> 00:48:16,900
那各位子杰这边可以把摄像头关闭一下了

1019
00:48:16,900 --> 00:48:19,510
嗯谢谢嗯

1020
00:48:19,510 --> 00:48:22,230
好的啊

1021
00:48:22,230 --> 00:48:22,670
不好意思

1022
00:48:22,670 --> 00:48:23,450
陈晓打扰了

1023
00:48:23,450 --> 00:48:26,790
那我们有请您嗯好谢谢主持人

1024
00:48:26,790 --> 00:48:28,390
那各位听众朋友

1025
00:48:28,390 --> 00:48:29,670
各位与会的师兄师姐

1026
00:48:29,670 --> 00:48:30,670
大家下午好

1027
00:48:30,670 --> 00:48:33,150
我是来自中国科学技术大学的

1028
00:48:33,150 --> 00:48:34,910
硕士一年级的学生范陈晓

1029
00:48:34,910 --> 00:48:36,990
今天我为大家来介绍一下

1030
00:48:36,990 --> 00:48:40,550
这一篇已经被NEUROPES2025接收为sport light

1031
00:48:40,550 --> 00:48:43,450
一篇偏向应用性质的文章

1032
00:48:43,450 --> 00:48:48,290
名为生成式药物推荐的细粒度列表及对齐

1033
00:48:48,690 --> 00:48:51,170
首先我先为大家简单介绍一下

1034
00:48:51,170 --> 00:48:53,270
药物推荐任务的背景

1035
00:48:53,270 --> 00:48:56,230
以及我这一篇工作的动机

1036
00:48:56,230 --> 00:48:57,590
药物推荐任务

1037
00:48:57,590 --> 00:49:00,930
它旨在根据输入患者的健康信息

1038
00:49:00,930 --> 00:49:03,670
例如患者被诊断的疾病列表

1039
00:49:03,670 --> 00:49:05,470
所接受的手术列表

1040
00:49:05,470 --> 00:49:08,310
以及相关的人口学信息

1041
00:49:08,310 --> 00:49:12,320
来为患者输出一个安全且有效的药物集合

1042
00:49:12,320 --> 00:49:14,080
希望这个输出的药物集合

1043
00:49:14,080 --> 00:49:16,920
可以同时满足处方场景下的

1044
00:49:16,920 --> 00:49:20,100
准确性以及安全性的要求

1045
00:49:20,540 --> 00:49:24,420
而我们发现现有方法它是存在一定的不足的

1046
00:49:24,420 --> 00:49:27,580
这可以通过从任务层面和算法层面

1047
00:49:27,580 --> 00:49:29,240
进行一些划分

1048
00:49:29,240 --> 00:49:30,740
例如在任务层面

1049
00:49:30,740 --> 00:49:33,990
最为显著的便是POEMIZE预测的问题

1050
00:49:33,990 --> 00:49:37,250
无论是较早的这一类基于神经网络的方法

1051
00:49:37,250 --> 00:49:38,850
例如左下角的这个图

1052
00:49:38,850 --> 00:49:41,500
他们通常通过embedding之间的一些计算

1053
00:49:41,500 --> 00:49:43,950
来得到每一个药物的推荐分数

1054
00:49:43,950 --> 00:49:46,550
还是近年来基于大模型的一些方法

1055
00:49:46,550 --> 00:49:47,310
例如右下角

1056
00:49:47,310 --> 00:49:50,460
它是通过对候选集当中的所有药物

1057
00:49:50,460 --> 00:49:51,940
开展POEVIES的

1058
00:49:51,940 --> 00:49:53,100
yes or NO的判别

1059
00:49:53,100 --> 00:49:57,860
它们本质上都是在对每个药物进行单独的判断

1060
00:49:58,310 --> 00:50:01,770
而我们认为这种POEMISE预测的范式

1061
00:50:01,770 --> 00:50:04,630
他忽略了在实际的处方场景下

1062
00:50:04,630 --> 00:50:07,470
药物之间在有效性上的相互联系

1063
00:50:07,470 --> 00:50:10,420
以及在安全性上相互制衡的关系

1064
00:50:10,420 --> 00:50:12,860
而这使得他对药物之间

1065
00:50:12,860 --> 00:50:15,130
协同信息的建模不够充分

1066
00:50:15,130 --> 00:50:18,270
我认为我们可以将这类方法理解作是

1067
00:50:18,270 --> 00:50:20,410
它是在对每个药物进行判别

1068
00:50:20,410 --> 00:50:22,210
我认为这是不合适的

1069
00:50:22,210 --> 00:50:22,930
业务推销

1070
00:50:22,930 --> 00:50:27,130
它更应当是视作是一个list vise的决策过程

1071
00:50:27,799 --> 00:50:28,519
除此之外

1072
00:50:28,519 --> 00:50:31,359
我们还发现现有的药物推荐方法

1073
00:50:31,359 --> 00:50:34,400
对患者的健康状况建模力度是不足的

1074
00:50:34,400 --> 00:50:36,560
无论是nn pace的方法

1075
00:50:36,560 --> 00:50:40,430
他们可以通过网络架构的设计引入外部知识

1076
00:50:40,430 --> 00:50:43,770
例如通过GNN来编码分子亚结构信息

1077
00:50:43,770 --> 00:50:48,290
但是它们的输入仍然局限在结构化的输入当中

1078
00:50:48,290 --> 00:50:50,390
还是说基于大模型的方法

1079
00:50:50,390 --> 00:50:53,210
他们虽然可以利用丰富的文本理解能力

1080
00:50:53,210 --> 00:50:54,779
和自由的输入空间

1081
00:50:54,779 --> 00:50:57,739
来提供更为全面的患者健康建模

1082
00:50:57,739 --> 00:51:01,400
但是他们对输入的理解仍然停留在文本层面

1083
00:51:01,400 --> 00:51:04,460
缺乏对协同信号和外部知识的建模

1084
00:51:04,460 --> 00:51:07,560
因此我们也希望可以去去设计一个

1085
00:51:07,560 --> 00:51:08,980
更为充分的框架

1086
00:51:08,980 --> 00:51:11,460
同时利用这两两类方法的优点

1087
00:51:11,460 --> 00:51:15,140
来实现对患者信息更加全面的建模

1088
00:51:15,710 --> 00:51:18,170
而除了任务层面的不足之外

1089
00:51:18,170 --> 00:51:19,270
在算法层面

1090
00:51:19,270 --> 00:51:23,260
我们也发现NLP领域实现list wise的决策

1091
00:51:23,260 --> 00:51:25,340
通常是依赖上RL了

1092
00:51:25,340 --> 00:51:29,290
而现近一两年来备受关注的gr po

1093
00:51:29,290 --> 00:51:31,430
它尽管能取得一定的效果

1094
00:51:31,430 --> 00:51:34,900
但是他这种基于结果的奖励的形式

1095
00:51:34,900 --> 00:51:39,100
使得他对大模型生成的每一条回复当中的

1096
00:51:39,100 --> 00:51:40,000
所有token

1097
00:51:40,000 --> 00:51:43,250
实际上是赋予了一个相同大小的优势函数

1098
00:51:43,250 --> 00:51:46,210
也就是在这个loss function当中的这个advantage

1099
00:51:46,210 --> 00:51:47,130
AIT中

1100
00:51:47,130 --> 00:51:51,189
他对每一个回答I当中的所有token t

1101
00:51:51,189 --> 00:51:54,909
实际上都是使用这个回答I对应的回答质量

1102
00:51:54,909 --> 00:51:56,709
这个RI来进行衡量的

1103
00:51:56,709 --> 00:51:59,060
而没有去清晰的建模

1104
00:51:59,060 --> 00:52:00,360
在这个回答当中

1105
00:52:00,360 --> 00:52:04,830
不同的TOTOKEN他是否有在回答质量上面的差异

1106
00:52:04,830 --> 00:52:05,510
基于此

1107
00:52:05,510 --> 00:52:09,070
我们认为我们可能也需要一个更加细粒度的

1108
00:52:09,070 --> 00:52:10,450
step level的奖励信号

1109
00:52:10,450 --> 00:52:13,920
这也是目前GRPO所没有做到的

1110
00:52:14,200 --> 00:52:15,850
基于上述不足

1111
00:52:15,850 --> 00:52:19,570
嗯嗯我认为我们的工作可以总结为在任务层面

1112
00:52:19,570 --> 00:52:20,890
针对药物推荐任务

1113
00:52:20,890 --> 00:52:23,680
我们设计了一个list vise的药物推进框架

1114
00:52:23,680 --> 00:52:27,100
并且在建模患者信息时使用了多元表

1115
00:52:27,100 --> 00:52:29,240
多元知识融合的混合表征策略

1116
00:52:29,240 --> 00:52:31,760
来实现丰富的患者信息建模

1117
00:52:31,760 --> 00:52:33,040
而在算法层面

1118
00:52:33,040 --> 00:52:35,040
针对GRPO

1119
00:52:35,040 --> 00:52:37,940
基于结果奖励的粗力度信号的不足

1120
00:52:37,940 --> 00:52:40,240
我们设计了一个STATEWIGRPO

1121
00:52:40,240 --> 00:52:43,300
来实现细粒度的奖励对齐

1122
00:52:43,700 --> 00:52:46,160
那么在展开具体的方法之前

1123
00:52:46,160 --> 00:52:50,319
先简单为大家介绍一下药物推荐的任务定义

1124
00:52:50,319 --> 00:52:53,319
对于数据集当中的每个患者

1125
00:52:53,319 --> 00:52:56,199
他都将对应有若干次入院记录

1126
00:52:56,199 --> 00:52:58,880
而对患者的每次入院的visit

1127
00:52:58,880 --> 00:53:02,300
我们都可以将其中的信息划分为三个部分

1128
00:53:02,300 --> 00:53:05,760
首先是由莫代ho的向量表示的结构化信息

1129
00:53:05,760 --> 00:53:09,899
例如患者所接受的手术所诊断的疾病

1130
00:53:09,899 --> 00:53:14,120
以及包括年龄性别在内的相关人口学信息

1131
00:53:14,120 --> 00:53:17,280
第二部分是由自然文本形式所表示的

1132
00:53:17,280 --> 00:53:21,530
非结构化文本信息来表征患者特意的健康状况

1133
00:53:21,530 --> 00:53:23,110
例如患者的既往病史

1134
00:53:23,110 --> 00:53:24,130
过敏史等等

1135
00:53:24,130 --> 00:53:27,660
这一类难以用结构化信息直接进行描述的

1136
00:53:27,660 --> 00:53:29,620
而最后则是在数据集当中

1137
00:53:29,620 --> 00:53:31,020
患者被处方的药物集合

1138
00:53:31,020 --> 00:53:34,190
这就作为我们推荐任务的ground truth

1139
00:53:34,510 --> 00:53:35,870
对于药物推进任务

1140
00:53:35,870 --> 00:53:38,720
他就希望对每一次入院的wait

1141
00:53:38,720 --> 00:53:39,160
都能够

1142
00:53:39,160 --> 00:53:42,360
根据患者的结构化和非结构化的健康状况

1143
00:53:42,360 --> 00:53:44,140
去推荐一个药物集合

1144
00:53:44,140 --> 00:53:47,200
它能够保证和具体

1145
00:53:47,200 --> 00:53:50,000
也就是双方的这一个的相似性的同时

1146
00:53:50,000 --> 00:53:52,960
可以尽可能的减少推荐集合当中

1147
00:53:52,960 --> 00:53:55,170
药物相互作用比率的大小

1148
00:53:55,700 --> 00:53:57,800
在这个任务定义之下

1149
00:53:57,800 --> 00:54:01,820
以及希望解决之前POEVISE预测范式这个不足的

1150
00:54:01,820 --> 00:54:02,560
这个目的

1151
00:54:02,560 --> 00:54:06,330
我们设计了一个两阶段的推进框框架

1152
00:54:06,330 --> 00:54:08,380
整体也就是一个先进

1153
00:54:08,380 --> 00:54:11,440
先对候选集当中所有的药物进行初筛

1154
00:54:11,440 --> 00:54:15,590
然后再从LISWISE的层面进行精细调整

1155
00:54:15,590 --> 00:54:16,710
的一个思路

1156
00:54:16,710 --> 00:54:17,630
具体而言

1157
00:54:17,630 --> 00:54:19,310
我们先训练了一个classifier

1158
00:54:19,310 --> 00:54:22,930
然后对候选集当中的所有药物进行分类

1159
00:54:22,930 --> 00:54:25,200
得到初步预测的一个药物列表

1160
00:54:25,200 --> 00:54:29,440
然后再通过上强化训练一个list买的决策模型

1161
00:54:29,440 --> 00:54:33,300
对其模型在考虑药物之间协同作用的处方

1162
00:54:33,300 --> 00:54:33,960
准确性

1163
00:54:33,960 --> 00:54:37,040
以及考虑药物之间不良相互作用的安全性

1164
00:54:37,040 --> 00:54:37,960
推进的能力

1165
00:54:37,960 --> 00:54:39,960
来对NP进行修正

1166
00:54:39,960 --> 00:54:43,470
最后得到最终推荐的这一个药物列表

1167
00:54:43,470 --> 00:54:46,170
而此处它药物推荐的效果

1168
00:54:46,170 --> 00:54:49,040
就依赖于对患者信息的全面建模

1169
00:54:49,040 --> 00:54:53,400
出于之前想要解决患者表达瓶颈的这一个目的

1170
00:54:53,400 --> 00:54:55,000
我们在患者建模时

1171
00:54:55,000 --> 00:54:57,240
使用了一套多元知识融合的框架

1172
00:54:57,240 --> 00:54:58,880
具体体现在蓝色部分

1173
00:54:58,880 --> 00:55:01,520
我们既利用了大模型的文本理解能力

1174
00:55:01,520 --> 00:55:03,400
使用了一些非结构化的文本信息

1175
00:55:03,400 --> 00:55:06,290
来建模患者特异的健康状况

1176
00:55:06,290 --> 00:55:08,010
例如患者的过敏史

1177
00:55:08,010 --> 00:55:10,500
患者入院时正在服用的药物等等

1178
00:55:10,500 --> 00:55:12,380
同时对于结构化信息

1179
00:55:12,380 --> 00:55:14,500
我们也参照之前类似于lara code

1180
00:55:14,500 --> 00:55:16,320
LLLNE类文章的思路

1181
00:55:16,320 --> 00:55:20,380
将包含有协同知识和外部知识的外部embedding

1182
00:55:20,380 --> 00:55:22,940
把它映射到大模型的embedding空间当中

1183
00:55:22,940 --> 00:55:26,740
来实现一个更为丰富的患者情况建模

1184
00:55:27,290 --> 00:55:31,960
而对于前述所提到的算法层面的GRPO

1185
00:55:31,960 --> 00:55:33,600
基于结果奖励的不足

1186
00:55:33,600 --> 00:55:36,020
我们在训练list white policy的同时

1187
00:55:36,020 --> 00:55:38,240
设计了一个stable white g r p o

1188
00:55:38,240 --> 00:55:40,520
那么以药物推荐场景为例

1189
00:55:40,520 --> 00:55:43,380
传统的GRPO如左下图所示

1190
00:55:43,380 --> 00:55:45,080
对于每一个回答

1191
00:55:45,080 --> 00:55:49,080
尽管每个回答当中可能包含有多个药

1192
00:55:49,080 --> 00:55:51,480
尽管每个回答当中可能包含多个药物

1193
00:55:51,480 --> 00:55:54,460
而每个药物它的优劣可能是不同的

1194
00:55:54,460 --> 00:55:59,040
但是gr po级与列表级的回答质量计算reward

1195
00:55:59,040 --> 00:56:01,260
使得对一个回答当中

1196
00:56:01,260 --> 00:56:04,820
所有的token都赋予一个相同大小的优势函数

1197
00:56:04,820 --> 00:56:07,420
这种方式便忽略了不同token之间

1198
00:56:07,420 --> 00:56:08,770
回答质量的差异

1199
00:56:08,770 --> 00:56:10,850
例如对于左下图可能有一个回答

1200
00:56:10,850 --> 00:56:13,690
他同时回答了M1M2和M3这三个药物

1201
00:56:13,690 --> 00:56:16,010
但是GRPU基于结果的奖励

1202
00:56:16,010 --> 00:56:17,850
会使得可能这三个药物

1203
00:56:17,850 --> 00:56:19,290
他们的回答质量是不一样的

1204
00:56:19,290 --> 00:56:22,450
但是他都赋予它一个相同大小的优势函数

1205
00:56:22,450 --> 00:56:25,660
我们认为这个建模是不不太合适的

1206
00:56:25,660 --> 00:56:28,100
所以我们所涉及的statewide g r p o

1207
00:56:28,100 --> 00:56:30,920
需要完成的就是对回答进行划分

1208
00:56:30,920 --> 00:56:33,400
例如此处对同一个回答M1M2M三

1209
00:56:33,400 --> 00:56:35,640
我们对它划分成了三个step

1210
00:56:35,640 --> 00:56:39,920
进而去评估每一个step的回答质量的差

1211
00:56:39,920 --> 00:56:42,280
IE赋予一个step label的奖励信号

1212
00:56:42,280 --> 00:56:46,160
将其与传统GRPO列表及的优势函数相结合

1213
00:56:46,160 --> 00:56:51,340
来实现更为稀利度的奖励信号的一个赋予

1214
00:56:51,540 --> 00:56:53,620
体现在数学形式上

1215
00:56:53,620 --> 00:56:56,560
就可以认为我们的STABILIGRPO

1216
00:56:56,560 --> 00:56:59,390
对大模型生成的每一个回答oi

1217
00:56:59,390 --> 00:57:00,970
进行了状态划分

1218
00:57:00,970 --> 00:57:02,870
将其划分为nx step

1219
00:57:02,870 --> 00:57:05,750
每个step对应模型的一个回复的状态

1220
00:57:05,750 --> 00:57:08,070
在这个状态划分的思路下

1221
00:57:08,070 --> 00:57:11,330
我们可以在GRPO的优势函数当中

1222
00:57:11,330 --> 00:57:14,569
除了这一个基于结果的列表及奖励之外

1223
00:57:14,569 --> 00:57:16,769
再通过奖励塑形的思路

1224
00:57:16,769 --> 00:57:21,320
引入一个基于相邻两个step对应状态

1225
00:57:21,320 --> 00:57:24,040
质量差异的这么一个奖励信号

1226
00:57:24,040 --> 00:57:26,060
来建模更加细粒度的奖励信号

1227
00:57:26,060 --> 00:57:28,090
也就是这一个函数F

1228
00:57:28,740 --> 00:57:30,940
而上述的这种设定

1229
00:57:30,940 --> 00:57:34,730
驱使着我们需要去衡量每个step的回答质量

1230
00:57:34,730 --> 00:57:38,150
那么便可以很自然的引入一一个势能函数

1231
00:57:38,150 --> 00:57:39,870
势能函数的差值

1232
00:57:39,870 --> 00:57:42,680
也就是这里的FN减去SN减一

1233
00:57:42,680 --> 00:57:46,340
便可以理解做是step n对应的那几个token

1234
00:57:46,340 --> 00:57:48,480
对模型质量带来的改变

1235
00:57:48,480 --> 00:57:52,280
那么便可以很自然地用于衡量这几个token的回

1236
00:57:52,280 --> 00:57:52,940
答质量

1237
00:57:52,940 --> 00:57:57,100
为优势函数提供更加细的奖励信号

1238
00:57:57,830 --> 00:58:00,670
将之前所定义的step by的gr po

1239
00:58:00,670 --> 00:58:03,080
应用在药物推荐任务当中的话

1240
00:58:03,080 --> 00:58:05,480
我们便可以将LLM生成

1241
00:58:05,480 --> 00:58:08,050
一次性生成一个药物列表的过程

1242
00:58:08,050 --> 00:58:11,200
视作是一个状态转移的过程

1243
00:58:11,200 --> 00:58:12,860
如下图A所示

1244
00:58:12,860 --> 00:58:14,280
对一个药物列表

1245
00:58:14,280 --> 00:58:17,590
我们可以对药物名称按照step进行划分

1246
00:58:17,590 --> 00:58:19,470
每个step包含多个token

1247
00:58:19,470 --> 00:58:21,250
也就是对应一个药物名称

1248
00:58:21,250 --> 00:58:25,160
也是在此处模型一次性生成了一串药物名称

1249
00:58:25,160 --> 00:58:27,380
我们可以将其根据NM1M2M三

1250
00:58:27,380 --> 00:58:29,580
这三个药物划分成三个step

1251
00:58:29,580 --> 00:58:33,180
也就是每个药物对应一个回答的step

1252
00:58:33,180 --> 00:58:35,660
那么模型的状态便可以衡量

1253
00:58:35,660 --> 00:58:39,580
做回答到当前step为止的药物列表

1254
00:58:39,580 --> 00:58:41,140
而状态的转移变

1255
00:58:41,140 --> 00:58:44,300
取决于下一个step对应的药物名称

1256
00:58:44,300 --> 00:58:45,649
对药物列表的修正

1257
00:58:45,649 --> 00:58:49,609
也就是说模型的第一个step它如这个B图所示

1258
00:58:49,609 --> 00:58:52,410
模型的第一个step他回答了药物M1

1259
00:58:52,410 --> 00:58:53,990
那么这个M1的回答

1260
00:58:53,990 --> 00:58:56,930
使得模型当前的状态从大N0

1261
00:58:56,930 --> 00:58:58,210
转移到了大NM1

1262
00:58:58,210 --> 00:59:00,130
而下一个又回答了药物NM2

1263
00:59:00,130 --> 00:59:03,289
使得了状态从M1转移到了M2

1264
00:59:03,289 --> 00:59:04,790
基于此

1265
00:59:04,790 --> 00:59:09,630
我们便可以对此处每一个状态对应的药物列表

1266
00:59:09,630 --> 00:59:11,850
来去衡量其回答质量

1267
00:59:11,850 --> 00:59:14,370
定义了一个从准确性

1268
00:59:14,370 --> 00:59:14,870
安全性

1269
00:59:14,870 --> 00:59:18,110
合法性来定义的势能函数范围

1270
00:59:18,110 --> 00:59:22,390
那么我们便可以通过这个势能函数F

1271
00:59:22,390 --> 00:59:27,220
来衡量每一个药物状态的质量大小

1272
00:59:27,220 --> 00:59:29,080
那么通过势能

1273
00:59:29,080 --> 00:59:33,050
当前状态与前一个状态的势能函数的差值

1274
00:59:33,050 --> 00:59:36,370
便可用于衡量对应step的那几个token

1275
00:59:36,370 --> 00:59:37,630
的回答质量大小

1276
00:59:37,630 --> 00:59:42,230
由此便可以为原本基于结果的优势函数

1277
00:59:42,230 --> 00:59:46,029
引入了一个step level的奖励信号

1278
00:59:46,349 --> 00:59:48,469
以上是对方法的一些介绍

1279
00:59:48,469 --> 00:59:52,120
接下来为大家简单带过一下实验结果

1280
00:59:52,120 --> 00:59:54,560
我们开展了以下三部分实验

1281
00:59:54,560 --> 00:59:55,840
分别是准确性

1282
00:59:55,840 --> 00:59:58,040
安全可控性和泛化性实验

1283
00:59:58,040 --> 00:59:59,980
准确性如右表所示

1284
00:59:59,980 --> 01:00:02,769
可以发现我们的方法F基于其他的

1285
01:00:02,769 --> 01:00:04,549
相比起其他的baseline

1286
01:00:04,549 --> 01:00:08,540
在准确性指标上均取得了较为显著的提升

1287
01:00:08,540 --> 01:00:11,360
而开展安全可控性的原因在于

1288
01:00:11,360 --> 01:00:13,000
与训练集当中的ground truth

1289
01:00:13,000 --> 01:00:15,580
它是包含有一定的药物相互作用的

1290
01:00:15,580 --> 01:00:18,490
例如我们这里所使用的训练集LINUX3

1291
01:00:18,490 --> 01:00:22,450
它包含大概有13.6%的相互作用率

1292
01:00:22,450 --> 01:00:24,970
因此你和数据集并不能满足

1293
01:00:24,970 --> 01:00:26,850
安全推荐的一个目的

1294
01:00:26,850 --> 01:00:29,590
也就是说准确性和安全性之间

1295
01:00:29,590 --> 01:00:32,030
它是存在着一个cheat off

1296
01:00:32,030 --> 01:00:33,470
通过实验我们发现

1297
01:00:33,470 --> 01:00:37,210
我们可以通过调整强化学习当中的权重

1298
01:00:37,210 --> 01:00:42,410
来在实现不同安全性限制下面的

1299
01:00:42,410 --> 01:00:44,330
准确性和安全性之间的平衡

1300
01:00:44,330 --> 01:00:46,320
而我们的方法的这条红线

1301
01:00:46,320 --> 01:00:49,100
还是始终优于其他相关贝拉的

1302
01:00:49,100 --> 01:00:51,460
最后我们在外部数据集上

1303
01:00:51,460 --> 01:00:53,220
也测试了我们实验的泛化性

1304
01:00:53,220 --> 01:00:55,930
同样也发现其优于其他数据集

1305
01:00:55,930 --> 01:01:00,370
同时针对我们所设计的STABILIGRPO

1306
01:01:00,370 --> 01:01:03,649
我们也将它和传统的gr po进行了一定的比较

1307
01:01:03,649 --> 01:01:08,089
我们可以发现它可以取得更快的reward的上升

1308
01:01:08,089 --> 01:01:09,789
以及更稳定的学习

1309
01:01:09,789 --> 01:01:14,380
这也证明了我们在基于结果的优劣势函数当中

1310
01:01:14,380 --> 01:01:17,200
引入了一个stable wide c力度的奖励信号

1311
01:01:17,200 --> 01:01:19,410
这个方法它是有效的

1312
01:01:19,410 --> 01:01:21,090
而其他的实验细节

1313
01:01:21,090 --> 01:01:23,490
例如完整的消融实验和catch study

1314
01:01:23,490 --> 01:01:24,370
由于时间限制

1315
01:01:24,370 --> 01:01:27,490
就不太像这里进行展示了

1316
01:01:27,770 --> 01:01:29,550
总结一下我们的工作

1317
01:01:29,550 --> 01:01:33,420
我们提出了一个名为friend list wise药物推进框架

1318
01:01:33,420 --> 01:01:38,540
将药物推进过程视作是一个主要务的决策过程

1319
01:01:38,540 --> 01:01:42,240
在框架当中我们引入了step white g r p o利

1320
01:01:42,240 --> 01:01:45,390
利用了基于概率的奖励塑形

1321
01:01:45,390 --> 01:01:48,069
来提供更加细粒度的奖励信号

1322
01:01:48,069 --> 01:01:49,909
同时通过一系列实验

1323
01:01:49,909 --> 01:01:51,709
我们证明了我们的方法在有效性

1324
01:01:51,709 --> 01:01:54,779
安全可控性和泛化性上的优异表现

1325
01:01:54,779 --> 01:01:57,459
嗯我的方法我的分享就是这样

1326
01:01:57,459 --> 01:01:58,299
那关于论文

1327
01:01:58,299 --> 01:02:00,659
其他更多细节也欢迎大家查阅原文

1328
01:02:00,659 --> 01:02:02,400
或者是直接和我联系

1329
01:02:02,400 --> 01:02:03,580
欢迎大家向我提问

1330
01:02:03,580 --> 01:02:04,740
大家圣地牙哥见

1331
01:02:04,740 --> 01:02:06,540
谢谢

1332
01:02:21,280 --> 01:02:26,460
好嗯谢谢谢谢陈晓

1333
01:02:43,160 --> 01:02:45,939
哎你好嗯

1334
01:02:45,939 --> 01:02:47,259
小宇博士您好

1335
01:02:47,259 --> 01:02:48,099
您在吗

1336
01:02:48,099 --> 01:02:48,939
唉好的

1337
01:02:48,939 --> 01:02:49,919
我在嗯

1338
01:02:49,919 --> 01:02:51,699
咱们可以共享一下屏幕

1339
01:02:51,699 --> 01:02:52,659
是要看视频

1340
01:02:52,659 --> 01:02:54,120
是

1341
01:02:57,440 --> 01:02:58,240
OK大家好

1342
01:02:58,240 --> 01:03:00,140
我是这个中国科学技术大学

1343
01:03:00,140 --> 01:03:01,920
一年级的博士生孔晓宇

1344
01:03:01,920 --> 01:03:04,010
然后这一期给大家分享一下

1345
01:03:04,010 --> 01:03:05,490
我们历次的新工作室

1346
01:03:05,490 --> 01:03:06,870
Think before recommendation

1347
01:03:06,870 --> 01:03:10,570
这是一个把reasoning用在reading prediction

1348
01:03:10,570 --> 01:03:12,070
任务上的一个新工作

1349
01:03:12,070 --> 01:03:16,470
然后呃最近这是我们上半年的一些研究

1350
01:03:16,470 --> 01:03:19,950
然后下半年我主要在follow在focus on

1351
01:03:19,950 --> 01:03:21,870
这些就是狭义的生日推荐

1352
01:03:21,870 --> 01:03:25,030
上面就是7月SID的这种生日推荐

1353
01:03:25,030 --> 01:03:27,310
然后也做了一些新的工作

1354
01:03:27,310 --> 01:03:28,430
包括这个mini玩热

1355
01:03:28,430 --> 01:03:30,230
欢迎大家关注

1356
01:03:30,230 --> 01:03:32,570
然后我们说一下这个工作

1357
01:03:32,570 --> 01:03:37,110
和先前两位同学聊的可能范式有点不一样

1358
01:03:37,110 --> 01:03:40,030
就我们这个任务是在呃reading prediction

1359
01:03:40,030 --> 01:03:41,970
在reading prediction任务上做的

1360
01:03:41,970 --> 01:03:46,650
然后这个比较像这个像刚才的R方EC

1361
01:03:46,650 --> 01:03:49,570
可能是像一个在圣师召回的任务

1362
01:03:49,570 --> 01:03:52,270
然后我们这边可能就比较偏向于金牌的

1363
01:03:52,270 --> 01:03:54,240
后续的这些操作

1364
01:03:54,240 --> 01:03:55,880
然后我们这边的motivation

1365
01:03:55,880 --> 01:03:58,280
主要是利用大模型的推理能力来去强化

1366
01:03:58,280 --> 01:04:01,450
我们现在的这个reading prediction的这个任务

1367
01:04:01,450 --> 01:04:02,930
我们这里的input

1368
01:04:02,930 --> 01:04:08,050
其实就是包括呃用户的这种个人的provide

1369
01:04:08,050 --> 01:04:10,620
包括一些他的呃个人的属性

1370
01:04:10,620 --> 01:04:12,040
可能是年龄呃

1371
01:04:12,040 --> 01:04:13,120
性别这些信息

1372
01:04:13,120 --> 01:04:17,720
然后呃后面还有跟着一个user的交互的历史

1373
01:04:17,720 --> 01:04:22,410
这边是以文本形式组织了一个呃用户的历史呃

1374
01:04:22,410 --> 01:04:23,570
作为input给到用户

1375
01:04:23,570 --> 01:04:25,150
然后还有一个input的是

1376
01:04:25,150 --> 01:04:27,850
他需要作为打分的这个target的item

1377
01:04:27,850 --> 01:04:32,319
这里target item的信息就包括呃他的meta信息可能有title

1378
01:04:32,319 --> 01:04:34,359
description category呃

1379
01:04:34,359 --> 01:04:36,259
包括品牌等等一些信息

1380
01:04:36,259 --> 01:04:39,140
因为先前的大模型推理能力

1381
01:04:39,140 --> 01:04:41,300
做这个去强化评分预测

1382
01:04:41,300 --> 01:04:43,180
其实已经有很多的一些尝试了

1383
01:04:43,180 --> 01:04:46,390
包括额啊

1384
01:04:46,390 --> 01:04:47,790
他们的主要主要的思路就是

1385
01:04:47,790 --> 01:04:50,230
将先前教师这种推理能力

1386
01:04:50,230 --> 01:04:52,220
去蒸馏到小模型里面去

1387
01:04:52,220 --> 01:04:55,620
呃先前的范式呃已经有不少的工作了

1388
01:04:55,620 --> 01:04:57,200
然后他们呃

1389
01:04:57,200 --> 01:04:59,720
第一步就主要是分别给这些教教师模型

1390
01:04:59,720 --> 01:05:01,930
去做这个评分的子任务

1391
01:05:01,930 --> 01:05:05,020
这个子任务他们通过呃不断的这种试错

1392
01:05:05,020 --> 01:05:07,320
总结出了差不多三个范式

1393
01:05:07,320 --> 01:05:11,630
就是把单单单部的这种评分预测拆解成三步

1394
01:05:11,630 --> 01:05:15,000
第一步是呃去extract user interest

1395
01:05:15,000 --> 01:05:19,920
让这个教师模型去根据输入的用户历史

1396
01:05:19,920 --> 01:05:24,040
item和用户的profile去提取用户的个人兴趣

1397
01:05:24,040 --> 01:05:27,319
做出一个呃这种trace的总结

1398
01:05:27,319 --> 01:05:31,950
第二步是做呃summarize的target item这种feature呃

1399
01:05:31,950 --> 01:05:36,150
同样是把他给他item的信息给到这种teacher model

1400
01:05:36,150 --> 01:05:37,790
让他去呃

1401
01:05:37,790 --> 01:05:41,890
总总结这个他他给的item的这种这种特征

1402
01:05:41,890 --> 01:05:44,530
第三步是做这种匹配度的分析

1403
01:05:44,530 --> 01:05:47,930
把刚才的这种呃同样是teacher model呃

1404
01:05:47,930 --> 01:05:51,530
得到的user interest和他总结出的这种target feature

1405
01:05:51,530 --> 01:05:53,570
去做这种匹配度的分析

1406
01:05:53,570 --> 01:05:56,660
然后我们做完这个第三步之后

1407
01:05:56,660 --> 01:05:58,840
会分别得到三种不同的轨迹

1408
01:05:58,840 --> 01:06:02,160
而是呃一种就三种不同的这种

1409
01:06:02,160 --> 01:06:04,280
构建了三种instruction的这种数据集

1410
01:06:04,280 --> 01:06:08,730
第一种是呃去提取用户这种个人兴趣的数据

1411
01:06:08,730 --> 01:06:10,930
第二种是总结商品的偏好

1412
01:06:10,930 --> 01:06:13,410
第三种是做这种匹配度分析的三种数据集

1413
01:06:13,410 --> 01:06:17,029
我去分别去做三个蒸馏的这种小任务

1414
01:06:17,029 --> 01:06:20,369
把三三个不同的这种T恤model的这种能力

1415
01:06:20,369 --> 01:06:23,640
去分别的去蒸馏到下游的三个小模型里面去

1416
01:06:23,640 --> 01:06:26,300
然后在小这三个小模型的基础上

1417
01:06:26,300 --> 01:06:28,000
teacher student模型的基础上

1418
01:06:28,000 --> 01:06:30,260
我再举引入第四个模型

1419
01:06:30,260 --> 01:06:34,050
这是一个比较呃稍微有点复杂的这种排版

1420
01:06:34,050 --> 01:06:35,770
去把前面三个的小小模型

1421
01:06:35,770 --> 01:06:38,750
teachers or students这种模型的轨迹串起来

1422
01:06:38,750 --> 01:06:41,110
去给最后一个模型做呃

1423
01:06:41,110 --> 01:06:42,810
predict rate这个任任务去

1424
01:06:42,810 --> 01:06:45,880
可能再去接一个这种llage processor这种头

1425
01:06:45,880 --> 01:06:48,100
去专门的去预测呃

1426
01:06:48,100 --> 01:06:50,810
真正的和用户的这种打分任务去靠近

1427
01:06:50,810 --> 01:06:53,530
所以刚才这个基于教师蒸馏

1428
01:06:53,530 --> 01:06:54,690
这种范式可能是比较冗余的

1429
01:06:54,690 --> 01:06:57,670
而且他有两个非常强烈的信念

1430
01:06:57,670 --> 01:07:01,930
一种是他首先认为teacher model的推理轨迹都是好的

1431
01:07:01,930 --> 01:07:04,670
像我们刚才呃阿方EC的呃

1432
01:07:04,670 --> 01:07:06,770
这个作者他也谈论到这个问题

1433
01:07:06,770 --> 01:07:09,550
就是说呃我们推荐我们这种大模型

1434
01:07:09,550 --> 01:07:12,270
它本身的这种训练语调和推荐场景差异很大

1435
01:07:12,270 --> 01:07:15,320
所以其实大模型的这种就算是教师模型

1436
01:07:15,320 --> 01:07:18,240
它呃这种比较强大的月闭源模型

1437
01:07:18,240 --> 01:07:20,320
它在推理真实的推推理场景里

1438
01:07:20,320 --> 01:07:22,890
它根据用户的呃偏好呃

1439
01:07:22,890 --> 01:07:25,770
历史所提炼出的用户偏好也未必是最优的

1440
01:07:25,770 --> 01:07:28,730
呃包括我们自己也做了很多相应的一些小实验

1441
01:07:28,730 --> 01:07:31,290
就是你把提前model的这种推理轨迹

1442
01:07:31,290 --> 01:07:35,310
你直接放到小模型的这种呃from里面去

1443
01:07:35,310 --> 01:07:37,030
让他去做呃

1444
01:07:37,030 --> 01:07:38,490
下一步的下游任务

1445
01:07:38,490 --> 01:07:41,210
这一步很可能对于模型的整体的表现

1446
01:07:41,210 --> 01:07:42,090
反而是一个负提升

1447
01:07:42,090 --> 01:07:43,850
因为呃大模型本身

1448
01:07:43,850 --> 01:07:45,970
它不是一个针对推荐场景做的

1449
01:07:45,970 --> 01:07:47,400
特定优化的这种事

1450
01:07:47,400 --> 01:07:50,810
然后呃我们在强化学习出来以前

1451
01:07:50,810 --> 01:07:53,930
其实我们也没有办法对这种呃

1452
01:07:53,930 --> 01:07:55,130
推理轨迹去做优化

1453
01:07:55,130 --> 01:07:57,950
这是一个静态的这种问题

1454
01:07:57,950 --> 01:07:59,510
他的首先教师模型

1455
01:07:59,510 --> 01:08:00,230
它推推荐场景

1456
01:08:00,230 --> 01:08:01,230
他的能力就比较有限

1457
01:08:01,230 --> 01:08:04,980
他自己得到了这种蒸馏数据教育小模型

1458
01:08:04,980 --> 01:08:07,500
他自己在模型额推荐任务上表现就很一般

1459
01:08:07,500 --> 01:08:08,420
针对到小模型之后

1460
01:08:08,420 --> 01:08:10,710
这种能力会反而会受到进一步的削弱

1461
01:08:10,710 --> 01:08:13,150
呃其次是他我们首先这种强烈的鲜艳

1462
01:08:13,150 --> 01:08:16,779
是说我们觉得呃teacher model的这种reasoning trace

1463
01:08:16,779 --> 01:08:17,380
它就是好的

1464
01:08:17,380 --> 01:08:18,060
但是这种trace

1465
01:08:18,060 --> 01:08:21,479
是无法在下游里面进一步优化的

1466
01:08:21,679 --> 01:08:23,448
呃第三个问题是呃

1467
01:08:23,448 --> 01:08:25,389
通过这种蒸馏的方式得到这种啊

1468
01:08:25,389 --> 01:08:26,629
reason trace呃

1469
01:08:26,629 --> 01:08:28,069
先前的一些研究已经很表明了

1470
01:08:28,069 --> 01:08:30,109
说这种SIT蒸馏过来的能力

1471
01:08:30,109 --> 01:08:31,950
很可能只是一种呃

1472
01:08:31,950 --> 01:08:35,370
表层的这种去模仿他的teacher的这种推理

1473
01:08:35,370 --> 01:08:38,029
而实际上对他真正的这种推荐的能力

1474
01:08:38,029 --> 01:08:39,380
并没有太大的提升

1475
01:08:39,380 --> 01:08:40,939
然后我们受到额

1476
01:08:40,939 --> 01:08:41,939
因为这是上半年的工作

1477
01:08:41,939 --> 01:08:43,809
我们说到年前可能是呃

1478
01:08:43,809 --> 01:08:46,709
呃deep fak r one这种这些工作的启发

1479
01:08:46,709 --> 01:08:49,939
我们认为说可以去结合GRPO强化学习呃

1480
01:08:49,939 --> 01:08:52,540
去根据这种真实的推荐场景

1481
01:08:52,540 --> 01:08:54,660
去真正的把pass k变成pass one

1482
01:08:54,660 --> 01:08:57,020
去优化我们的模型

1483
01:08:57,020 --> 01:09:00,290
去真正的做这些能对呃

1484
01:09:00,290 --> 01:09:02,250
真实的推荐有益的这种推理

1485
01:09:02,250 --> 01:09:03,870
这就是我们的motivation

1486
01:09:03,870 --> 01:09:06,910
然后呃主要是做了

1487
01:09:06,910 --> 01:09:10,410
这里我们先去做一个纯基于RL反式强化学习

1488
01:09:10,410 --> 01:09:12,130
反式的这种react zo的这种范式

1489
01:09:12,130 --> 01:09:15,029
这个模这个这个这个框架主要做了两件事

1490
01:09:15,029 --> 01:09:17,908
第一件事情就是基于强化学习去做自主优化

1491
01:09:17,908 --> 01:09:19,099
像刚才的呃

1492
01:09:19,099 --> 01:09:21,259
R方AC可能跟我们是这种inside的是一样的

1493
01:09:21,259 --> 01:09:23,198
就是说我们基于强化学习

1494
01:09:23,198 --> 01:09:24,839
去摆脱教师模型的依赖

1495
01:09:24,839 --> 01:09:25,939
通过这种rule out

1496
01:09:25,939 --> 01:09:28,510
我们把我们去设定一个呃

1497
01:09:28,510 --> 01:09:30,330
对于推荐场景是面向推荐的

1498
01:09:30,330 --> 01:09:32,630
这种这种这种这种规则奖励

1499
01:09:32,630 --> 01:09:35,590
去同时去优化我们的真实的目标

1500
01:09:35,590 --> 01:09:37,850
他们可能就是预测next item

1501
01:09:37,850 --> 01:09:40,510
然后我们这里可能是这个这种reading的prediction

1502
01:09:40,510 --> 01:09:42,500
准确度去面向这些指标

1503
01:09:42,500 --> 01:09:44,300
去同时优化我们的推理轨迹

1504
01:09:44,300 --> 01:09:46,250
和我们的真实的这种预测

1505
01:09:46,250 --> 01:09:49,729
然后来来以死以推理轨迹去指导指导我们

1506
01:09:49,729 --> 01:09:51,170
去我们的模型去如何学会

1507
01:09:51,170 --> 01:09:54,189
真正对于推荐有益的这种这种推理过程

1508
01:09:54,189 --> 01:09:56,510
然后第二个就是这种以推荐为中心的推理

1509
01:09:56,510 --> 01:09:57,610
就是我们刚才说的

1510
01:09:57,610 --> 01:09:59,610
我们以推荐max matrix为目标

1511
01:09:59,610 --> 01:10:02,470
去同时做这种join的这种优化

1512
01:10:02,950 --> 01:10:06,070
那首先我们的框架其实还蛮简单的呃

1513
01:10:06,070 --> 01:10:06,910
Reply roll

1514
01:10:06,910 --> 01:10:10,150
这个是基于纯强化学习来去呃

1515
01:10:10,150 --> 01:10:12,670
我们设定一个面向推荐的奖励系统

1516
01:10:12,670 --> 01:10:15,730
来鼓励模型去做做这种结构化思考这种预测

1517
01:10:15,730 --> 01:10:18,790
因为先前我们呃先前的这种呃

1518
01:10:18,790 --> 01:10:22,390
基于推理强化来去做reading prediction这种任务的

1519
01:10:22,390 --> 01:10:24,290
先前很多研究都已经验证过

1520
01:10:24,290 --> 01:10:25,760
就说是指这种

1521
01:10:25,760 --> 01:10:28,760
我们把单个的reading prediction任务来去拆解成

1522
01:10:28,760 --> 01:10:32,320
这种结构化的多部的这种思考

1523
01:10:32,320 --> 01:10:35,240
反而会最终提提升最终的这种推荐的准确度

1524
01:10:35,240 --> 01:10:39,890
就是我们我们当时最开始的尝试也是呃

1525
01:10:39,890 --> 01:10:42,290
把单步的这种无结构的推理

1526
01:10:42,290 --> 01:10:44,790
我们通过显示这种弱的设置

1527
01:10:44,790 --> 01:10:45,910
把它分解成四部

1528
01:10:45,910 --> 01:10:47,299
我们是PTO前面的工作

1529
01:10:47,299 --> 01:10:50,579
第一步是这种呃ANNA去分析了用户的偏好

1530
01:10:50,579 --> 01:10:53,760
第二步是呃对于item的这种特征进行提取

1531
01:10:53,760 --> 01:10:57,240
第三步是我们把前面两步得到的呃

1532
01:10:57,240 --> 01:11:00,180
得到的这种呃分析的特征

1533
01:11:00,180 --> 01:11:01,680
来去做这种匹配度的分析

1534
01:11:01,680 --> 01:11:04,380
我们根据这个分析再去做最终的reading prediction

1535
01:11:04,380 --> 01:11:05,550
任务的预测

1536
01:11:05,550 --> 01:11:08,070
但是与先前的这种SP范式不同的是

1537
01:11:08,070 --> 01:11:10,450
先前的范式可能是每个模型各做一个事

1538
01:11:10,450 --> 01:11:13,290
就是只只面向一个目标来优化

1539
01:11:13,290 --> 01:11:16,190
像线下的这四步可能是分四步走

1540
01:11:16,190 --> 01:11:18,930
需要四个提前model分别去做呃

1541
01:11:18,930 --> 01:11:21,610
用户的偏好分析和item的这种提炼

1542
01:11:21,610 --> 01:11:24,929
这种这些这些四步走的这种模型做这个过程

1543
01:11:24,929 --> 01:11:27,209
但是我们在这里用一个模型来全部解决

1544
01:11:27,209 --> 01:11:32,600
把呃通过设置四种这种弱的format的这种奖励

1545
01:11:32,600 --> 01:11:36,520
我们在一个推理轨迹里面强制模型同时去做呃

1546
01:11:36,520 --> 01:11:38,850
这三前面的三部

1547
01:11:38,850 --> 01:11:41,010
然后通过这种匹配度分析得到完之后

1548
01:11:41,010 --> 01:11:43,670
我们再去在rate的这种结构里面

1549
01:11:43,670 --> 01:11:46,450
去预测最终的这种分数呃

1550
01:11:46,450 --> 01:11:48,030
后面的这个answer

1551
01:11:48,030 --> 01:11:49,410
这个这个这个奖励

1552
01:11:49,410 --> 01:11:52,230
其实是我们面向这个这个推荐场景的数奖励

1553
01:11:52,230 --> 01:11:53,730
就是这是一个呃

1554
01:11:53,730 --> 01:11:56,230
根据把这种面向整个数据集的

1555
01:11:56,230 --> 01:11:57,730
MAMAE的这个指标

1556
01:11:57,730 --> 01:12:01,769
去把它转换成面向单个的这种呃

1557
01:12:01,769 --> 01:12:05,809
单个sample这种reward就是根据target的呃

1558
01:12:05,809 --> 01:12:08,830
这种ground truth的分数

1559
01:12:08,830 --> 01:12:11,410
和我们模型实际预测的这种分数来去

1560
01:12:11,410 --> 01:12:15,640
得到一个面向推荐的呃reward answer对

1561
01:12:15,640 --> 01:12:19,449
然后像先前的呃阿方EC的作者可能谈到说呃

1562
01:12:19,449 --> 01:12:21,689
他们还设置了一个额外的这种呃

1563
01:12:21,689 --> 01:12:23,289
这种这种dance的reward

1564
01:12:23,289 --> 01:12:25,969
来现在模型的早期作为这种更稠密的奖励

1565
01:12:25,969 --> 01:12:28,570
防止呃可能模型早期什么都不会的时候

1566
01:12:28,570 --> 01:12:30,510
他有也有一个比较稀疏的奖励

1567
01:12:30,510 --> 01:12:30,930
呃

1568
01:12:30,930 --> 01:12:33,070
也有一个比较这种稳定的奖励

1569
01:12:33,070 --> 01:12:35,550
然后我们这里没有这种问题呃

1570
01:12:35,550 --> 01:12:37,130
但是同样的这种inside的

1571
01:12:37,130 --> 01:12:39,470
我们是说呃在强化学习里面

1572
01:12:39,470 --> 01:12:42,600
你面向推荐你的这种reorder设置

1573
01:12:42,600 --> 01:12:44,600
你你越接近你的evaluate

1574
01:12:44,600 --> 01:12:46,739
evaluate的时候设置的呃

1575
01:12:46,739 --> 01:12:47,299
reword本身

1576
01:12:47,299 --> 01:12:48,299
比方说我们在这里

1577
01:12:48,299 --> 01:12:52,179
evaluate的指标就是MAE一个群体的指标

1578
01:12:52,179 --> 01:12:54,670
然后我们在这里把它额换算成了

1579
01:12:54,670 --> 01:12:55,470
在单个的sample里面

1580
01:12:55,470 --> 01:12:57,390
我的我们把它变成了一个呃

1581
01:12:57,390 --> 01:12:59,570
面向单个sample的M1

1582
01:12:59,570 --> 01:13:00,470
像这种

1583
01:13:00,470 --> 01:13:04,100
我们我这两个你在训练时候的这种呃奖励

1584
01:13:04,100 --> 01:13:09,080
和你evaluate的时候的这种matrix越接近它就越呃

1585
01:13:09,080 --> 01:13:12,860
越没有这种受到奖励hack的风险对

1586
01:13:12,860 --> 01:13:15,100
可能你如果你换一个其他的

1587
01:13:15,100 --> 01:13:17,400
类似于没有那么直接的中间指标呃

1588
01:13:17,400 --> 01:13:19,840
很可能会出现你的你的训练过程中

1589
01:13:19,840 --> 01:13:20,760
你的reword持续上涨

1590
01:13:20,760 --> 01:13:23,280
但是你evaluate你的指标会额

1591
01:13:23,280 --> 01:13:25,510
可能会出现hit这种过程对

1592
01:13:25,510 --> 01:13:27,930
然后我们在设置完这个前面的这个呃

1593
01:13:27,930 --> 01:13:29,900
format加answer的时候呃

1594
01:13:29,900 --> 01:13:31,260
双层reword叠加之后

1595
01:13:31,260 --> 01:13:33,020
我们在这里做GRPO强化学习

1596
01:13:33,020 --> 01:13:35,800
再去呃让模型采样采样出呃

1597
01:13:35,800 --> 01:13:37,300
WRT出几个结果

1598
01:13:37,300 --> 01:13:40,780
然后根据这种我们先前设置的奖励结果

1599
01:13:40,780 --> 01:13:42,260
我们去优化其中

1600
01:13:42,260 --> 01:13:46,970
对于额推荐的matrix有益的阵容呃

1601
01:13:46,970 --> 01:13:49,760
这种推理轨迹把pass k变成pas索完

1602
01:13:49,760 --> 01:13:51,680
然后去鼓励模型更多的输出

1603
01:13:51,680 --> 01:13:53,600
这种有益于推荐的奖励

1604
01:13:53,600 --> 01:13:54,280
要有意义

1605
01:13:54,280 --> 01:13:55,200
推荐的思考

1606
01:13:55,200 --> 01:13:57,560
然后去再去做预测

1607
01:13:58,060 --> 01:14:00,549
然后我们基于先前的这种RAZO

1608
01:14:00,549 --> 01:14:02,869
这种纯纯pure的强化学习

1609
01:14:02,869 --> 01:14:03,309
这种方式

1610
01:14:03,309 --> 01:14:04,449
我们又做了一个

1611
01:14:04,449 --> 01:14:07,680
相当于做了一个refined的初始化模型呃

1612
01:14:07,680 --> 01:14:10,720
因为像先前我们对于这种教师模型的来说

1613
01:14:10,720 --> 01:14:12,360
我们有一个非常强烈的先验说

1614
01:14:12,360 --> 01:14:15,570
我们认为教师模型它的推理都是好的

1615
01:14:15,850 --> 01:14:17,030
教师模型说什么

1616
01:14:17,030 --> 01:14:20,330
然后我们就让我们的student model学什么呃

1617
01:14:20,330 --> 01:14:22,730
这个先叶显然是有一个非常强烈的假设

1618
01:14:22,730 --> 01:14:25,390
然后呃我们如何判断呃

1619
01:14:25,390 --> 01:14:27,710
教师模型的思考到底是对的还是错的

1620
01:14:27,710 --> 01:14:29,230
它到底对推荐有没有帮助

1621
01:14:29,230 --> 01:14:31,559
我们在这里做了一个相当于refine的过程

1622
01:14:31,559 --> 01:14:33,059
就对于教师模型错误示范

1623
01:14:33,059 --> 01:14:36,380
我们去经过一个首先经过一个reflection呃

1624
01:14:36,380 --> 01:14:39,060
做一个构建一个冷启动的数据集

1625
01:14:39,060 --> 01:14:40,000
如何做的呢

1626
01:14:40,000 --> 01:14:41,220
我们是呃

1627
01:14:41,220 --> 01:14:42,560
首先我们让轿车模型

1628
01:14:42,560 --> 01:14:46,400
还是根据刚才的这种reading reading prediction任务的input

1629
01:14:46,400 --> 01:14:49,189
它包括包含用有user profile啊

1630
01:14:49,189 --> 01:14:51,089
还有这种item的metal metadata

1631
01:14:51,089 --> 01:14:52,709
还有说这个具体的任务

1632
01:14:52,709 --> 01:14:55,009
还有用户的一些历史交互

1633
01:14:55,009 --> 01:14:57,809
我们让这里这里的提权model去产生一个追踪

1634
01:14:57,809 --> 01:14:58,830
零轨迹

1635
01:14:58,830 --> 01:15:03,590
也就是呃这个这个这个这个这个Y

1636
01:15:03,590 --> 01:15:07,620
我们让这个Y呃去去去给模型去做

1637
01:15:07,620 --> 01:15:09,460
根据这个Y去做呃

1638
01:15:09,460 --> 01:15:11,440
reading prediction prediction的这个任务

1639
01:15:11,440 --> 01:15:13,900
如果这个模型能够准确的预测出呃

1640
01:15:13,900 --> 01:15:15,680
符合和ground choice比较匹配的结果

1641
01:15:15,680 --> 01:15:18,770
我们就认为这个推理它对于推荐是有帮助的

1642
01:15:18,770 --> 01:15:20,450
但如果他预测错了

1643
01:15:20,450 --> 01:15:21,910
我们该如何去做

1644
01:15:21,910 --> 01:15:23,870
self refine来去修正这个错误呢

1645
01:15:23,870 --> 01:15:26,430
我们在这里用一个非常简单直接的方法

1646
01:15:26,430 --> 01:15:28,330
就是我们同时把呃

1647
01:15:28,330 --> 01:15:31,810
X就是我们的用户地址输入和呃

1648
01:15:31,810 --> 01:15:33,970
包括他给他给大家怎么信息

1649
01:15:33,970 --> 01:15:36,830
还有外外外外间额外星

1650
01:15:36,830 --> 01:15:39,710
也就是ground truth本身用户实际打多少分

1651
01:15:39,710 --> 01:15:41,980
我们把这个任务重新设计成一个

1652
01:15:41,980 --> 01:15:44,770
相当于是呃反思的任务

1653
01:15:44,770 --> 01:15:46,690
刚才的任务是我们根据用户的输入

1654
01:15:46,690 --> 01:15:49,610
我们去预测它到呃预测他的轨迹和打多少分

1655
01:15:49,610 --> 01:15:51,610
我们现在是给定用户的输入

1656
01:15:51,610 --> 01:15:53,130
和他实际预测的分数

1657
01:15:53,130 --> 01:15:55,070
我们在这里让模型只要做的是

1658
01:15:55,070 --> 01:15:56,030
我们去反思

1659
01:15:56,030 --> 01:15:58,130
他为什么要根据这里的输入

1660
01:15:58,130 --> 01:16:00,060
就可以得到这个Y自制里

1661
01:16:00,060 --> 01:16:02,939
得到了一个这种相当于类似于反思的这种呃

1662
01:16:02,939 --> 01:16:03,439
推理轨迹

1663
01:16:03,439 --> 01:16:06,079
我们把这个轨迹重新加到呃

1664
01:16:06,079 --> 01:16:08,030
这个这个Y垂子里面去

1665
01:16:08,030 --> 01:16:08,710
额

1666
01:16:08,710 --> 01:16:11,230
就通过这种正确的这种推理轨迹

1667
01:16:11,230 --> 01:16:13,130
我们构建了一部分一部分这种轨迹

1668
01:16:13,130 --> 01:16:14,670
还有这种错误的错误的轨迹

1669
01:16:14,670 --> 01:16:18,390
我们通过这种模型的self ref去把他们俩叠起来

1670
01:16:18,390 --> 01:16:20,960
构建一个新的这种呃

1671
01:16:21,400 --> 01:16:24,900
把他俩并点构构建成这种呃trace data

1672
01:16:24,900 --> 01:16:27,440
我们通过这个数据集来去对呃

1673
01:16:27,440 --> 01:16:29,260
大模型做一个初步的冷启动

1674
01:16:29,260 --> 01:16:32,600
让模型在早期有一个比较好的这种呃

1675
01:16:32,600 --> 01:16:35,740
这种这种学这种结构的推理的能力

1676
01:16:35,740 --> 01:16:37,180
还有包括呃

1677
01:16:37,180 --> 01:16:39,900
同时不受这种报这种错误的轨迹

1678
01:16:39,900 --> 01:16:41,840
带来的太多的这种负面影响

1679
01:16:41,840 --> 01:16:43,140
来做一个冷启动

1680
01:16:43,140 --> 01:16:46,000
然后我们再基于这个得到棱形的模型基础上

1681
01:16:46,000 --> 01:16:48,620
再去实行刚才的这种强化学习的框架

1682
01:16:48,620 --> 01:16:51,880
来去对模型的上限进做一个进一步的提升

1683
01:16:53,680 --> 01:16:54,480
唉总结一下

1684
01:16:54,480 --> 01:16:57,440
就是我们的思路就是基基于GPU强化学习来

1685
01:16:57,440 --> 01:17:01,290
我们通过提示词构建去进行这种结构化的推理

1686
01:17:01,290 --> 01:17:04,090
然然后我们设定了一个基于规则的奖励系统

1687
01:17:04,090 --> 01:17:06,210
我们去面向推荐的matrix

1688
01:17:06,210 --> 01:17:09,679
这里是M1和MAE去指去鼓励模型

1689
01:17:09,679 --> 01:17:12,579
同时join的去优化它的推理能力

1690
01:17:12,579 --> 01:17:15,720
和这种rating prediction的准确度呃

1691
01:17:15,720 --> 01:17:19,380
通过后面的指标来去优化前面的模型

1692
01:17:19,380 --> 01:17:21,440
去鼓励他去做呃

1693
01:17:21,440 --> 01:17:23,580
有益于推荐这种思考

1694
01:17:25,960 --> 01:17:28,360
额真正实验结果就在下面

1695
01:17:28,360 --> 01:17:31,600
然后上面的这些是包括传统的呃

1696
01:17:31,600 --> 01:17:35,059
这种就是这种reasoning的这种这种传统范式

1697
01:17:35,059 --> 01:17:37,599
中间的这些是基于大模型base的一些工作

1698
01:17:37,599 --> 01:17:38,739
包括receiver

1699
01:17:38,739 --> 01:17:41,700
还有一些比较新的一些工作

1700
01:17:41,700 --> 01:17:43,200
然后下面就是我们的指标

1701
01:17:43,200 --> 01:17:45,300
可以看出我们的这种整体效果

1702
01:17:45,300 --> 01:17:48,320
比传统模型和包括现有的基于大模型的呃

1703
01:17:48,320 --> 01:17:50,720
returing test方法都有一个比较显著的提升

1704
01:17:50,720 --> 01:17:52,480
因为先前的这种大模型base方法

1705
01:17:52,480 --> 01:17:55,100
确实有一个非常呃值得质疑的点

1706
01:17:55,100 --> 01:17:56,900
就是说他们有强烈的鲜艳

1707
01:17:56,900 --> 01:18:00,060
觉得说teacher model呃做的都是对的

1708
01:18:00,060 --> 01:18:02,550
但实际上呃提前model可能在推荐上

1709
01:18:02,550 --> 01:18:04,310
你并没有做充分的适配

1710
01:18:04,310 --> 01:18:07,090
我们在这里就是我们我们通过把提前model

1711
01:18:07,090 --> 01:18:11,690
根据他的这种呃面向推荐的是标求

1712
01:18:11,690 --> 01:18:14,870
我们在这里是M1去优化它的推理

1713
01:18:14,870 --> 01:18:18,290
让他能去能去做这种真正有益于推进的推理

1714
01:18:18,290 --> 01:18:20,990
我们以此来去获得了一个更好的推理轨迹

1715
01:18:20,990 --> 01:18:23,030
与更好的推进的准确度

1716
01:18:23,310 --> 01:18:25,410
然后另外一个发现就是我们的这个模型

1717
01:18:25,410 --> 01:18:28,630
它的冷启动表现确实比较比较的优越呃

1718
01:18:28,630 --> 01:18:30,110
在cos start和warm场景

1719
01:18:30,110 --> 01:18:33,510
都有一个比较显著的一种性能的提升

1720
01:18:34,500 --> 01:18:37,440
然后这里是我们的一些abolition实验呃

1721
01:18:37,440 --> 01:18:39,500
第一个实验就是像我们刚才说呃

1722
01:18:39,500 --> 01:18:40,240
聊到的说

1723
01:18:40,240 --> 01:18:42,520
我们根据我们在RECOZERO

1724
01:18:42,520 --> 01:18:43,960
这个纯强化学习方案之前

1725
01:18:43,960 --> 01:18:48,079
我们去做一个呃基于self refine的模型的冷启动

1726
01:18:48,079 --> 01:18:51,500
可以看出我们做完self refine之后嗯

1727
01:18:51,740 --> 01:18:53,380
就是个红色的线呃

1728
01:18:53,380 --> 01:18:58,390
他早期就有一个比基于纯纯纯呃强化学习

1729
01:18:58,390 --> 01:19:01,510
有一个早期显著的一种新的机制

1730
01:19:02,940 --> 01:19:03,940
彼此的性能差距

1731
01:19:03,940 --> 01:19:06,500
在后期的强化学里面被逐渐的追求

1732
01:19:06,500 --> 01:19:08,520
但是这个这个优势始终存在

1733
01:19:08,520 --> 01:19:12,400
所以说我们以这种self refine来去作为呃模型

1734
01:19:12,400 --> 01:19:12,800
冷系统

1735
01:19:12,800 --> 01:19:16,280
去提升它的初始的这种结构化思考和呃

1736
01:19:16,280 --> 01:19:17,650
推理的能力

1737
01:19:17,650 --> 01:19:20,430
这对于最终整体的性能是有一个正面提升

1738
01:19:20,430 --> 01:19:22,530
那右边是我们呃

1739
01:19:22,530 --> 01:19:26,170
面向这些不同的组件做的一些雷神实验

1740
01:19:26,660 --> 01:19:29,200
有一个非常呃显著的

1741
01:19:29,200 --> 01:19:32,460
这个就是没有没有这种thinking过程

1742
01:19:32,460 --> 01:19:34,100
和这种没有多不思考的过程

1743
01:19:34,100 --> 01:19:34,750
呃

1744
01:19:34,750 --> 01:19:38,030
模型的这种表现都是不如我们的完整版的这种

1745
01:19:38,030 --> 01:19:40,150
这种real one

1746
01:19:41,320 --> 01:19:41,720
啊

1747
01:19:41,720 --> 01:19:44,120
下面是我们和先前的一些

1748
01:19:44,120 --> 01:19:46,440
基于大模型的范式的呃

1749
01:19:46,440 --> 01:19:49,160
这种这种蒸馏范式的一些呃

1750
01:19:49,160 --> 01:19:51,300
他的推理的开销的比较

1751
01:19:51,300 --> 01:19:55,410
因为先前的范式可能都是我基于一个teacher model

1752
01:19:55,410 --> 01:19:58,590
我最终蒸馏出的可能三四个这种student模型

1753
01:19:58,590 --> 01:20:00,290
他们可能在单步的这种开销

1754
01:20:00,290 --> 01:20:01,530
INFENCE开销上比我们小

1755
01:20:01,530 --> 01:20:04,130
但是他最终需要可能做三次或者四次这种推理

1756
01:20:04,130 --> 01:20:06,490
才能得到最终的这种reading prediction的结果

1757
01:20:06,490 --> 01:20:07,590
而我们把它

1758
01:20:07,590 --> 01:20:10,410
把我们在一个强化学习的过程过程里面呃

1759
01:20:10,410 --> 01:20:12,350
用一个模型来去优化所有的任务

1760
01:20:12,350 --> 01:20:14,260
这可能是四个step

1761
01:20:14,260 --> 01:20:17,060
来去获得了一个整体更小的开销

1762
01:20:17,900 --> 01:20:21,000
然后今天的分享他差不多就这些呃

1763
01:20:21,000 --> 01:20:22,580
谢谢大家聆听

1764
01:20:25,000 --> 01:20:26,820
好的谢谢小野博士

1765
01:20:26,820 --> 01:20:29,240
那下面又即将进入我们的panel

1766
01:20:29,240 --> 01:20:33,160
那有请我们各位嘉宾可以上线

1767
01:20:45,000 --> 01:20:47,680
哎大家可以看一下视频

1768
01:20:54,060 --> 01:20:55,180
好各位伙伴

1769
01:20:55,180 --> 01:20:55,940
如果准备好了

1770
01:20:55,940 --> 01:20:57,560
我们可以随时开始

1771
01:20:57,560 --> 01:21:02,300
好的好的嗯啊hello

1772
01:21:02,300 --> 01:21:02,800
大家好

1773
01:21:02,800 --> 01:21:05,140
可以看到我看到我的屏幕吗

1774
01:21:05,140 --> 01:21:07,270
以及能听到我声音吗

1775
01:21:07,270 --> 01:21:08,910
没问题的

1776
01:21:08,910 --> 01:21:11,770
对对好的好的哦

1777
01:21:12,090 --> 01:21:13,850
呃大家可以开一下麦

1778
01:21:13,850 --> 01:21:15,270
咱们的嘉宾们都可以开一下麦

1779
01:21:15,270 --> 01:21:18,090
因为大家都会都会发言

1780
01:21:18,090 --> 01:21:18,810
然后呢

1781
01:21:18,810 --> 01:21:20,250
咱们今天PO的一个主题的话

1782
01:21:20,250 --> 01:21:22,890
是一个热road to the generative recommendation

1783
01:21:22,890 --> 01:21:27,920
是探讨一下怎么去实现一个线程师推荐的呃

1784
01:21:27,920 --> 01:21:28,860
一个讨论

1785
01:21:28,860 --> 01:21:34,100
那刚刚呢我们四位同学已经进行了每一个工作

1786
01:21:34,100 --> 01:21:36,920
具体的一个点的分析啊介绍

1787
01:21:36,920 --> 01:21:37,780
那我们这里的话

1788
01:21:37,780 --> 01:21:42,740
主要是期望从一个整体的一个角度去提供一个

1789
01:21:42,740 --> 01:21:43,740
全局的概览

1790
01:21:43,740 --> 01:21:45,560
什么是生成式推荐

1791
01:21:45,560 --> 01:21:47,120
为什么要做生成推荐

1792
01:21:47,120 --> 01:21:49,220
那现在做生成推荐有哪些路线

1793
01:21:49,220 --> 01:21:51,150
以及还有哪些挑战

1794
01:21:51,150 --> 01:21:53,190
以及未来可以做哪些点

1795
01:21:53,190 --> 01:21:55,680
从这些角度来展开一个讨论

1796
01:21:56,040 --> 01:21:57,280
在开始之前呢

1797
01:21:57,280 --> 01:22:02,560
我们先啊简单的介绍一下我们的啊嘉宾

1798
01:22:02,560 --> 01:22:07,140
首先啊我自己是呃我呃叫张扬

1799
01:22:07,140 --> 01:22:11,820
然后呢嗯非常高兴可以来主持这个PELO啊

1800
01:22:11,820 --> 01:22:15,380
然后我现在是IOS的一个postdoc

1801
01:22:15,380 --> 01:22:17,940
然后在LEX加加啊lab

1802
01:22:17,940 --> 01:22:21,430
我们lab目前啊正在招收啊

1803
01:22:21,430 --> 01:22:22,810
可以一直招收这种CA

1804
01:22:22,810 --> 01:22:25,209
C以及消费的访问学生啊

1805
01:22:25,209 --> 01:22:28,089
如果大家有兴趣可以啊联系我

1806
01:22:28,089 --> 01:22:29,889
然后我们的方向是

1807
01:22:29,889 --> 01:22:34,040
比如说LM或者agent的个性化推荐以及MTAGENT

1808
01:22:34,400 --> 01:22:37,820
然后呢我们的呃今天主要参与讨论的嘉宾的话

1809
01:22:37,820 --> 01:22:40,660
就是刚刚进行工作介绍的四位同学

1810
01:22:40,660 --> 01:22:42,380
包括由润阳同学

1811
01:22:42,380 --> 01:22:43,140
林子杰同学

1812
01:22:43,140 --> 01:22:48,340
还有就是啊来自USTC的范程潇和啊

1813
01:22:48,340 --> 01:22:51,349
孔孝宇同学好

1814
01:22:51,349 --> 01:22:56,669
那回到呃我们的今天的主题呃

1815
01:22:57,669 --> 01:23:01,040
呃额就是申请式推荐

1816
01:23:01,040 --> 01:23:04,420
那在我们进入额更加深入的讨论之前

1817
01:23:04,420 --> 01:23:05,980
我们首先要进行一个讨论

1818
01:23:05,980 --> 01:23:08,780
是什么是申请式推荐呃

1819
01:23:08,780 --> 01:23:13,300
那它与传统的推荐的一个主要区别是什么啊

1820
01:23:13,300 --> 01:23:16,660
以此来给大家一个初步的印象

1821
01:23:16,660 --> 01:23:18,740
让大家对于新电视推荐这个东西

1822
01:23:18,740 --> 01:23:19,800
有一个初步的了解

1823
01:23:19,800 --> 01:23:22,840
然后我们来进行一个记比较深入的讨论

1824
01:23:22,840 --> 01:23:26,680
那接下来的话我想有请我们的呃

1825
01:23:26,680 --> 01:23:30,090
润阳先来发表一下自己的观点

1826
01:23:30,410 --> 01:23:31,730
这样可以听到吗

1827
01:23:31,730 --> 01:23:33,450
嗯OK可以的

1828
01:23:33,450 --> 01:23:34,910
嗯好嗯

1829
01:23:34,910 --> 01:23:39,020
我觉得就是生成式推荐的一个范例

1830
01:23:39,020 --> 01:23:40,720
一个范本应该就是tiger吧

1831
01:23:40,720 --> 01:23:43,100
那个可能是就是大家提到深圳师推荐的

1832
01:23:43,100 --> 01:23:46,720
一个非常经典的就是理解呃

1833
01:23:46,720 --> 01:23:47,460
具体而言

1834
01:23:47,460 --> 01:23:52,470
其实就是以往的话推荐可能会和检索有些类似

1835
01:23:52,470 --> 01:23:55,230
就是它有像SASRK一样的模型

1836
01:23:55,230 --> 01:23:59,180
然后他负责呃对用户生成一个embedding

1837
01:23:59,180 --> 01:24:01,580
然后这个embedding和哪个ITEEMBEDDING最像呢

1838
01:24:01,580 --> 01:24:02,920
我们就推荐哪个item

1839
01:24:02,920 --> 01:24:05,910
然后生成式改变了这一点

1840
01:24:05,910 --> 01:24:08,370
就是我们现在是希望一个模型

1841
01:24:08,370 --> 01:24:11,170
它接受的输入是用户的序列

1842
01:24:11,170 --> 01:24:13,730
或者或者某种其他的信息吧

1843
01:24:13,730 --> 01:24:16,330
然后它生成的就是比如说什么1234

1844
01:24:16,330 --> 01:24:20,050
然后这1234就代表了一个item的代码呃

1845
01:24:20,050 --> 01:24:24,050
然后这个之所以也也是

1846
01:24:24,050 --> 01:24:25,610
因为这个深圳1234这个事情

1847
01:24:25,610 --> 01:24:27,770
它会被叫做深圳书推荐嘛

1848
01:24:27,770 --> 01:24:30,850
然后在这个基础上呢比较大的一些区别

1849
01:24:30,850 --> 01:24:33,970
我觉得第一当然就是模型是比以往大很多的

1850
01:24:33,970 --> 01:24:36,160
至少至少是P5级别

1851
01:24:36,160 --> 01:24:38,720
然后包括现在咱们用LM这种更大的bug

1852
01:24:38,720 --> 01:24:39,820
我去做这个事情

1853
01:24:39,820 --> 01:24:42,900
然后这也就意味着带来了一些

1854
01:24:45,100 --> 01:24:45,820
除此以外呢

1855
01:24:45,820 --> 01:24:50,370
就是我觉得研究问题额也比较也shifted from

1856
01:24:50,370 --> 01:24:53,290
怎么去更好地生成一个更好的embedding

1857
01:24:53,290 --> 01:24:56,870
变成了怎么能更好的去对这个item

1858
01:24:56,870 --> 01:24:58,630
给他一个更好的id

1859
01:24:59,390 --> 01:25:01,430
对这大概是我的理解

1860
01:25:01,790 --> 01:25:02,290
Ok

1861
01:25:02,290 --> 01:25:05,350
所以润阳的理解就是说我们现在的深层式推荐

1862
01:25:05,350 --> 01:25:07,650
主要是指tiger这一套对吧

1863
01:25:07,650 --> 01:25:11,680
然后呢我们在默认场景下是指这种tiger这一套

1864
01:25:11,680 --> 01:25:15,410
然后可能就是它的特点是生成了一个描述item

1865
01:25:15,410 --> 01:25:17,610
一个序列的一个东西

1866
01:25:17,780 --> 01:25:20,500
好谢谢谢谢润阳

1867
01:25:20,500 --> 01:25:26,530
然后我们接下来请啊来自中科大的孔小宇同学

1868
01:25:26,530 --> 01:25:27,610
孔小小雨

1869
01:25:27,610 --> 01:25:29,730
你的你的观点是什么呢

1870
01:25:29,730 --> 01:25:31,650
呃我个人觉得这个深圳推荐

1871
01:25:31,650 --> 01:25:35,380
目前还是没有一个特别可能是狭义的定义

1872
01:25:35,380 --> 01:25:37,880
是比较像刚才润阳所说的是呃

1873
01:25:37,880 --> 01:25:42,360
就是tier one rick那一套是根据3D的这种呃架构

1874
01:25:42,360 --> 01:25:47,020
仅用用一个深圳式的模型去直接去预测I呃

1875
01:25:47,020 --> 01:25:49,390
用户的下一个感兴趣的list

1876
01:25:49,390 --> 01:25:52,950
呃其实我觉得个人觉得那种比较广义的推荐

1877
01:25:52,950 --> 01:25:55,950
包括我们刚才可能我们我们四位同学聊到的呃

1878
01:25:55,950 --> 01:25:57,890
这些工作都算是比较广义的生日推荐

1879
01:25:57,890 --> 01:25:59,820
然后呃可能业界的话

1880
01:25:59,820 --> 01:26:02,440
现在落地更多的一个比较比较

1881
01:26:02,440 --> 01:26:04,080
这种偏改革派的方式

1882
01:26:04,080 --> 01:26:06,020
是这种声称是召回呃

1883
01:26:06,020 --> 01:26:07,980
因为先我们像我们先聊一下

1884
01:26:07,980 --> 01:26:09,020
先进这种判别式的范式

1885
01:26:09,020 --> 01:26:10,840
我们就是呃拿更多的算力

1886
01:26:10,840 --> 01:26:14,769
可能是一个一个模型的主体去做一种呃做双塔

1887
01:26:14,769 --> 01:26:18,319
去做user的embedding和item embedding的这种学习

1888
01:26:18,319 --> 01:26:20,819
然后对于他们真正的这种可能交叉计算

1889
01:26:20,819 --> 01:26:23,639
可能就只有一个MOP或者是直接直接相乘

1890
01:26:23,639 --> 01:26:24,299
这种结果

1891
01:26:24,299 --> 01:26:26,620
它更多的这种算力是堆在你的

1892
01:26:26,620 --> 01:26:28,240
如何去构造好的embedding上面去

1893
01:26:28,240 --> 01:26:31,630
而不是说你呃他是孤立的计算这种user

1894
01:26:31,630 --> 01:26:34,030
user的embedding和item embedding这种相似度

1895
01:26:34,030 --> 01:26:36,610
而不是说我去根据用户的整体的这种呃

1896
01:26:36,610 --> 01:26:37,530
推荐的上下文

1897
01:26:37,530 --> 01:26:39,130
我通过一个生成式模型

1898
01:26:39,130 --> 01:26:40,910
我在这种呃

1899
01:26:40,910 --> 01:26:43,810
呃这种这种影视的特征特征交叉里面

1900
01:26:43,810 --> 01:26:46,150
我去自己自己去学会呃

1901
01:26:46,150 --> 01:26:48,940
这种这种用户的这种偏好

1902
01:26:48,940 --> 01:26:49,420
对

1903
01:26:49,420 --> 01:26:51,500
这是可能和之前判别式种差别

1904
01:26:51,500 --> 01:26:53,940
然后包括刚才聊到这个生成召回业绩的

1905
01:26:53,940 --> 01:26:56,260
这种比较委婉的这种这种改法

1906
01:26:56,260 --> 01:26:58,899
就是可能还是说利用一个生成模

1907
01:26:58,899 --> 01:26:59,919
生成式模型的架构

1908
01:26:59,919 --> 01:27:03,339
然后去你去你去你去强化模型

1909
01:27:03,339 --> 01:27:05,870
去呃捕获上下文的那种信息

1910
01:27:05,870 --> 01:27:07,390
然后去构建一个更好的影片

1911
01:27:07,390 --> 01:27:08,950
你可能呃包括这个这个工作

1912
01:27:08,950 --> 01:27:11,770
我觉得润阳可能他可能他的工作也是偏这种

1913
01:27:11,770 --> 01:27:12,810
深圳召回这种路子吧

1914
01:27:12,810 --> 01:27:15,170
然后另外的这种可能就是还有一条路子

1915
01:27:15,170 --> 01:27:16,719
可能是这种偏文本形式的

1916
01:27:16,719 --> 01:27:18,879
就是可能是淘宝的

1917
01:27:18,879 --> 01:27:20,819
阿里这边目前在做像rep c b t

1918
01:27:20,819 --> 01:27:23,919
还有一些呃玩think可能也是做了

1919
01:27:23,919 --> 01:27:24,239
一方面

1920
01:27:24,239 --> 01:27:27,770
这些探索就是你呃可能是基于一个文本形式的

1921
01:27:27,770 --> 01:27:29,730
或者说你是既既有文本能力

1922
01:27:29,730 --> 01:27:32,610
也去也具有这种SID的理解能力

1923
01:27:32,610 --> 01:27:36,550
然后结合这两者去能够去探索这个用户的这种

1924
01:27:36,550 --> 01:27:39,910
呃一种新的这种推荐的形态吧

1925
01:27:39,910 --> 01:27:43,750
就是不是去纯粹的你和用户的过去的这种交

1926
01:27:43,750 --> 01:27:45,270
互的这种历史的日志

1927
01:27:45,270 --> 01:27:47,220
而是去可能去主动的交互

1928
01:27:47,220 --> 01:27:49,100
或者说去帮用户完成一个工作流啊

1929
01:27:49,100 --> 01:27:51,360
像美团的小美这种结合agent来做

1930
01:27:51,360 --> 01:27:53,740
我觉得这些都算是一种广义的商城推荐对

1931
01:27:53,740 --> 01:27:55,620
大概是这些哦

1932
01:27:55,620 --> 01:27:56,620
OK好

1933
01:27:56,620 --> 01:27:58,400
所以小雨的观点是说

1934
01:27:58,400 --> 01:28:01,889
我们现在现在是推荐想上一个非常广义的定义

1935
01:28:01,889 --> 01:28:06,049
然后这个可能比如说用了生成式模型的都不算

1936
01:28:06,049 --> 01:28:11,040
或者说呃呃就是不是基于传统这种

1937
01:28:11,040 --> 01:28:12,540
进行一个匹配的

1938
01:28:12,540 --> 01:28:14,900
这种可能都算是一个生成式的定义

1939
01:28:14,900 --> 01:28:16,760
那这我有个小小的问题

1940
01:28:16,760 --> 01:28:21,350
就是呃刚刚提到就是说这个什么生成

1941
01:28:21,350 --> 01:28:23,750
当然可能也是那个问题

1942
01:28:23,750 --> 01:28:25,830
也是同时给各位同学一起的

1943
01:28:25,830 --> 01:28:29,210
就是说这种啊基于tiger base

1944
01:28:29,210 --> 01:28:31,010
比如semantic base的方法来讲

1945
01:28:31,010 --> 01:28:32,050
它和这种传统的

1946
01:28:32,050 --> 01:28:33,190
比如SAS瑞克来讲

1947
01:28:33,190 --> 01:28:34,550
那从你们的角度来讲

1948
01:28:34,550 --> 01:28:38,540
你觉得他们主要的区别在哪呢

1949
01:28:39,820 --> 01:28:41,140
呃我先打吗

1950
01:28:41,140 --> 01:28:43,180
可以可以OK

1951
01:28:43,180 --> 01:28:46,150
就是呃个人觉得首先是模型

1952
01:28:46,150 --> 01:28:49,200
模型这种大小的参数量的一种巨大的区别吧

1953
01:28:49,200 --> 01:28:51,720
然后其次是可能是萨特这种呃

1954
01:28:51,720 --> 01:28:53,000
其现在这种升值方式

1955
01:28:53,000 --> 01:28:55,970
你去把呃这种数亿级别的或者是数额

1956
01:28:55,970 --> 01:28:57,690
比方one rock就是一个数亿级别的

1957
01:28:57,690 --> 01:28:59,950
压缩到呃3×8192的这种码

1958
01:28:59,950 --> 01:29:01,070
这种码本里面去

1959
01:29:01,070 --> 01:29:03,470
把这种信息压缩成这种小的细粒度

1960
01:29:03,470 --> 01:29:04,990
这种token把它拆分开

1961
01:29:04,990 --> 01:29:08,390
我去先去发明一种生一种推荐场景的语言

1962
01:29:08,390 --> 01:29:09,920
我再去基于这个语言

1963
01:29:09,920 --> 01:29:11,840
这个发明的这种推荐场景的语言

1964
01:29:11,840 --> 01:29:16,040
我再去做后续这种这种呃生成式的推荐

1965
01:29:16,040 --> 01:29:20,230
然后他发现把信息把这种i id item的

1966
01:29:20,230 --> 01:29:22,150
这种单个的表示去简化之后

1967
01:29:22,150 --> 01:29:24,790
反而随着你的模型的skating上去之后

1968
01:29:24,790 --> 01:29:25,870
它的模型有一个更显著

1969
01:29:25,870 --> 01:29:27,190
这种skinning la的呈现

1970
01:29:27,190 --> 01:29:29,789
我觉得这可能是呃现在的这种生成式

1971
01:29:29,789 --> 01:29:32,589
生成式的推荐和先前的可能是序列推荐呃

1972
01:29:32,589 --> 01:29:34,330
有一个比较大的区别的地方

1973
01:29:34,330 --> 01:29:37,770
OK所以啊从我捕捉到关键词

1974
01:29:37,770 --> 01:29:39,360
你刚刚提的词是语言

1975
01:29:39,360 --> 01:29:42,280
所以从我的角度来讲啊

1976
01:29:42,280 --> 01:29:43,560
我get到的角度来讲

1977
01:29:43,560 --> 01:29:46,110
就是说我们这个语言可能是

1978
01:29:46,110 --> 01:29:47,790
就是比如说我们人类的语言

1979
01:29:47,790 --> 01:29:49,630
它是不同人之间的可以通用的

1980
01:29:49,630 --> 01:29:50,870
那我想问问你的意思

1981
01:29:50,870 --> 01:29:55,570
是不是说你这个语言是在比如说我们这些item呢

1982
01:29:55,570 --> 01:29:57,690
这种跨item之间

1983
01:29:57,690 --> 01:29:58,570
可能都是一种

1984
01:29:58,570 --> 01:30:00,770
可以进行一个信息共享的一个形式

1985
01:30:00,770 --> 01:30:02,030
你是想表达这个意思吗

1986
01:30:02,030 --> 01:30:03,590
对对是的是的嗯

1987
01:30:03,590 --> 01:30:07,670
OK好我我自己也很赞同这个这个这个说法

1988
01:30:07,670 --> 01:30:09,830
其他同学有什么补充吗

1989
01:30:14,540 --> 01:30:16,700
OK好额

1990
01:30:16,700 --> 01:30:18,780
那我们继续刚刚的问题

1991
01:30:18,780 --> 01:30:22,600
就是啊对于我们这个slides上的一个问题

1992
01:30:22,600 --> 01:30:25,760
我们请子杰同学来进行一个回答

1993
01:30:27,060 --> 01:30:28,960
OKOK呃

1994
01:30:28,960 --> 01:30:32,220
关于圣诞士推荐和传统推荐的话

1995
01:30:32,220 --> 01:30:34,150
我理解圣诞士推荐呃

1996
01:30:34,150 --> 01:30:38,470
实际上更多就是使用the semantic id sequence

1997
01:30:38,470 --> 01:30:41,830
去做一个呃一个item的一个表征吗

1998
01:30:41,830 --> 01:30:44,590
这带给我们的是一个很重要的一点是

1999
01:30:44,590 --> 01:30:47,010
我们可以用NTP的一个训练方式

2000
01:30:47,010 --> 01:30:50,670
去做一个模型的一个train呃

2001
01:30:50,670 --> 01:30:52,070
为什么这是一个很重要的点呢

2002
01:30:52,070 --> 01:30:55,320
是因为首先NLP跟推荐任务

2003
01:30:55,320 --> 01:30:57,360
本身还是有一定相似性的

2004
01:30:57,360 --> 01:31:00,180
呃就是都是一些用的history

2005
01:31:00,180 --> 01:31:02,540
然后我们去预测某些token sequence

2006
01:31:02,540 --> 01:31:06,519
那NLP上我们发现说就是额NTP这个任务

2007
01:31:06,519 --> 01:31:08,679
他会有一个很强的空间表示能力

2008
01:31:08,679 --> 01:31:10,439
而它的泛化性也很好

2009
01:31:10,439 --> 01:31:12,939
然后那我们为了在呃

2010
01:31:12,939 --> 01:31:15,610
就是你在推荐任务上去构造它

2011
01:31:15,610 --> 01:31:18,450
我们必须要让我们的那个item

2012
01:31:18,450 --> 01:31:22,250
是用某些有语义信息的一些token去表示

2013
01:31:22,250 --> 01:31:25,840
然后我们就可以用呃这个NTP的一个方法

2014
01:31:25,840 --> 01:31:28,160
去做每一个模型的一个训练

2015
01:31:28,160 --> 01:31:30,690
我觉得这是一个主要区别嗯

2016
01:31:30,690 --> 01:31:32,770
OK所以子杰

2017
01:31:32,770 --> 01:31:33,990
你前到点是说

2018
01:31:33,990 --> 01:31:37,919
你需你在进行这个NTP loss训练的时候

2019
01:31:37,919 --> 01:31:41,539
你需要基于一些用语义的token来去构建的东西

2020
01:31:41,539 --> 01:31:42,339
这个意思吗

2021
01:31:42,339 --> 01:31:43,400
嗯对的

2022
01:31:43,400 --> 01:31:46,320
比如如果我只用一个token去训练的话

2023
01:31:46,320 --> 01:31:48,560
就表示整个item池里面的话

2024
01:31:48,560 --> 01:31:51,730
我根本没有办法去做一个有效的NTP训练

2025
01:31:51,730 --> 01:31:54,860
嗯我觉得主要还是对齐这个NBB训练

2026
01:31:54,860 --> 01:31:58,740
OK所以你这个你我在我的理解就是

2027
01:31:58,740 --> 01:32:01,580
可能你这个观点跟小雨刚刚提到的

2028
01:32:01,580 --> 01:32:06,730
就是说当你去建建模一种语言的时候

2029
01:32:06,730 --> 01:32:08,650
如果说你这个token是有语义的话

2030
01:32:08,650 --> 01:32:09,890
那可能就是建模一种语言

2031
01:32:09,890 --> 01:32:13,370
这个语言是把不同的item给连接在一起的

2032
01:32:13,370 --> 01:32:15,290
耶啊是这意思

2033
01:32:15,290 --> 01:32:17,120
嗯好的嗯

2034
01:32:17,120 --> 01:32:19,360
然后成交有什么观点吗

2035
01:32:19,360 --> 01:32:20,360
陈潇有什么观点吗

2036
01:32:20,360 --> 01:32:22,080
嗯那在我看来

2037
01:32:22,080 --> 01:32:25,380
如果要把收藏师推荐和传统推荐

2038
01:32:25,380 --> 01:32:27,560
划分清楚一个界限的话

2039
01:32:27,560 --> 01:32:33,750
我认为是他不再是依赖于检索和排序

2040
01:32:33,750 --> 01:32:35,990
也就是它不是完全依赖于贡献

2041
01:32:35,990 --> 01:32:38,290
统计的一个训练过程

2042
01:32:38,290 --> 01:32:41,620
而是更像是建模在一个条件序列生成上

2043
01:32:41,620 --> 01:32:47,130
像是一种一种从匹配到推理的一个转变

2044
01:32:47,130 --> 01:32:48,170
这是我的一些认知

2045
01:32:49,850 --> 01:32:51,400
OK好的

2046
01:32:51,400 --> 01:32:54,630
可能那总结下来大家可能有不同的观点

2047
01:32:54,630 --> 01:32:57,190
有的可能就是说从狭义的角度来讲

2048
01:32:57,190 --> 01:33:01,170
就是说是认为semantic id这种是一个呃

2049
01:33:01,170 --> 01:33:03,150
现在的一个默认的一个形式推荐

2050
01:33:03,150 --> 01:33:04,570
但是大家都有一些共同点

2051
01:33:04,570 --> 01:33:08,049
比如说这种semantic d它本质上建模了

2052
01:33:08,049 --> 01:33:09,769
或者说他陷入一种信息

2053
01:33:09,769 --> 01:33:11,990
那他这种机理是什么

2054
01:33:11,990 --> 01:33:14,270
他可能就是说类似于刚刚小雨提到的

2055
01:33:14,270 --> 01:33:15,270
还有子杰提到的

2056
01:33:15,270 --> 01:33:19,090
是搞一个类似语言或者一个semantic的东西

2057
01:33:19,090 --> 01:33:26,310
实现这些跨item跨跨这些不同的呃类

2058
01:33:26,310 --> 01:33:29,559
或者说不同的物品之间的一种信息的一种通信

2059
01:33:29,559 --> 01:33:32,119
这可能是我们大家相对比较统一的观点

2060
01:33:32,119 --> 01:33:35,079
然后呢陈晓陈晓也提到

2061
01:33:35,079 --> 01:33:38,139
就是可能就是说从建模的一个

2062
01:33:38,139 --> 01:33:40,579
比如说数学的一个对象来讲

2063
01:33:40,579 --> 01:33:41,739
可能是有个区别的

2064
01:33:41,739 --> 01:33:43,240
嗯好

2065
01:33:43,240 --> 01:33:45,940
那我们呃对于这个话题

2066
01:33:45,940 --> 01:33:48,460
就是基本能有一定的共识

2067
01:33:48,460 --> 01:33:51,880
那我们接下来就是进入下一个话题呃

2068
01:33:53,300 --> 01:33:55,660
啊这是刚刚就是我们也提到了吗

2069
01:33:55,660 --> 01:34:01,480
就比如说陈那个润阳觉得可能就是呃这个呃

2070
01:34:01,880 --> 01:34:03,040
Semantic tiger

2071
01:34:03,040 --> 01:34:06,620
这种semantic based是一种默认的一种GENERETIC

2072
01:34:06,620 --> 01:34:07,740
就是一个生成式推荐

2073
01:34:07,740 --> 01:34:09,600
然后小雨刚刚提到

2074
01:34:09,600 --> 01:34:11,370
就是说可能是啊

2075
01:34:11,370 --> 01:34:13,070
我们这个森林的推荐相对是比较广泛的

2076
01:34:13,070 --> 01:34:16,630
你可能用很多种呃形式来实现这个推荐

2077
01:34:16,630 --> 01:34:18,750
你都可以是一种新产式的东西

2078
01:34:18,750 --> 01:34:21,270
那比如说我们今天的这个专题的话

2079
01:34:21,270 --> 01:34:22,490
还是叫什么深产式推荐

2080
01:34:22,490 --> 01:34:25,010
但是我们啊几乎所有同学的paper

2081
01:34:25,010 --> 01:34:26,370
都是基于大模型的

2082
01:34:26,370 --> 01:34:27,810
所以呢这个时候有两个问

2083
01:34:27,810 --> 01:34:29,010
有一个问题就出现了

2084
01:34:29,010 --> 01:34:30,350
那这有个路线之争

2085
01:34:30,350 --> 01:34:33,470
对比lm base的生成式推荐和semantic base

2086
01:34:33,470 --> 01:34:35,560
就是tiger ral based的啊

2087
01:34:35,560 --> 01:34:36,560
现在推荐来讲

2088
01:34:36,560 --> 01:34:40,520
你们啊大家觉得这两种类型来讲

2089
01:34:40,520 --> 01:34:42,540
他们有各自什么的优点和缺点

2090
01:34:42,540 --> 01:34:45,900
然后你们各自的倾向是性选择哪

2091
01:34:45,900 --> 01:34:47,540
比如说走哪一条路线

2092
01:34:47,540 --> 01:34:50,020
是你们自己认为更OK的

2093
01:34:50,020 --> 01:34:50,900
或者更喜欢的

2094
01:34:50,900 --> 01:34:53,230
或者说更具有前景的

2095
01:34:53,230 --> 01:34:59,260
那我们呃有有同学想先来讲讲吗

2096
01:34:59,540 --> 01:35:01,940
嗯那那我先来回答一下吧

2097
01:35:01,940 --> 01:35:05,790
好的OK嗯就是就像我刚刚也说

2098
01:35:05,790 --> 01:35:08,710
就是我觉得就是tiger base的那种semantic id

2099
01:35:08,710 --> 01:35:11,030
它应该是一种狭义上的定义

2100
01:35:11,030 --> 01:35:13,280
或者说就是我们认为最经典的一个理解

2101
01:35:13,280 --> 01:35:18,230
但是我觉得呃其实semantic d base加l m base

2102
01:35:18,230 --> 01:35:20,130
它本身不是一个矛盾的选项

2103
01:35:20,130 --> 01:35:23,639
有人在就是lm base上面去做semantic d的生成

2104
01:35:23,639 --> 01:35:25,299
也是比较著名的works

2105
01:35:25,299 --> 01:35:30,589
然后呢呃但是就一定要说就是你说往哪条路走

2106
01:35:30,589 --> 01:35:31,469
是个比较好的方向

2107
01:35:31,469 --> 01:35:32,669
我觉得还是on base的

2108
01:35:32,669 --> 01:35:34,840
因为就是我比较同意小雨的观点

2109
01:35:34,840 --> 01:35:37,200
你在l base上上做generative

2110
01:35:37,200 --> 01:35:39,840
它本身就是能够有非常多的形式

2111
01:35:39,840 --> 01:35:41,080
非常多的可能性

2112
01:35:41,080 --> 01:35:43,160
就包括你可以用agent也好

2113
01:35:43,160 --> 01:35:44,840
就像我这种做reasoning的也好

2114
01:35:44,840 --> 01:35:49,230
或者说是呃让LM生成更好的embedding

2115
01:35:49,230 --> 01:35:51,510
或让LM生成更准确的identifier

2116
01:35:51,510 --> 01:35:53,870
它就是有非常多路径可以去探索

2117
01:35:53,870 --> 01:35:55,910
它本身带来更多的可能性

2118
01:35:55,910 --> 01:35:58,810
然后我觉得这可能也是呃

2119
01:35:58,810 --> 01:36:00,430
为什么我们现在能看到这么多

2120
01:36:00,430 --> 01:36:03,059
elon base的composition的work

2121
01:36:03,219 --> 01:36:06,119
所以说对这是我的偏好

2122
01:36:06,119 --> 01:36:09,700
OK刚刚刚刚润阳提到一个点是说

2123
01:36:09,700 --> 01:36:12,340
比如说我们这种lm base的方法

2124
01:36:12,340 --> 01:36:14,380
相对于传统来讲可以做一些额外事情

2125
01:36:14,380 --> 01:36:16,299
比如说刚填了个reasoning这个事情对吧

2126
01:36:16,299 --> 01:36:17,219
然后呢

2127
01:36:17,219 --> 01:36:19,979
现在我们可以看到就是说基于一些传统的方法

2128
01:36:19,979 --> 01:36:25,960
比如说哦那个哦哦比如说对于tiger

2129
01:36:25,960 --> 01:36:27,000
或者说对于传动的

2130
01:36:27,000 --> 01:36:27,840
甚至SARS瑞克

2131
01:36:27,840 --> 01:36:30,720
他们也会去研究一些所谓的一个reasoning

2132
01:36:30,720 --> 01:36:32,170
那种方法

2133
01:36:32,170 --> 01:36:33,830
比如说进行一些nand reasoning

2134
01:36:33,830 --> 01:36:35,950
那他们也是一种reading的方法

2135
01:36:35,950 --> 01:36:37,690
那你觉得在这种情况下

2136
01:36:37,690 --> 01:36:40,499
如果这些semantic base的方法

2137
01:36:40,499 --> 01:36:42,259
它也可semantic id base的方法

2138
01:36:42,259 --> 01:36:43,819
它也可以进行一个推理了之后

2139
01:36:43,819 --> 01:36:46,459
那我们的lm base的方法相当于semantic base方法

2140
01:36:46,459 --> 01:36:48,059
还有什么优点吗

2141
01:36:49,600 --> 01:36:52,360
我觉得大家比较经常讲的一个点

2142
01:36:52,360 --> 01:36:54,260
还是LMPRETRANOWLEDGE

2143
01:36:54,260 --> 01:36:56,310
就是说额是的

2144
01:36:56,310 --> 01:36:58,650
我们可以为推荐专门构造门语言

2145
01:36:58,650 --> 01:37:01,750
但是如果我们在这门语言上嵌套另外一门语言

2146
01:37:01,750 --> 01:37:03,940
就是我们平常人类使用的这个语言

2147
01:37:03,940 --> 01:37:07,600
那么大模型应该能更好地利用它的ping node做

2148
01:37:07,600 --> 01:37:09,490
去做更好的呃

2149
01:37:09,490 --> 01:37:15,240
就是推荐就特别是在比如cool star这种场景上

2150
01:37:15,840 --> 01:37:18,360
OK好的嗯行

2151
01:37:18,360 --> 01:37:19,260
谢谢瑞阳

2152
01:37:19,260 --> 01:37:23,800
然后呃其他同学有什么要补充的吗

2153
01:37:25,690 --> 01:37:28,370
那我稍微说说我的观点好呃

2154
01:37:28,370 --> 01:37:30,250
我个人倒是不觉得说

2155
01:37:30,250 --> 01:37:32,610
随便提个id贝斯和大模型贝斯

2156
01:37:32,610 --> 01:37:34,620
一定是两个冲突的点

2157
01:37:34,620 --> 01:37:35,020
呃

2158
01:37:35,020 --> 01:37:36,140
如果说哎大模型base

2159
01:37:36,140 --> 01:37:38,080
可能是基于文本形式的推荐的话

2160
01:37:38,080 --> 01:37:40,020
那可能倒是两个比较对立

2161
01:37:40,020 --> 01:37:41,929
但我个人理解就是说呃

2162
01:37:41,929 --> 01:37:44,309
首先我们为什么需要大模型base

2163
01:37:44,309 --> 01:37:46,379
因为像刚才润阳说的说呃

2164
01:37:46,379 --> 01:37:47,819
大模型贝斯这种世界知识

2165
01:37:47,819 --> 01:37:50,619
它一定是有很多用户在推荐场景里面

2166
01:37:50,619 --> 01:37:51,939
没有真实的表露出来的

2167
01:37:51,939 --> 01:37:53,330
这种额外的信息量

2168
01:37:53,330 --> 01:37:56,450
它可以通过大模型贝斯的这种这种这种架构

2169
01:37:56,450 --> 01:37:56,650
来

2170
01:37:56,650 --> 01:37:56,890
去

2171
01:37:56,890 --> 01:37:59,680
把这个东西引入到推进真实的训练系统里面去

2172
01:37:59,680 --> 01:38:02,040
然后为什么我们需要随便提个id base

2173
01:38:02,040 --> 01:38:03,640
首先我们通过这个呃

2174
01:38:03,640 --> 01:38:05,560
目前的可能这种开销的角度考虑

2175
01:38:05,560 --> 01:38:07,230
像现现在的这种呃

2176
01:38:07,230 --> 01:38:09,830
可能落已经业界已经落地的生成确定的工作

2177
01:38:09,830 --> 01:38:10,990
包括one rick

2178
01:38:10,990 --> 01:38:12,839
包括呃可能是GPR

2179
01:38:12,839 --> 01:38:13,599
包括one piece

2180
01:38:13,599 --> 01:38:14,519
他们都是这种

2181
01:38:14,519 --> 01:38:17,610
要不就是基于这种仍然是生成召回这种方式

2182
01:38:17,610 --> 01:38:19,550
要不就是可能是SID这种形式

2183
01:38:19,550 --> 01:38:22,370
它是出于呃一方面是出于这种开销的考虑

2184
01:38:22,370 --> 01:38:26,140
另外可能呃是就是我基于

2185
01:38:26,140 --> 01:38:27,680
像如果把这两点结合起来的话

2186
01:38:27,680 --> 01:38:29,590
我基于大模型的世界知识

2187
01:38:29,590 --> 01:38:31,470
和随便TIKID去做对齐之后

2188
01:38:31,470 --> 01:38:34,559
semetic id也能拥有呃大模型呃

2189
01:38:34,559 --> 01:38:37,679
它本身带来的这种世界知识的这种额外的信息

2190
01:38:37,679 --> 01:38:41,179
同时它有更加的优越的这种呃

2191
01:38:41,179 --> 01:38:44,500
开销上的这种优势对大概是这样啊

2192
01:38:44,500 --> 01:38:48,180
OK所以瑞阳和小雨可能比较统一的观点

2193
01:38:48,180 --> 01:38:53,270
就是说相对于这个呃semantic baid base的方法来讲

2194
01:38:53,270 --> 01:38:55,030
我们pre training好的LM

2195
01:38:55,030 --> 01:38:59,280
它是包含很多推荐世界之外的一些word

2196
01:38:59,280 --> 01:39:01,960
所谓的FACSIC就是这种word一个low ledge

2197
01:39:01,960 --> 01:39:06,120
所以这些word knowledge可以帮助我们做一个更好

2198
01:39:06,650 --> 01:39:08,170
OK好

2199
01:39:08,170 --> 01:39:11,890
然后子杰和陈晓有什么补充的吗

2200
01:39:12,310 --> 01:39:15,450
OK那我这边补充个我的观点吧

2201
01:39:15,450 --> 01:39:18,430
就是ALPH贝斯的这个东西

2202
01:39:18,430 --> 01:39:20,530
它毕竟是基于呃

2203
01:39:20,530 --> 01:39:21,350
要么是文本

2204
01:39:21,350 --> 01:39:24,110
要么是一些多模态的这个token那个表征

2205
01:39:24,110 --> 01:39:27,670
但很多时候它实际上呃他的那个trajectory

2206
01:39:27,670 --> 01:39:30,130
以他的一个推理还是比较可视的

2207
01:39:30,130 --> 01:39:32,120
比起semantic d来说

2208
01:39:32,120 --> 01:39:34,400
那我自己的一个经验是

2209
01:39:34,400 --> 01:39:35,600
在很多场景下

2210
01:39:35,600 --> 01:39:40,409
我们呃实际上就是它应用lm base和semantic id base

2211
01:39:40,409 --> 01:39:42,270
可能是完全不一样的场景

2212
01:39:42,270 --> 01:39:46,510
比如说一些淘宝猜搜或者是小红书渡劫的AI搜

2213
01:39:46,510 --> 01:39:48,930
但实际上他都会用到一些呃

2214
01:39:48,930 --> 01:39:51,670
就是只能继续重文本的一个呃

2215
01:39:51,670 --> 01:39:53,290
类似推荐的一个任务

2216
01:39:53,290 --> 01:39:56,420
那这个实际上是只能使用lm base的

2217
01:39:56,420 --> 01:39:57,820
然后semantic id base

2218
01:39:57,820 --> 01:39:59,860
如果我们不考虑他的这个呃

2219
01:39:59,860 --> 01:40:01,670
trajectory的一个推理的话

2220
01:40:01,670 --> 01:40:03,430
呃当然在大厂的一个

2221
01:40:03,430 --> 01:40:04,990
比如字节的这个推荐系统中

2222
01:40:04,990 --> 01:40:06,070
肯定是semi id

2223
01:40:06,070 --> 01:40:10,419
完全优于m base的一个一个一个那个范式的

2224
01:40:10,419 --> 01:40:11,659
但是假如我们想

2225
01:40:11,659 --> 01:40:14,960
就是如果没有这个性能开销的一个限制的话

2226
01:40:14,960 --> 01:40:18,720
我们完全完全可以LM先先先reason1段出来

2227
01:40:18,720 --> 01:40:21,250
然后再推理出最终那个SYMANTICD

2228
01:40:21,250 --> 01:40:23,890
所以额换换过来而言

2229
01:40:23,890 --> 01:40:27,140
就是当我们如果想要决定最后那个item库

2230
01:40:27,140 --> 01:40:28,420
它是一个很大很复杂

2231
01:40:28,420 --> 01:40:31,400
而且他就是这个场景是非常推荐的

2232
01:40:31,400 --> 01:40:33,880
像这结论很广泛的一个推荐场景的话

2233
01:40:33,880 --> 01:40:36,840
呃我们是肯定是要使用siantic id的

2234
01:40:36,840 --> 01:40:39,080
然后如果我们想要推关键的呃

2235
01:40:39,080 --> 01:40:41,200
想要关注的是这些文本的一个

2236
01:40:41,200 --> 01:40:42,800
推理的一个能力呃

2237
01:40:42,800 --> 01:40:45,560
或者说在一些只能使用文本的一个场景下

2238
01:40:45,560 --> 01:40:48,240
那l o n base是我们需要研究的一个重点

2239
01:40:48,860 --> 01:40:49,770
是这样

2240
01:40:49,770 --> 01:40:53,090
OK这个角度确实也是一个比较好的角度

2241
01:40:53,090 --> 01:40:56,380
如果说对于我们l m base的方法来讲

2242
01:40:56,380 --> 01:40:59,540
可以提供一个更加清晰的一个推理路径

2243
01:40:59,540 --> 01:41:02,540
有助于用户或者去帮助用户理解

2244
01:41:02,540 --> 01:41:04,590
或者说去说服用户

2245
01:41:04,590 --> 01:41:06,330
我们这个推荐的合理性对吧

2246
01:41:06,330 --> 01:41:09,309
然后可能就是说对于这semantic d base来讲

2247
01:41:09,309 --> 01:41:11,809
就是在adam poor size非常大的时候

2248
01:41:11,809 --> 01:41:13,429
这种考虑效率的情况下

2249
01:41:13,429 --> 01:41:15,469
我们可能就是更倾向于做SM

2250
01:41:15,469 --> 01:41:16,269
对对对

2251
01:41:16,269 --> 01:41:17,860
嗯好的

2252
01:41:17,860 --> 01:41:19,240
陈晓有什么补充吗

2253
01:41:19,240 --> 01:41:24,200
嗯我现在我可能个人更认为sii d base的方法

2254
01:41:24,200 --> 01:41:27,619
它更像是在l o n base方法

2255
01:41:27,619 --> 01:41:30,559
它在语义对齐上面不足的一个无奈之举

2256
01:41:30,559 --> 01:41:32,679
但是我认为基于SID的方法

2257
01:41:32,679 --> 01:41:36,410
它好像更侧重于结构化和腰压缩

2258
01:41:36,410 --> 01:41:40,010
本质上可能丧失了一定大模型固有的通用能力

2259
01:41:40,010 --> 01:41:41,490
所以我从直觉上来觉得

2260
01:41:41,490 --> 01:41:44,520
我觉得如果能通过在ln base的方法上

2261
01:41:44,520 --> 01:41:46,320
实现更好的PRETRAINING之类的

2262
01:41:46,320 --> 01:41:46,800
Token net

2263
01:41:46,800 --> 01:41:50,760
可能会能更好的去保证这个通用能力

2264
01:41:51,360 --> 01:41:54,120
OK你刚刚实啊提到

2265
01:41:54,120 --> 01:41:56,960
就是说在LM的基础上来实现一个pre training

2266
01:41:56,960 --> 01:42:00,530
你是指比如说是类似于一种continue pretraining的形式

2267
01:42:00,530 --> 01:42:02,170
去融入可推荐的知识吗

2268
01:42:02,170 --> 01:42:03,210
这个意思哦

2269
01:42:03,210 --> 01:42:04,250
对的对的对的

2270
01:42:04,250 --> 01:42:06,620
而不要像SID的方法

2271
01:42:06,620 --> 01:42:11,130
我认为它为了结构化而压缩的这个范式

2272
01:42:11,130 --> 01:42:14,250
感觉对通用能力是有所丧失的啊

2273
01:42:14,250 --> 01:42:17,719
OK好了解行啊

2274
01:42:17,719 --> 01:42:18,919
对于这个问题

2275
01:42:18,919 --> 01:42:20,499
大家都发表了自己的观点

2276
01:42:20,499 --> 01:42:22,359
然后呢可能就是润尧

2277
01:42:22,359 --> 01:42:23,839
我们开始就提到一个很重要的一点

2278
01:42:23,839 --> 01:42:27,280
我觉得是呃l on based和semantic id based的方法

2279
01:42:27,280 --> 01:42:28,160
它不是冲突的

2280
01:42:28,160 --> 01:42:29,900
两个是可以融合的一个东西

2281
01:42:29,900 --> 01:42:31,120
然后大家也提到了

2282
01:42:31,120 --> 01:42:35,350
就是说呃l m base的方法

2283
01:42:35,350 --> 01:42:38,070
相对于set transmc id base方法来讲

2284
01:42:38,070 --> 01:42:42,060
它会有一个就是这种所谓的

2285
01:42:42,060 --> 01:42:43,540
word knowledge的一个优势

2286
01:42:43,540 --> 01:42:45,680
可能还有一些就是大模型本身

2287
01:42:45,680 --> 01:42:49,400
其他通用方面的能力带来了一个优势

2288
01:42:49,400 --> 01:42:50,440
然后呢

2289
01:42:50,440 --> 01:42:54,450
这个优势可能就包括刚刚子杰提到的那种啊

2290
01:42:54,450 --> 01:42:56,970
透透明那个可解释的

2291
01:42:56,970 --> 01:43:00,880
可理解的一种路径的一些东西对

2292
01:43:00,880 --> 01:43:04,320
然后SEMCD的话可能就是说现在base的方法的话

2293
01:43:04,320 --> 01:43:06,880
可能现在就是在这种不需要解释

2294
01:43:06,880 --> 01:43:08,640
需要考虑效率

2295
01:43:08,640 --> 01:43:10,360
item poor非常大的情况下

2296
01:43:10,360 --> 01:43:12,200
可能是一个比较好的选择

2297
01:43:12,200 --> 01:43:15,880
嗯好的哦

2298
01:43:16,650 --> 01:43:19,010
那我们接下来进入下一个话题

2299
01:43:19,010 --> 01:43:24,669
然后我相信大家可能在自己的paper投稿中

2300
01:43:24,669 --> 01:43:26,149
会遇到一个这样一个问题

2301
01:43:26,149 --> 01:43:29,540
就是说他们会问呃

2302
01:43:29,540 --> 01:43:32,580
我们为什么要用大模型来做推荐呢

2303
01:43:32,580 --> 01:43:35,350
比如说大模型的呃

2304
01:43:35,350 --> 01:43:38,090
这种部署的成本很高

2305
01:43:38,090 --> 01:43:39,630
然后呢你有些很多事情

2306
01:43:39,630 --> 01:43:41,310
比如说你大模型做推理的事情

2307
01:43:41,310 --> 01:43:43,870
你现在可以把这一套推理的一些机理

2308
01:43:43,870 --> 01:43:47,520
给移到我们这个深那个生成式推荐

2309
01:43:47,520 --> 01:43:49,780
就是这种semantic d base一个框架下

2310
01:43:49,780 --> 01:43:56,330
那这个时候为什么这个LM找lm base的方法

2311
01:43:56,330 --> 01:43:58,230
它是一个必须的东西

2312
01:43:58,230 --> 01:44:01,370
就包括你们刚刚提到的就是搞这个word knowledge

2313
01:44:01,370 --> 01:44:02,670
那我们可能就要说明一点

2314
01:44:02,670 --> 01:44:04,630
就是说为什么这个word knowledge

2315
01:44:04,630 --> 01:44:06,940
在推荐的这个场景下

2316
01:44:06,940 --> 01:44:09,000
它是一个适用的对

2317
01:44:09,000 --> 01:44:12,879
那这个时候如果大家都是做这种l m basic类型的

2318
01:44:12,879 --> 01:44:14,199
如果有其他人来问我们

2319
01:44:14,199 --> 01:44:16,879
就是说我们这个森林人推荐

2320
01:44:16,879 --> 01:44:19,049
为什么一定要用LM

2321
01:44:19,049 --> 01:44:22,049
那这个时候大家对于怎么去说服别人

2322
01:44:22,049 --> 01:44:25,409
也就是说讲清楚我们的动机有什么观点吗

2323
01:44:34,660 --> 01:44:39,530
那三言要不还是我先发言好啊

2324
01:44:39,930 --> 01:44:44,450
我觉得其实严格意义来说就是没有说必须一词

2325
01:44:44,450 --> 01:44:47,480
当然没有说你必须干什么才是好的研究

2326
01:44:47,480 --> 01:44:50,000
但是我觉得lm based recommendation

2327
01:44:50,000 --> 01:44:52,640
一定是就是我作为做科研类的人

2328
01:44:52,640 --> 01:44:54,250
要去探索的东西之一

2329
01:44:54,250 --> 01:44:57,670
因为这个东西它已经在太多其他的领域

2330
01:44:57,670 --> 01:44:59,770
就是发光发大

2331
01:44:59,770 --> 01:45:00,850
展现它的用途

2332
01:45:00,850 --> 01:45:04,050
所以我觉得这是一个非常呃intuitive

2333
01:45:04,050 --> 01:45:05,090
并且logical的事情

2334
01:45:05,090 --> 01:45:06,450
去看LM如何

2335
01:45:06,450 --> 01:45:10,290
能够帮助我们在推荐上得到更好的结果

2336
01:45:10,290 --> 01:45:11,690
嗯然后除此以外呢

2337
01:45:11,690 --> 01:45:15,260
现在也已经不止11234篇的研究有证明

2338
01:45:15,260 --> 01:45:16,520
我们用lm base的话

2339
01:45:16,520 --> 01:45:18,860
是要比之前的纯死MMD的id的

2340
01:45:18,860 --> 01:45:21,140
或者甚至连SMAID都比SAS

2341
01:45:21,140 --> 01:45:23,480
react那一代的模型要好的嗯

2342
01:45:23,480 --> 01:45:25,760
所以在这样的基础上

2343
01:45:25,760 --> 01:45:28,320
我觉得呃单从指标来说

2344
01:45:28,320 --> 01:45:31,440
我们就应该有更更更大的动力

2345
01:45:31,440 --> 01:45:35,890
去使用更大的computation开销

2346
01:45:35,890 --> 01:45:37,210
去获得更好的结果

2347
01:45:37,210 --> 01:45:41,300
然后我们才应该反之去优化我们的efficiency

2348
01:45:41,300 --> 01:45:44,190
和呃硬件和计算吧

2349
01:45:44,190 --> 01:45:45,610
然后除此以外

2350
01:45:45,610 --> 01:45:49,340
我觉得刚刚嗯提到一个非常重要的点

2351
01:45:49,340 --> 01:45:51,940
也就是INTERPRETABILITY和EXPENSIBILITY

2352
01:45:51,940 --> 01:45:54,300
也是我们现在可能除了指标以外

2353
01:45:54,300 --> 01:45:55,600
特别关注的一个问题

2354
01:45:55,600 --> 01:45:58,740
而这个事情本身是以往的方法

2355
01:45:58,740 --> 01:45:59,920
都没有办法带来的

2356
01:45:59,920 --> 01:46:04,950
可能目前LM是唯一一个比较通用的方案

2357
01:46:04,950 --> 01:46:07,340
对这是我的观点

2358
01:46:07,340 --> 01:46:11,730
OK查岛同学有什么对这个问题有什么关联吗

2359
01:46:11,730 --> 01:46:13,170
呃那我稍微补充一下

2360
01:46:13,170 --> 01:46:17,250
就是说为什么是我觉得就不只是不做大模型

2361
01:46:17,250 --> 01:46:17,530
推荐

2362
01:46:17,530 --> 01:46:18,730
人会问你这个问题

2363
01:46:18,730 --> 01:46:22,469
还有一些可能是把已经把这个随便基于semantic

2364
01:46:22,469 --> 01:46:24,240
id模型skating上去之后

2365
01:46:24,240 --> 01:46:25,360
他依然会问你说

2366
01:46:25,360 --> 01:46:28,890
你为什么要基于大模型base来去做这种推荐

2367
01:46:28,890 --> 01:46:31,600
就是现在业界可能有两种比较

2368
01:46:31,600 --> 01:46:33,160
可能算是矛盾的观点

2369
01:46:33,160 --> 01:46:37,440
一种是说认为说呃你对一个大模型架构的东西

2370
01:46:37,440 --> 01:46:42,440
然后你去不停的去做对这个模型做增量训练

2371
01:46:42,440 --> 01:46:44,580
你后面这个世界知识它就消失了

2372
01:46:44,580 --> 01:46:45,480
它有没有用的

2373
01:46:45,480 --> 01:46:47,880
就是它的模型只在你训练的第一版

2374
01:46:47,880 --> 01:46:50,500
第一天在这个指标上表现的很好

2375
01:46:50,500 --> 01:46:54,100
然后你不停的额后续的做这种增量训练之后

2376
01:46:54,100 --> 01:46:56,600
他反而模型它的指标会不断的下降

2377
01:46:56,600 --> 01:46:58,960
呃这是其中的呃一个现象

2378
01:46:58,960 --> 01:47:01,960
然后另外一派他是就像呃刚才润阳说的

2379
01:47:01,960 --> 01:47:05,200
就是认为世界知识对于这种推荐是有用的

2380
01:47:05,200 --> 01:47:09,509
可能包括呃快手那个阿拉阿拉三次方吉尔

2381
01:47:09,509 --> 01:47:12,200
还有包括可能是google p u m

2382
01:47:12,200 --> 01:47:13,640
他们都是另一派

2383
01:47:13,640 --> 01:47:15,040
就觉得说这个事

2384
01:47:15,040 --> 01:47:18,970
这这种视野就是就算是在semantic语义上

2385
01:47:18,970 --> 01:47:19,490
也是有用的

2386
01:47:19,490 --> 01:47:22,530
一方面可能是呃刚才聊到的这个世界知识

2387
01:47:22,530 --> 01:47:22,850
通用能力

2388
01:47:22,850 --> 01:47:24,800
一方面是这种通用序列的处理能力

2389
01:47:24,800 --> 01:47:25,560
对于这个是有用

2390
01:47:25,560 --> 01:47:26,960
这两派的观点是矛盾的

2391
01:47:26,960 --> 01:47:29,080
我个人是偏向于第二派

2392
01:47:29,080 --> 01:47:32,370
就是我觉得说为什么有的人做生份证推荐

2393
01:47:32,370 --> 01:47:33,190
把SK练上去之后

2394
01:47:33,190 --> 01:47:35,030
依然觉得说大模型是没必要的

2395
01:47:35,030 --> 01:47:35,990
世界这是没必要的

2396
01:47:35,990 --> 01:47:38,750
就是这个本身它的这种嗯

2397
01:47:38,750 --> 01:47:40,330
这种这种这是认知的偏差

2398
01:47:40,330 --> 01:47:41,610
可能来自于大模型本身

2399
01:47:41,610 --> 01:47:44,730
这种SP范式的带来的这种世界世世呃

2400
01:47:44,730 --> 01:47:46,360
知识的遗忘嘛

2401
01:47:46,360 --> 01:47:47,200
就灾难性的遗忘

2402
01:47:47,200 --> 01:47:48,400
就是你对于早期这种世界知识

2403
01:47:48,400 --> 01:47:51,020
反而你因为你后期你因为生殖推荐

2404
01:47:51,020 --> 01:47:54,070
因为推荐这种固有的这种场景的需求

2405
01:47:54,070 --> 01:47:55,710
它每每天都会进来大量的用户

2406
01:47:55,710 --> 01:47:57,990
你如果只把这个大量的用户来去训练

2407
01:47:57,990 --> 01:47:59,510
这种进行种增量的训练话

2408
01:47:59,510 --> 01:48:01,070
你前面的最开始的这种呃

2409
01:48:01,070 --> 01:48:03,300
实验室很久就是会消失

2410
01:48:03,300 --> 01:48:04,620
但是很多人呃

2411
01:48:04,620 --> 01:48:07,300
我个人我个人依然偏向于认为世界知识有用

2412
01:48:07,300 --> 01:48:08,860
但是在他们的这种场景里

2413
01:48:08,860 --> 01:48:11,420
可能后面就世界知识随着你训练增量的消失

2414
01:48:11,420 --> 01:48:13,720
他反而就在这个推荐的场景上

2415
01:48:13,720 --> 01:48:14,940
表现又会更差一些

2416
01:48:14,940 --> 01:48:19,300
就没有相比企业没有设计师是这种模型对OK行

2417
01:48:19,300 --> 01:48:20,420
所以小雨的观点是

2418
01:48:20,420 --> 01:48:23,300
可能说这种你的continue的training

2419
01:48:23,300 --> 01:48:26,049
或者是按这种这种

2420
01:48:27,670 --> 01:48:30,110
固定时间间隔确定会导致世界知识的遗忘

2421
01:48:30,110 --> 01:48:33,590
可能会认为误认为这种实质是没有用的

2422
01:48:33,590 --> 01:48:34,360
Ok

2423
01:48:34,360 --> 01:48:36,680
然后子杰和陈晓有没有什么

2424
01:48:36,680 --> 01:48:38,080
对于这个问题的补充

2425
01:48:40,410 --> 01:48:42,650
狭义的去理解这个l on base

2426
01:48:42,650 --> 01:48:43,290
瑞克的话

2427
01:48:43,290 --> 01:48:45,330
就我们直接去借用别的

2428
01:48:45,330 --> 01:48:47,490
已经别人经可圈好的LLLM

2429
01:48:47,490 --> 01:48:49,880
然后套上去去做一个推荐

2430
01:48:49,880 --> 01:48:53,700
那这个必要性如果纯从提升大语言模型

2431
01:48:53,700 --> 01:48:56,620
就是呃提提升推荐系统精确性来说

2432
01:48:56,620 --> 01:49:00,990
我觉得嗯确实没有什么一定有的那个必要

2433
01:49:00,990 --> 01:49:03,600
说不定tiger或者是些semantic架构

2434
01:49:03,600 --> 01:49:04,720
那SKILLING上去

2435
01:49:04,720 --> 01:49:07,280
然后这些事肯定会有一个更好的一个效果

2436
01:49:07,280 --> 01:49:11,029
不一定要依赖于lm base的一个一个架构

2437
01:49:11,029 --> 01:49:12,629
这是必须思考一个问题

2438
01:49:12,629 --> 01:49:16,370
但是如果从我刚才那个呃就是视角去看

2439
01:49:16,370 --> 01:49:17,510
整体大言模型

2440
01:49:17,510 --> 01:49:20,390
在推荐或者其他下游任务的一个研究来说

2441
01:49:20,390 --> 01:49:23,219
我觉得一个visible的一个呃

2442
01:49:23,219 --> 01:49:24,699
就是这个trajectory

2443
01:49:24,699 --> 01:49:26,239
就是他一个reason的过程

2444
01:49:26,239 --> 01:49:29,699
或者其他我们注入成熟的注入知识的一个方式

2445
01:49:29,699 --> 01:49:30,819
我觉得是非常重要的

2446
01:49:30,819 --> 01:49:31,339
为什么呢

2447
01:49:31,339 --> 01:49:34,680
因为呃我们需要去推荐一个东西的时候

2448
01:49:34,680 --> 01:49:38,240
我们实际上并不是对一个就是空中楼阁去做一

2449
01:49:38,240 --> 01:49:40,240
个准确性的一个提升

2450
01:49:40,240 --> 01:49:42,320
我们实际上要应对的一个事情

2451
01:49:42,320 --> 01:49:44,940
就是我们的策略是在不停改变的

2452
01:49:44,940 --> 01:49:48,200
大元模型对于当今算法工程是一个最大的改变

2453
01:49:48,200 --> 01:49:50,100
就是他完全可以用

2454
01:49:50,100 --> 01:49:52,420
比如说哪怕洗漱去去强化学习

2455
01:49:52,420 --> 01:49:55,180
SFT或其他方式上各种策略

2456
01:49:55,180 --> 01:49:58,720
那么目前我觉得只有l m base的一个架构

2457
01:49:58,720 --> 01:49:59,540
可以做到

2458
01:49:59,540 --> 01:50:02,570
就是对于reason过程中

2459
01:50:02,570 --> 01:50:04,810
以及最终那个知识进行一个呃

2460
01:50:04,810 --> 01:50:06,530
大量算法工程师

2461
01:50:06,530 --> 01:50:09,050
进行一个上策略的一个这样的一个方式

2462
01:50:09,050 --> 01:50:12,330
其他的一个基于表的学习或其他一个东西

2463
01:50:12,330 --> 01:50:14,270
他可能比较改漏词或者改特征

2464
01:50:14,270 --> 01:50:15,590
我觉得这是大元模型

2465
01:50:15,590 --> 01:50:19,350
目前在为什么能在工业界区如此多广泛领域

2466
01:50:19,350 --> 01:50:21,250
广泛领域的核心的原因

2467
01:50:21,250 --> 01:50:23,450
所以如果从这个角度去说的话

2468
01:50:23,450 --> 01:50:26,719
比如l base records在某些特定的角度

2469
01:50:26,719 --> 01:50:29,439
就是我们学商策略的情况以及灵活性

2470
01:50:29,439 --> 01:50:29,999
通用性

2471
01:50:29,999 --> 01:50:31,279
可解释性的情况下

2472
01:50:31,279 --> 01:50:33,300
是必须要去做研究的

2473
01:50:33,300 --> 01:50:34,740
嗯OK好

2474
01:50:34,740 --> 01:50:36,980
陈晓了有什么观点嗯

2475
01:50:36,980 --> 01:50:39,540
我比较赞同孔小玉的观点

2476
01:50:39,540 --> 01:50:42,540
就是我觉得LOL的世界知识

2477
01:50:42,540 --> 01:50:43,940
或者说他的推理能力

2478
01:50:43,940 --> 01:50:48,330
可能会随着大量用户交互数据的灌进来

2479
01:50:48,330 --> 01:50:49,450
会显得遗忘

2480
01:50:49,450 --> 01:50:52,829
而显得他在单纯拟合用户交互历史的

2481
01:50:52,829 --> 01:50:55,069
这个任务上显得似乎作用不大

2482
01:50:55,069 --> 01:50:57,509
但是LLM的语言接口

2483
01:50:57,509 --> 01:50:59,480
它仍然给我们提供了一些

2484
01:50:59,480 --> 01:51:01,800
在推荐新的范式上应用的可能

2485
01:51:01,800 --> 01:51:05,160
例如是交互或者是引导或者是可解式的推荐

2486
01:51:05,160 --> 01:51:07,950
我认为这些都是传统推荐他所做不了的

2487
01:51:07,950 --> 01:51:09,110
OK好

2488
01:51:09,110 --> 01:51:10,150
所以总结一下

2489
01:51:10,150 --> 01:51:14,780
大家可能就是觉得呃用大模型来做推荐事情

2490
01:51:14,780 --> 01:51:16,220
肯定上潜力上是有的

2491
01:51:16,220 --> 01:51:17,420
可能就是现在大家做不好

2492
01:51:17,420 --> 01:51:19,220
可能是因为潜力没有发发挥出来

2493
01:51:19,220 --> 01:51:24,090
所以觉得呃这种semantic id base或传统的方法

2494
01:51:24,090 --> 01:51:25,210
相对于大模型来讲

2495
01:51:25,210 --> 01:51:28,190
可能大模型代表不了一个什么东西

2496
01:51:28,190 --> 01:51:30,280
那这个原因可能就是说

2497
01:51:30,280 --> 01:51:32,780
我们没有发挥好它一个特长

2498
01:51:32,780 --> 01:51:36,620
或者说我们在使用的过程中去损失一些东西

2499
01:51:36,620 --> 01:51:38,700
那在这在这这这里呢

2500
01:51:38,700 --> 01:51:41,580
我也想补充我的一个角度啊

2501
01:51:41,580 --> 01:51:43,700
就是刚刚子杰提到了

2502
01:51:43,700 --> 01:51:46,680
就是说呃那个啥

2503
01:51:46,680 --> 01:51:50,840
我们现在都是看说怎么用大模型去帮助

2504
01:51:50,840 --> 01:51:51,600
推进的角度

2505
01:51:51,600 --> 01:51:53,160
其实我们也可以换一个角度看

2506
01:51:53,160 --> 01:51:55,960
就是说我们推荐是不是对大模型来讲

2507
01:51:55,960 --> 01:51:57,100
也是一个必须的

2508
01:51:57,100 --> 01:51:59,600
就是从我们现在大模型的应用来讲

2509
01:51:59,600 --> 01:52:01,400
比如说我们的OpenAI

2510
01:52:01,400 --> 01:52:04,260
最近不是出了一个类似于导购的功能吗

2511
01:52:04,260 --> 01:52:07,840
那它相当于就是在他的网站的平台上去做一个

2512
01:52:07,840 --> 01:52:08,840
推荐的一个工作

2513
01:52:08,840 --> 01:52:13,260
那这个时候你大模型本身应用到各种

2514
01:52:13,260 --> 01:52:16,320
就是比如说贴近人们的一个生活场景的时候

2515
01:52:16,320 --> 01:52:18,760
那这个时候可能他是他本身需要一个推荐的能

2516
01:52:18,760 --> 01:52:21,129
力来去帮助他

2517
01:52:21,129 --> 01:52:23,929
当然从功利的角度说他赚钱对吧

2518
01:52:23,929 --> 01:52:25,809
其他的可能就是说你说的好听点

2519
01:52:25,809 --> 01:52:27,320
就是满足用户的一些需求

2520
01:52:27,320 --> 01:52:29,480
所以我觉得这些也是一个角度吧

2521
01:52:29,480 --> 01:52:33,970
然后啊我在这儿分享一下我自己的一个观点

2522
01:52:33,970 --> 01:52:38,540
然后呢呃啊对于这个话题我们就到此结束

2523
01:52:38,540 --> 01:52:41,820
希望听众朋友们可以哈通过这个讨论

2524
01:52:41,820 --> 01:52:45,550
在这里汲取一些可以呃有用的灵感吧

2525
01:52:45,550 --> 01:52:48,840
然后比如说来帮助我们说服一些审稿人

2526
01:52:49,360 --> 01:52:54,240
那接下来我们的话题就进入到啊现状

2527
01:52:54,240 --> 01:52:56,890
就是说大家觉得现在对于l m based

2528
01:52:56,890 --> 01:52:57,770
recommendation来讲

2529
01:52:57,770 --> 01:52:59,250
它主要一个研究挑战是什么

2530
01:52:59,250 --> 01:53:02,210
目前有哪些东西限制这个方向发展

2531
01:53:02,980 --> 01:53:06,020
我们可以简短的过一下嗯

2532
01:53:07,660 --> 01:53:11,720
还要不我还是未央香江好行啊

2533
01:53:11,720 --> 01:53:14,430
我觉得就我个人的科研经验来说

2534
01:53:14,430 --> 01:53:17,960
我觉得限制低之一是算力呃

2535
01:53:17,960 --> 01:53:20,480
尤其是现在LM就是用于其他任务

2536
01:53:20,480 --> 01:53:22,900
LM已经SCALLING到几百B的方向

2537
01:53:22,900 --> 01:53:26,000
但是在推荐上我们是不可能用这个方向啊

2538
01:53:26,000 --> 01:53:29,129
用这个算这个大小的模型去做任何推荐的

2539
01:53:29,129 --> 01:53:32,389
是没法满足任何这种实施需求的

2540
01:53:32,389 --> 01:53:33,409
这是直接一

2541
01:53:33,409 --> 01:53:34,269
然后第二的话

2542
01:53:34,269 --> 01:53:37,540
也就是我们上次在话题三这条讨论这个点

2543
01:53:37,540 --> 01:53:39,200
我觉得目前的可能

2544
01:53:39,200 --> 01:53:42,900
我们训练LM进行推荐的方法仍然不是最优的

2545
01:53:42,900 --> 01:53:44,540
仍然还有很大的潜力

2546
01:53:44,540 --> 01:53:46,900
这仍然是个我们要去解决的问题

2547
01:53:48,220 --> 01:53:54,830
OK好的那个陈晓陈晓你每次都最后讲

2548
01:53:54,830 --> 01:53:56,990
这次先讲一讲嗯好

2549
01:53:56,990 --> 01:53:59,790
那我觉得现在可能l o n base的方法

2550
01:53:59,790 --> 01:54:01,420
还是就是大模型

2551
01:54:01,420 --> 01:54:03,060
传统的文本意义上的embedding

2552
01:54:03,060 --> 01:54:05,820
它没法很好地对齐到推荐任务上面

2553
01:54:05,820 --> 01:54:07,840
所以我认为像之前所提到

2554
01:54:07,840 --> 01:54:09,360
s i d base的方法的提出

2555
01:54:09,360 --> 01:54:12,490
也是为了能够解决嗯

2556
01:54:12,490 --> 01:54:15,830
大模型在对item上面建模不足的这个问题

2557
01:54:15,830 --> 01:54:18,230
我认为这是大模型面临的主要问题

2558
01:54:19,470 --> 01:54:21,099
OK好的

2559
01:54:22,139 --> 01:54:23,179
直接与小雨

2560
01:54:23,179 --> 01:54:24,969
你们那行

2561
01:54:24,969 --> 01:54:26,649
那我也简单说一下吧

2562
01:54:26,649 --> 01:54:30,249
然后这点上我呃非常同意陈晓的一个观点

2563
01:54:30,249 --> 01:54:33,429
因为呃我自研究所也能感受到

2564
01:54:33,429 --> 01:54:37,500
这个text或者是semantic d这样的一个家的概念

2565
01:54:37,500 --> 01:54:40,120
如果把它进行一个框架上面的程度

2566
01:54:40,120 --> 01:54:42,830
我觉得是这样重要一点嗯

2567
01:54:43,470 --> 01:54:45,190
OK小雨呢

2568
01:54:45,190 --> 01:54:47,390
OK那我从现实角度演说

2569
01:54:47,390 --> 01:54:50,030
因为我们自己做大模型贝斯的推荐系统

2570
01:54:50,030 --> 01:54:52,300
但是呃众所周知

2571
01:54:52,300 --> 01:54:54,600
推荐系统是一个非常面向真实的

2572
01:54:54,600 --> 01:54:56,910
真实的场景的一个方向

2573
01:54:56,910 --> 01:54:59,070
然后算力确实是一个非常大的挑战

2574
01:54:59,070 --> 01:55:01,950
可能现在呃现在受这种大厂

2575
01:55:01,950 --> 01:55:03,350
即使大厂所受这种算力限制

2576
01:55:03,350 --> 01:55:06,000
可能也是只能做0.5B或者是1.5B

2577
01:55:06,000 --> 01:55:08,140
这种规模的模型的这种skating

2578
01:55:08,140 --> 01:55:10,480
但是而且是基于semantic d

2579
01:55:10,480 --> 01:55:11,940
或者是生成召回的这种方式

2580
01:55:11,940 --> 01:55:16,000
但是我们可能呃想要把大模型base推荐真正做好

2581
01:55:16,000 --> 01:55:17,340
做大做c skin上去的话

2582
01:55:17,340 --> 01:55:18,980
还是需要一个更大的这种算体知识

2583
01:55:18,980 --> 01:55:21,480
但这个呃东西目前的发展的方向

2584
01:55:21,480 --> 01:55:24,040
可能是目前的大厂所觉得短期看不到收益的

2585
01:55:24,040 --> 01:55:25,940
他可能就是对于

2586
01:55:25,940 --> 01:55:28,060
可能我们目前研究知识又相对的来说

2587
01:55:28,060 --> 01:55:28,740
比较有限吧

2588
01:55:28,740 --> 01:55:29,900
这是一个比较现实的事情

2589
01:55:30,940 --> 01:55:31,900
OK好的嗯

2590
01:55:31,900 --> 01:55:35,850
嗯大家呃提的点都概括一下

2591
01:55:35,850 --> 01:55:39,250
可能就是说那个研究算力的限制

2592
01:55:39,250 --> 01:55:41,150
包括这种公益力的支持力度

2593
01:55:41,150 --> 01:55:45,300
然后就是啊大模型和推荐这个gap

2594
01:55:45,300 --> 01:55:47,460
这些可能是我们现在面临的一些挑战

2595
01:55:47,460 --> 01:55:49,100
那基于这样一个问题

2596
01:55:49,100 --> 01:55:50,520
基于这样一个东西

2597
01:55:50,520 --> 01:55:52,700
那我们接下一个问题

2598
01:55:52,700 --> 01:55:56,899
就是说大家觉得就是在大模型推荐这个方向哈

2599
01:55:56,899 --> 01:55:58,959
进来接下来有哪些比较有意思

2600
01:55:58,959 --> 01:56:02,879
可以继续关注的一个研究方向了

2601
01:56:06,020 --> 01:56:09,980
还是我先可以可以呃

2602
01:56:09,980 --> 01:56:11,420
我分享一个新闻吧

2603
01:56:11,420 --> 01:56:14,750
就是之前我看到就是那个perplexity

2604
01:56:14,750 --> 01:56:15,910
他们出的那个浏览器

2605
01:56:15,910 --> 01:56:17,290
然后呢它里面有个功能

2606
01:56:17,290 --> 01:56:20,120
就是能帮助用户自动的在亚马逊上买东西

2607
01:56:20,120 --> 01:56:21,560
然后亚马逊知道这个事情的时候

2608
01:56:21,560 --> 01:56:22,960
就起诉perplexity

2609
01:56:22,960 --> 01:56:24,510
是觉得这样的话

2610
01:56:24,510 --> 01:56:25,510
那他们他就推荐系统

2611
01:56:25,510 --> 01:56:26,870
他们的广告系统就没有用了

2612
01:56:26,870 --> 01:56:28,460
根本没办法向用户打广告

2613
01:56:28,460 --> 01:56:31,440
然后觉得如果PROGREITY使用了agent

2614
01:56:31,440 --> 01:56:32,540
去帮助用户们买的话

2615
01:56:32,540 --> 01:56:34,840
一定要标明他的机器人的身份

2616
01:56:34,840 --> 01:56:38,340
所以我觉得这可能也是就是呃

2617
01:56:38,340 --> 01:56:39,720
LM的这些新方向

2618
01:56:39,720 --> 01:56:44,360
比如agent会给呃广告或推荐领域带来的一个打击

2619
01:56:44,360 --> 01:56:46,180
但另一方面这也是机遇

2620
01:56:46,180 --> 01:56:49,180
就是我们如何比如说利用agent pipeline

2621
01:56:49,180 --> 01:56:52,710
去更好的帮助用户呃购买到心仪的产品

2622
01:56:52,710 --> 01:56:56,230
对嗯嗯这个角度很有意思

2623
01:56:58,800 --> 01:57:02,140
呃那我来好呃

2624
01:57:02,140 --> 01:57:04,060
也是基于agent的一切吧

2625
01:57:04,060 --> 01:57:06,420
呃一方面是一个说先说一个agent

2626
01:57:06,420 --> 01:57:09,600
就是呃目前呢可能是不管是大厂做推荐

2627
01:57:09,600 --> 01:57:10,480
还是我们做推荐

2628
01:57:10,480 --> 01:57:12,240
都是在一个比较孤立的一个

2629
01:57:12,240 --> 01:57:14,940
用户的数据上来去做拟合呃

2630
01:57:14,940 --> 01:57:17,420
比方说快手他可能就是短视频

2631
01:57:17,420 --> 01:57:19,339
然后阿里它就是电商

2632
01:57:19,339 --> 01:57:22,019
如果是只以用户孤立的在这个模型上的表现

2633
01:57:22,019 --> 01:57:23,279
来去做推荐的话

2634
01:57:23,279 --> 01:57:26,260
他推不好可能也是情有可原

2635
01:57:26,260 --> 01:57:29,540
然后真正的未来去大模型做贝斯推荐系统

2636
01:57:29,540 --> 01:57:33,170
可能如果说有一个可以真正的去呃

2637
01:57:33,170 --> 01:57:35,290
类似于用户推生活助手

2638
01:57:35,290 --> 01:57:38,559
或者是类似于用户的这种数字人的这种形式的

2639
01:57:38,559 --> 01:57:40,259
A键的产品出现的话

2640
01:57:40,259 --> 01:57:41,599
当一个推荐系统

2641
01:57:41,599 --> 01:57:44,539
它可以纵览用户整个的可能是不管是购买记录

2642
01:57:44,539 --> 01:57:46,210
还有他的说话方式

2643
01:57:46,210 --> 01:57:46,990
生活方式

2644
01:57:46,990 --> 01:57:49,330
它的location等等一些信息

2645
01:57:49,330 --> 01:57:52,609
用用这些非常大的就是呃

2646
01:57:52,609 --> 01:57:54,289
不只是在推荐场景的表现

2647
01:57:54,289 --> 01:57:56,049
还有一些他的说话的方式

2648
01:57:56,049 --> 01:57:57,209
包括他的一些偏好

2649
01:57:57,209 --> 01:57:58,729
这些都可以用大模型来去做

2650
01:57:58,729 --> 01:58:00,970
真正的这种呃比较精细的体验

2651
01:58:00,970 --> 01:58:02,910
做这种真正的这种personalize

2652
01:58:02,910 --> 01:58:04,070
真正的这种个人化的体验

2653
01:58:04,070 --> 01:58:07,470
而不是说只在一个某一个孤立的数据上去做这

2654
01:58:07,470 --> 01:58:09,730
种呃用户的拟合

2655
01:58:09,730 --> 01:58:13,000
而是去真正做这种类似于交互的去做呃

2656
01:58:13,000 --> 01:58:14,280
不仅可能不仅是绘画时

2657
01:58:14,280 --> 01:58:16,580
还有是这种主动的探索的帮助

2658
01:58:16,580 --> 01:58:20,500
帮助用户来做这种呃工作流的这种类似的推荐

2659
01:58:20,500 --> 01:58:21,600
对这是其中一个

2660
01:58:21,600 --> 01:58:22,520
然后另外一个就是

2661
01:58:22,520 --> 01:58:23,080
我觉得

2662
01:58:23,080 --> 01:58:26,690
可能因为吉米I3确实最近有点风头太盛

2663
01:58:26,690 --> 01:58:29,910
然后呃个人觉得推荐的

2664
01:58:29,910 --> 01:58:33,270
包括现在的SIID的这种推荐

2665
01:58:33,270 --> 01:58:35,590
都不一定是一个真正理想的形式

2666
01:58:35,590 --> 01:58:38,220
因为举一个不太雅的例子

2667
01:58:38,220 --> 01:58:41,940
比方说呃用户在文本空间就可能没法理解

2668
01:58:41,940 --> 01:58:46,620
说一个男的同时喜欢看呃电子产品和美女

2669
01:58:46,620 --> 01:58:48,060
所以我个人觉得

2670
01:58:48,060 --> 01:58:51,220
这个SID它可能可能可以作为一种新的模态

2671
01:58:51,220 --> 01:58:54,650
它不只是呃将这个作为一种语言去注入

2672
01:58:54,650 --> 01:58:55,330
语言的空间

2673
01:58:55,330 --> 01:58:59,760
而是作为独立于呃语言多模态呃

2674
01:58:59,760 --> 01:59:02,240
图片的另外一种新的模态去

2675
01:59:02,240 --> 01:59:03,720
可能是可能是基于SID

2676
01:59:03,720 --> 01:59:05,240
或者是其他一些更好的形式

2677
01:59:05,240 --> 01:59:08,300
去做一个真正的这种多模态的这种模型区

2678
01:59:08,300 --> 01:59:11,600
同时有呃绘画的理解的能力

2679
01:59:11,600 --> 01:59:14,780
也可以有真正的去给用户做好的推荐的能力

2680
01:59:14,780 --> 01:59:17,800
对OK那总结一下就是两个方向

2681
01:59:17,800 --> 01:59:20,000
第一个就是说可能类似于个人注册的形式

2682
01:59:20,000 --> 01:59:22,950
你可以拿到这个人的生活中的

2683
01:59:22,950 --> 01:59:24,330
大部分的一个数据

2684
01:59:24,330 --> 01:59:25,310
甚至所有的数据

2685
01:59:25,310 --> 01:59:29,840
然后来实现一个更更好的个性化建模

2686
01:59:29,840 --> 01:59:30,860
来实现一个推荐

2687
01:59:30,860 --> 01:59:32,260
然后另外一个可能就是说

2688
01:59:32,260 --> 01:59:37,349
我们是把我们的这种行为信息

2689
01:59:37,349 --> 01:59:39,540
当成一种新的模态进去

2690
01:59:39,540 --> 01:59:41,480
给这些大模型结合起来

2691
01:59:41,480 --> 01:59:44,910
既可以去建模一个行为上的一个语义

2692
01:59:44,910 --> 01:59:48,110
然后又同时可以基于这种什么文本上的语义

2693
01:59:48,110 --> 01:59:49,330
一起来做一些事情

2694
01:59:49,330 --> 01:59:51,370
当然也包括一些动模态的东西

2695
01:59:51,370 --> 01:59:53,170
OK挺有意思的

2696
01:59:53,170 --> 01:59:56,950
嗯那个陈晓陈晓呢

2697
01:59:56,950 --> 01:59:58,150
陈晓你也简单讲讲

2698
01:59:58,910 --> 02:00:00,610
我我还是我

2699
02:00:00,610 --> 02:00:01,690
我的主要观点也是

2700
02:00:01,690 --> 02:00:04,279
我认为它大模型在将来

2701
02:00:04,279 --> 02:00:08,500
可能可以从一个你和用户交互的数学基取机器

2702
02:00:08,500 --> 02:00:11,960
更像是转变为成为用户的一个贴身小秘的

2703
02:00:11,960 --> 02:00:12,520
这一个状态

2704
02:00:12,520 --> 02:00:13,760
就是把它成为一个A卷的

2705
02:00:13,760 --> 02:00:16,760
可以利用更好的利用大模型的推理和交互能力

2706
02:00:16,760 --> 02:00:18,720
来去调用工具进行规划决策

2707
02:00:18,720 --> 02:00:20,720
就像现在GPT的浏览器

2708
02:00:20,720 --> 02:00:22,300
他们在干的这一套事情一样

2709
02:00:22,300 --> 02:00:26,670
嗯嗯嗯OK好的

2710
02:00:26,670 --> 02:00:28,510
子杰呢子杰你也补充一下

2711
02:00:30,830 --> 02:00:33,290
嗯这点你比较认同陈晓

2712
02:00:33,290 --> 02:00:35,950
就是后面我们可能会多一些A点

2713
02:00:35,950 --> 02:00:39,740
或者更贴近用户的一个角度去做一个探索吧

2714
02:00:39,740 --> 02:00:42,300
然后那个如果从研究范式的角度来说

2715
02:00:42,300 --> 02:00:45,020
因为嗯如果是纯计算机领域的话

2716
02:00:45,020 --> 02:00:48,300
我们对于这种偏用户行为分析的

2717
02:00:48,300 --> 02:00:49,420
可能不如AHCI

2718
02:00:49,420 --> 02:00:52,490
或者是比如类似种类型交互领域的呃

2719
02:00:52,490 --> 02:00:53,450
我们可以借鉴一下

2720
02:00:53,450 --> 02:00:55,790
比如说快啊这种气氛呃

2721
02:00:55,790 --> 02:00:59,350
会议上的一些就是这种偏交叉型的一个方向啊

2722
02:00:59,350 --> 02:01:03,520
比如说哪种类型的LBLAKE的用户的那个

2723
02:01:03,520 --> 02:01:06,140
然后点击性能有什么样的影响

2724
02:01:06,140 --> 02:01:08,980
但是在研究角度是这样子嗯

2725
02:01:08,980 --> 02:01:12,700
OK你的意思是可以搞一些跨领域的一些合

2726
02:01:12,700 --> 02:01:13,500
并的一些研究

2727
02:01:13,500 --> 02:01:17,370
所以说嗯对参考h sci这样的

2728
02:01:17,370 --> 02:01:22,770
可能OK感觉各位同学提的方向都挺有意思的

2729
02:01:22,770 --> 02:01:25,030
嗯而且可能就是短期内

2730
02:01:25,030 --> 02:01:27,190
或者说近接下来一两年内

2731
02:01:27,190 --> 02:01:28,800
可能确实一些比较

2732
02:01:28,800 --> 02:01:29,960
从我个人的角度

2733
02:01:29,960 --> 02:01:32,060
我觉得还是非常值得研究一个点

2734
02:01:32,060 --> 02:01:35,910
那呃接下来就步入最后一个话题

2735
02:01:35,910 --> 02:01:38,950
可能就是跟我们这个新人推荐没有哦

2736
02:01:38,950 --> 02:01:40,430
没有结合的关系的

2737
02:01:40,430 --> 02:01:42,290
就比如我们刚刚提到这些点对吧

2738
02:01:42,290 --> 02:01:44,790
那是不是和我们刚刚入门

2739
02:01:44,790 --> 02:01:47,010
做深圳师推荐的一些同学来做呢

2740
02:01:47,010 --> 02:01:49,790
那怎么去更好的来入门一下

2741
02:01:49,790 --> 02:01:51,710
甚至推荐大家可以分享一下

2742
02:01:51,710 --> 02:01:53,590
简单分享自己的观点

2743
02:01:55,720 --> 02:01:58,220
嗯OK那那我接着我来吧

2744
02:01:58,220 --> 02:02:01,960
呃我觉得还是呃怎么说呢

2745
02:02:01,960 --> 02:02:04,640
还是要了解一下侠义下的侠义下的这个东西

2746
02:02:04,640 --> 02:02:06,080
然后了解一下这东西的历史

2747
02:02:06,080 --> 02:02:08,200
大家还是要看看tiger人跟paper

2748
02:02:08,200 --> 02:02:09,400
然后呢

2749
02:02:09,400 --> 02:02:10,600
呃第二个呢

2750
02:02:10,600 --> 02:02:13,960
就是我觉得我们托管还是分成非常inspiring的

2751
02:02:13,960 --> 02:02:16,550
就是额要去了解agent

2752
02:02:16,550 --> 02:02:20,590
以及personalized的推荐助手

2753
02:02:20,590 --> 02:02:21,190
这类

2754
02:02:21,190 --> 02:02:25,630
就是我觉得算是今年下半年比较火的一个方向

2755
02:02:26,740 --> 02:02:29,600
K所以既得有过去

2756
02:02:29,600 --> 02:02:33,800
也得有这种比较时髦的东西嗯

2757
02:02:34,870 --> 02:02:39,059
好呃那我正好打个广告

2758
02:02:40,359 --> 02:02:42,299
因为我们最近就做了一个开源项目

2759
02:02:42,299 --> 02:02:43,059
mini玩瑞克

2760
02:02:43,059 --> 02:02:45,459
然后他其实就是刚才我们聊到的

2761
02:02:45,459 --> 02:02:46,379
用大模型贝斯

2762
02:02:46,379 --> 02:02:50,899
然后去在大模型的语义和SID去做这种对齐

2763
02:02:50,899 --> 02:02:51,499
完了之后

2764
02:02:51,499 --> 02:02:53,239
你可能通过一些其他的trick

2765
02:02:53,239 --> 02:02:56,530
去让大模型可以来做基于SID的生成对现

2766
02:02:56,530 --> 02:02:59,690
然后感兴趣的朋友们都可以去关注一下

2767
02:02:59,690 --> 02:03:02,170
我们给个star之类的嗯

2768
02:03:02,170 --> 02:03:03,450
或者给我们拍拍砖

2769
02:03:03,450 --> 02:03:07,840
谢谢好的嗯

2770
02:03:08,480 --> 02:03:10,340
呃子杰陈晓

2771
02:03:10,340 --> 02:03:12,880
你俩有没有补充呃

2772
02:03:12,880 --> 02:03:14,040
我没有太多

2773
02:03:14,040 --> 02:03:16,180
因为我目前也在这个阶段

2774
02:03:16,180 --> 02:03:21,639
所以嗯还是后面去给小雨的那个LEO点个star

2775
02:03:21,639 --> 02:03:22,590
关注一下

2776
02:03:22,590 --> 02:03:27,480
这方面还是要多向各位学习好看下呢

2777
02:03:27,480 --> 02:03:31,409
嗯我也基本认同之前的各位师兄师姐的观点

2778
02:03:31,409 --> 02:03:34,009
就是我觉得可能就是从tiger看起

2779
02:03:34,009 --> 02:03:34,929
然后看一下

2780
02:03:34,929 --> 02:03:38,400
24年开始像later系列这一类编码技术

2781
02:03:38,400 --> 02:03:40,940
然后就可以构造到孔桥玉师兄他们说的

2782
02:03:40,940 --> 02:03:42,060
从简单的SFT

2783
02:03:42,060 --> 02:03:43,920
然后再上到一些强化的trick

2784
02:03:43,920 --> 02:03:46,180
去来去做写平号对齐

2785
02:03:47,500 --> 02:03:48,320
OK行

2786
02:03:48,320 --> 02:03:52,670
所以我们这里知道历史才可以掌握未来

2787
02:03:52,670 --> 02:03:55,500
所以大家啊如果有刚刚想入门

2788
02:03:55,500 --> 02:03:56,500
现在推荐的同学

2789
02:03:56,500 --> 02:03:58,840
可能就是先把一些经典的方法先去看一看

2790
02:03:58,840 --> 02:04:03,030
然后我们再把当前的热点给抓住行

2791
02:04:03,030 --> 02:04:04,890
然后我们今天的时间也差不多了

2792
02:04:04,890 --> 02:04:07,630
然后就到啊这里结束

2793
02:04:07,630 --> 02:04:10,770
感谢各位同学以及各位听众

2794
02:04:10,770 --> 02:04:15,299
抽出时间来参加我们这一个呃呃雷普啊

2795
02:04:15,299 --> 02:04:16,339
森林式推荐的专场

2796
02:04:16,339 --> 02:04:20,590
然后呃啊谢谢大家

2797
02:04:20,590 --> 02:04:22,710
那我们就这样

2798
02:04:22,710 --> 02:04:26,160
拜拜拜拜拜拜

